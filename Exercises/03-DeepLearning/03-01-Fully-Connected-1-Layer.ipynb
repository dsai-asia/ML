{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 113.453196\n",
      "Iteration 1 loss 62.949267\n",
      "Iteration 2 loss 54.054635\n",
      "Iteration 3 loss 49.285947\n",
      "Iteration 4 loss 46.270994\n",
      "Iteration 5 loss 44.164128\n",
      "Iteration 6 loss 42.535371\n",
      "Iteration 7 loss 41.278778\n",
      "Iteration 8 loss 40.255512\n",
      "Iteration 9 loss 39.354582\n",
      "Iteration 10 loss 38.562164\n",
      "Iteration 11 loss 37.907338\n",
      "Iteration 12 loss 37.404441\n",
      "Iteration 13 loss 36.869295\n",
      "Iteration 14 loss 36.378136\n",
      "Iteration 15 loss 35.955143\n",
      "Iteration 16 loss 35.633535\n",
      "Iteration 17 loss 35.274350\n",
      "Iteration 18 loss 34.929887\n",
      "Iteration 19 loss 34.642657\n",
      "Iteration 20 loss 34.361045\n",
      "Iteration 21 loss 34.114935\n",
      "Iteration 22 loss 33.886097\n",
      "Iteration 23 loss 33.649737\n",
      "Iteration 24 loss 33.410080\n",
      "Iteration 25 loss 33.264315\n",
      "Iteration 26 loss 33.056749\n",
      "Iteration 27 loss 32.889636\n",
      "Iteration 28 loss 32.727215\n",
      "Iteration 29 loss 32.482428\n",
      "Iteration 30 loss 32.379631\n",
      "Iteration 31 loss 32.270100\n",
      "Iteration 32 loss 32.144203\n",
      "Iteration 33 loss 31.993190\n",
      "Iteration 34 loss 31.872404\n",
      "Iteration 35 loss 31.725339\n",
      "Iteration 36 loss 31.621142\n",
      "Iteration 37 loss 31.455104\n",
      "Iteration 38 loss 31.429218\n",
      "Iteration 39 loss 31.328378\n",
      "Iteration 40 loss 31.197825\n",
      "Iteration 41 loss 31.139550\n",
      "Iteration 42 loss 31.041838\n",
      "Iteration 43 loss 30.906264\n",
      "Iteration 44 loss 30.873929\n",
      "Iteration 45 loss 30.783862\n",
      "Iteration 46 loss 30.704743\n",
      "Iteration 47 loss 30.624025\n",
      "Iteration 48 loss 30.535482\n",
      "Iteration 49 loss 30.484143\n",
      "Iteration 50 loss 30.377036\n",
      "Iteration 51 loss 30.335705\n",
      "Iteration 52 loss 30.266225\n",
      "Iteration 53 loss 30.214464\n",
      "Iteration 54 loss 30.142930\n",
      "Iteration 55 loss 30.073394\n",
      "Iteration 56 loss 30.002756\n",
      "Iteration 57 loss 29.971234\n",
      "Iteration 58 loss 29.891695\n",
      "Iteration 59 loss 29.860351\n",
      "Iteration 60 loss 29.779219\n",
      "Iteration 61 loss 29.749475\n",
      "Iteration 62 loss 29.684988\n",
      "Iteration 63 loss 29.646259\n",
      "Iteration 64 loss 29.599050\n",
      "Iteration 65 loss 29.547431\n",
      "Iteration 66 loss 29.472748\n",
      "Iteration 67 loss 29.446605\n",
      "Iteration 68 loss 29.322714\n",
      "Iteration 69 loss 29.355469\n",
      "Iteration 70 loss 29.302726\n",
      "Iteration 71 loss 29.264310\n",
      "Iteration 72 loss 29.231783\n",
      "Iteration 73 loss 29.189769\n",
      "Iteration 74 loss 29.153304\n",
      "Iteration 75 loss 29.072920\n",
      "Iteration 76 loss 29.063890\n",
      "Iteration 77 loss 29.047123\n",
      "Iteration 78 loss 29.000137\n",
      "Iteration 79 loss 28.949194\n",
      "Iteration 80 loss 28.911255\n",
      "Iteration 81 loss 28.881083\n",
      "Iteration 82 loss 28.818583\n",
      "Iteration 83 loss 28.796351\n",
      "Iteration 84 loss 28.758803\n",
      "Iteration 85 loss 28.761557\n",
      "Iteration 86 loss 28.721849\n",
      "Iteration 87 loss 28.668479\n",
      "Iteration 88 loss 28.634546\n",
      "Iteration 89 loss 28.575617\n",
      "Iteration 90 loss 28.562768\n",
      "Iteration 91 loss 28.542124\n",
      "Iteration 92 loss 28.491720\n",
      "Iteration 93 loss 28.448551\n",
      "Iteration 94 loss 28.460221\n",
      "Iteration 95 loss 28.420827\n",
      "Iteration 96 loss 28.322727\n",
      "Iteration 97 loss 28.361156\n",
      "Iteration 98 loss 28.333532\n",
      "Iteration 99 loss 28.312393\n",
      "Iteration 100 loss 28.269306\n",
      "Iteration 101 loss 28.234484\n",
      "Iteration 102 loss 28.206813\n",
      "Iteration 103 loss 28.177183\n",
      "Iteration 104 loss 28.163835\n",
      "Iteration 105 loss 28.110737\n",
      "Iteration 106 loss 28.090229\n",
      "Iteration 107 loss 28.076927\n",
      "Iteration 108 loss 28.042073\n",
      "Iteration 109 loss 27.982569\n",
      "Iteration 110 loss 27.998572\n",
      "Iteration 111 loss 27.984878\n",
      "Iteration 112 loss 27.905104\n",
      "Iteration 113 loss 27.940548\n",
      "Iteration 114 loss 27.866539\n",
      "Iteration 115 loss 27.894292\n",
      "Iteration 116 loss 27.827263\n",
      "Iteration 117 loss 27.834799\n",
      "Iteration 118 loss 27.687782\n",
      "Iteration 119 loss 27.809843\n",
      "Iteration 120 loss 27.770856\n",
      "Iteration 121 loss 27.719000\n",
      "Iteration 122 loss 27.713881\n",
      "Iteration 123 loss 27.679104\n",
      "Iteration 124 loss 27.652402\n",
      "Iteration 125 loss 27.641112\n",
      "Iteration 126 loss 27.598658\n",
      "Iteration 127 loss 27.581335\n",
      "Iteration 128 loss 27.520698\n",
      "Iteration 129 loss 27.550112\n",
      "Iteration 130 loss 27.524519\n",
      "Iteration 131 loss 27.517702\n",
      "Iteration 132 loss 27.478651\n",
      "Iteration 133 loss 27.473016\n",
      "Iteration 134 loss 27.443517\n",
      "Iteration 135 loss 27.407514\n",
      "Iteration 136 loss 27.401304\n",
      "Iteration 137 loss 27.355665\n",
      "Iteration 138 loss 27.310417\n",
      "Iteration 139 loss 27.341707\n",
      "Iteration 140 loss 27.327342\n",
      "Iteration 141 loss 27.319063\n",
      "Iteration 142 loss 27.266021\n",
      "Iteration 143 loss 27.258048\n",
      "Iteration 144 loss 27.230782\n",
      "Iteration 145 loss 27.221430\n",
      "Iteration 146 loss 27.125683\n",
      "Iteration 147 loss 27.179754\n",
      "Iteration 148 loss 27.164147\n",
      "Iteration 149 loss 27.167386\n",
      "Iteration 150 loss 27.135810\n",
      "Iteration 151 loss 27.095064\n",
      "Iteration 152 loss 27.111487\n",
      "Iteration 153 loss 27.064365\n",
      "Iteration 154 loss 27.057239\n",
      "Iteration 155 loss 27.029966\n",
      "Iteration 156 loss 27.004226\n",
      "Iteration 157 loss 27.016195\n",
      "Iteration 158 loss 26.992536\n",
      "Iteration 159 loss 26.984142\n",
      "Iteration 160 loss 26.932836\n",
      "Iteration 161 loss 26.916011\n",
      "Iteration 162 loss 26.906467\n",
      "Iteration 163 loss 26.891624\n",
      "Iteration 164 loss 26.842927\n",
      "Iteration 165 loss 26.756529\n",
      "Iteration 166 loss 26.865535\n",
      "Iteration 167 loss 26.795303\n",
      "Iteration 168 loss 26.813111\n",
      "Iteration 169 loss 26.796262\n",
      "Iteration 170 loss 26.781654\n",
      "Iteration 171 loss 26.734178\n",
      "Iteration 172 loss 26.760972\n",
      "Iteration 173 loss 26.717150\n",
      "Iteration 174 loss 26.699751\n",
      "Iteration 175 loss 26.699779\n",
      "Iteration 176 loss 26.660098\n",
      "Iteration 177 loss 26.685568\n",
      "Iteration 178 loss 26.655023\n",
      "Iteration 179 loss 26.636292\n",
      "Iteration 180 loss 26.594601\n",
      "Iteration 181 loss 26.592903\n",
      "Iteration 182 loss 26.607018\n",
      "Iteration 183 loss 26.583716\n",
      "Iteration 184 loss 26.557559\n",
      "Iteration 185 loss 26.540703\n",
      "Iteration 186 loss 26.531706\n",
      "Iteration 187 loss 26.517645\n",
      "Iteration 188 loss 26.501233\n",
      "Iteration 189 loss 26.468643\n",
      "Iteration 190 loss 26.443516\n",
      "Iteration 191 loss 26.414594\n",
      "Iteration 192 loss 26.423646\n",
      "Iteration 193 loss 26.426785\n",
      "Iteration 194 loss 26.408834\n",
      "Iteration 195 loss 26.371517\n",
      "Iteration 196 loss 26.384196\n",
      "Iteration 197 loss 26.371742\n",
      "Iteration 198 loss 26.346298\n",
      "Iteration 199 loss 26.348011\n",
      "Iteration 200 loss 26.323017\n",
      "Iteration 201 loss 26.315085\n",
      "Iteration 202 loss 26.308024\n",
      "Iteration 203 loss 26.276678\n",
      "Iteration 204 loss 26.275881\n",
      "Iteration 205 loss 26.245887\n",
      "Iteration 206 loss 26.205595\n",
      "Iteration 207 loss 26.248790\n",
      "Iteration 208 loss 26.195361\n",
      "Iteration 209 loss 26.181168\n",
      "Iteration 210 loss 26.157070\n",
      "Iteration 211 loss 26.174267\n",
      "Iteration 212 loss 26.163621\n",
      "Iteration 213 loss 26.118109\n",
      "Iteration 214 loss 26.101109\n",
      "Iteration 215 loss 26.120837\n",
      "Iteration 216 loss 26.111719\n",
      "Iteration 217 loss 26.090706\n",
      "Iteration 218 loss 26.075449\n",
      "Iteration 219 loss 26.060795\n",
      "Iteration 220 loss 26.070051\n",
      "Iteration 221 loss 26.034993\n",
      "Iteration 222 loss 26.031358\n",
      "Iteration 223 loss 26.036083\n",
      "Iteration 224 loss 25.998383\n",
      "Iteration 225 loss 25.999001\n",
      "Iteration 226 loss 25.985268\n",
      "Iteration 227 loss 25.940035\n",
      "Iteration 228 loss 25.958579\n",
      "Iteration 229 loss 25.935985\n",
      "Iteration 230 loss 25.926309\n",
      "Iteration 231 loss 25.912440\n",
      "Iteration 232 loss 25.888379\n",
      "Iteration 233 loss 25.896336\n",
      "Iteration 234 loss 25.849110\n",
      "Iteration 235 loss 25.867159\n",
      "Iteration 236 loss 25.876883\n",
      "Iteration 237 loss 25.846449\n",
      "Iteration 238 loss 25.847173\n",
      "Iteration 239 loss 25.834260\n",
      "Iteration 240 loss 25.800013\n",
      "Iteration 241 loss 25.805345\n",
      "Iteration 242 loss 25.762118\n",
      "Iteration 243 loss 25.785988\n",
      "Iteration 244 loss 25.752291\n",
      "Iteration 245 loss 25.735027\n",
      "Iteration 246 loss 25.753019\n",
      "Iteration 247 loss 25.731352\n",
      "Iteration 248 loss 25.715681\n",
      "Iteration 249 loss 25.707225\n",
      "Iteration 250 loss 25.712077\n",
      "Iteration 251 loss 25.680789\n",
      "Iteration 252 loss 25.687405\n",
      "Iteration 253 loss 25.660929\n",
      "Iteration 254 loss 25.626851\n",
      "Iteration 255 loss 25.642484\n",
      "Iteration 256 loss 25.626279\n",
      "Iteration 257 loss 25.623724\n",
      "Iteration 258 loss 25.610657\n",
      "Iteration 259 loss 25.605144\n",
      "Iteration 260 loss 25.589752\n",
      "Iteration 261 loss 25.542078\n",
      "Iteration 262 loss 25.547713\n",
      "Iteration 263 loss 25.558114\n",
      "Iteration 264 loss 25.530409\n",
      "Iteration 265 loss 25.544669\n",
      "Iteration 266 loss 25.535345\n",
      "Iteration 267 loss 25.528561\n",
      "Iteration 268 loss 25.519528\n",
      "Iteration 269 loss 25.488689\n",
      "Iteration 270 loss 25.452545\n",
      "Iteration 271 loss 25.488019\n",
      "Iteration 272 loss 25.469664\n",
      "Iteration 273 loss 25.441001\n",
      "Iteration 274 loss 25.431004\n",
      "Iteration 275 loss 25.388453\n",
      "Iteration 276 loss 25.418575\n",
      "Iteration 277 loss 25.389788\n",
      "Iteration 278 loss 25.418531\n",
      "Iteration 279 loss 25.414198\n",
      "Iteration 280 loss 25.383314\n",
      "Iteration 281 loss 25.308496\n",
      "Iteration 282 loss 25.365041\n",
      "Iteration 283 loss 25.359336\n",
      "Iteration 284 loss 25.331277\n",
      "Iteration 285 loss 25.344994\n",
      "Iteration 286 loss 25.270395\n",
      "Iteration 287 loss 25.334621\n",
      "Iteration 288 loss 25.278200\n",
      "Iteration 289 loss 25.283663\n",
      "Iteration 290 loss 25.263775\n",
      "Iteration 291 loss 25.274869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 292 loss 25.257319\n",
      "Iteration 293 loss 25.238840\n",
      "Iteration 294 loss 25.244575\n",
      "Iteration 295 loss 25.251989\n",
      "Iteration 296 loss 25.230017\n",
      "Iteration 297 loss 25.217188\n",
      "Iteration 298 loss 25.210241\n",
      "Iteration 299 loss 25.191471\n",
      "Iteration 300 loss 25.193058\n",
      "Iteration 301 loss 25.150786\n",
      "Iteration 302 loss 25.182610\n",
      "Iteration 303 loss 25.139394\n",
      "Iteration 304 loss 25.148586\n",
      "Iteration 305 loss 25.140783\n",
      "Iteration 306 loss 25.094792\n",
      "Iteration 307 loss 25.125881\n",
      "Iteration 308 loss 25.091888\n",
      "Iteration 309 loss 25.080812\n",
      "Iteration 310 loss 25.061782\n",
      "Iteration 311 loss 25.098462\n",
      "Iteration 312 loss 25.097278\n",
      "Iteration 313 loss 25.058711\n",
      "Iteration 314 loss 25.030671\n",
      "Iteration 315 loss 25.043802\n",
      "Iteration 316 loss 24.981454\n",
      "Iteration 317 loss 25.006872\n",
      "Iteration 318 loss 25.043896\n",
      "Iteration 319 loss 25.019481\n",
      "Iteration 320 loss 24.992162\n",
      "Iteration 321 loss 24.949441\n",
      "Iteration 322 loss 24.986290\n",
      "Iteration 323 loss 24.984058\n",
      "Iteration 324 loss 24.972800\n",
      "Iteration 325 loss 24.962665\n",
      "Iteration 326 loss 24.886988\n",
      "Iteration 327 loss 24.928892\n",
      "Iteration 328 loss 24.950478\n",
      "Iteration 329 loss 24.922672\n",
      "Iteration 330 loss 24.923450\n",
      "Iteration 331 loss 24.910941\n",
      "Iteration 332 loss 24.885232\n",
      "Iteration 333 loss 24.868482\n",
      "Iteration 334 loss 24.879773\n",
      "Iteration 335 loss 24.864165\n",
      "Iteration 336 loss 24.868497\n",
      "Iteration 337 loss 24.864959\n",
      "Iteration 338 loss 24.825688\n",
      "Iteration 339 loss 24.835009\n",
      "Iteration 340 loss 24.785310\n",
      "Iteration 341 loss 24.830107\n",
      "Iteration 342 loss 24.825666\n",
      "Iteration 343 loss 24.771301\n",
      "Iteration 344 loss 24.808220\n",
      "Iteration 345 loss 24.785178\n",
      "Iteration 346 loss 24.764911\n",
      "Iteration 347 loss 24.749892\n",
      "Iteration 348 loss 24.780709\n",
      "Iteration 349 loss 24.733646\n",
      "Iteration 350 loss 24.770912\n",
      "Iteration 351 loss 24.720193\n",
      "Iteration 352 loss 24.740269\n",
      "Iteration 353 loss 24.731147\n",
      "Iteration 354 loss 24.734771\n",
      "Iteration 355 loss 24.727165\n",
      "Iteration 356 loss 24.700503\n",
      "Iteration 357 loss 24.649145\n",
      "Iteration 358 loss 24.701981\n",
      "Iteration 359 loss 24.680893\n",
      "Iteration 360 loss 24.687538\n",
      "Iteration 361 loss 24.659752\n",
      "Iteration 362 loss 24.648665\n",
      "Iteration 363 loss 24.615514\n",
      "Iteration 364 loss 24.630118\n",
      "Iteration 365 loss 24.654498\n",
      "Iteration 366 loss 24.605375\n",
      "Iteration 367 loss 24.602278\n",
      "Iteration 368 loss 24.606221\n",
      "Iteration 369 loss 24.558371\n",
      "Iteration 370 loss 24.552622\n",
      "Iteration 371 loss 24.603678\n",
      "Iteration 372 loss 24.523926\n",
      "Iteration 373 loss 24.622570\n",
      "Iteration 374 loss 24.586878\n",
      "Iteration 375 loss 24.572844\n",
      "Iteration 376 loss 24.554567\n",
      "Iteration 377 loss 24.519989\n",
      "Iteration 378 loss 24.540365\n",
      "Iteration 379 loss 24.493214\n",
      "Iteration 380 loss 24.508659\n",
      "Iteration 381 loss 24.505121\n",
      "Iteration 382 loss 24.490377\n",
      "Iteration 383 loss 24.482152\n",
      "Iteration 384 loss 24.494698\n",
      "Iteration 385 loss 24.484615\n",
      "Iteration 386 loss 24.490023\n",
      "Iteration 387 loss 24.485658\n",
      "Iteration 388 loss 24.457413\n",
      "Iteration 389 loss 24.416867\n",
      "Iteration 390 loss 24.450088\n",
      "Iteration 391 loss 24.423786\n",
      "Iteration 392 loss 24.425921\n",
      "Iteration 393 loss 24.426432\n",
      "Iteration 394 loss 24.427721\n",
      "Iteration 395 loss 24.418491\n",
      "Iteration 396 loss 24.408958\n",
      "Iteration 397 loss 24.375521\n",
      "Iteration 398 loss 24.394655\n",
      "Iteration 399 loss 24.354907\n",
      "Iteration 400 loss 24.396323\n",
      "Iteration 401 loss 24.382524\n",
      "Iteration 402 loss 24.338711\n",
      "Iteration 403 loss 24.370614\n",
      "Iteration 404 loss 24.349409\n",
      "Iteration 405 loss 24.330567\n",
      "Iteration 406 loss 24.330688\n",
      "Iteration 407 loss 24.332426\n",
      "Iteration 408 loss 24.330538\n",
      "Iteration 409 loss 24.321821\n",
      "Iteration 410 loss 24.268425\n",
      "Iteration 411 loss 24.323832\n",
      "Iteration 412 loss 24.286953\n",
      "Iteration 413 loss 24.260848\n",
      "Iteration 414 loss 24.265685\n",
      "Iteration 415 loss 24.261998\n",
      "Iteration 416 loss 24.279648\n",
      "Iteration 417 loss 24.239840\n",
      "Iteration 418 loss 24.264347\n",
      "Iteration 419 loss 24.252547\n",
      "Iteration 420 loss 24.207968\n",
      "Iteration 421 loss 24.229919\n",
      "Iteration 422 loss 24.220155\n",
      "Iteration 423 loss 24.216924\n",
      "Iteration 424 loss 24.212301\n",
      "Iteration 425 loss 24.154787\n",
      "Iteration 426 loss 24.213294\n",
      "Iteration 427 loss 24.203844\n",
      "Iteration 428 loss 24.197276\n",
      "Iteration 429 loss 24.174346\n",
      "Iteration 430 loss 24.160972\n",
      "Iteration 431 loss 24.162942\n",
      "Iteration 432 loss 24.167676\n",
      "Iteration 433 loss 24.169915\n",
      "Iteration 434 loss 24.131763\n",
      "Iteration 435 loss 24.081834\n",
      "Iteration 436 loss 24.113081\n",
      "Iteration 437 loss 24.110413\n",
      "Iteration 438 loss 24.128686\n",
      "Iteration 439 loss 24.061434\n",
      "Iteration 440 loss 24.131050\n",
      "Iteration 441 loss 24.093732\n",
      "Iteration 442 loss 24.102481\n",
      "Iteration 443 loss 24.012728\n",
      "Iteration 444 loss 24.053124\n",
      "Iteration 445 loss 24.038544\n",
      "Iteration 446 loss 24.050306\n",
      "Iteration 447 loss 24.101978\n",
      "Iteration 448 loss 24.085178\n",
      "Iteration 449 loss 24.069471\n",
      "Iteration 450 loss 23.968054\n",
      "Iteration 451 loss 24.062858\n",
      "Iteration 452 loss 24.041248\n",
      "Iteration 453 loss 24.040023\n",
      "Iteration 454 loss 24.028574\n",
      "Iteration 455 loss 24.031740\n",
      "Iteration 456 loss 24.018563\n",
      "Iteration 457 loss 24.004305\n",
      "Iteration 458 loss 24.009997\n",
      "Iteration 459 loss 23.985375\n",
      "Iteration 460 loss 23.983322\n",
      "Iteration 461 loss 23.969519\n",
      "Iteration 462 loss 23.985282\n",
      "Iteration 463 loss 23.964518\n",
      "Iteration 464 loss 23.975423\n",
      "Iteration 465 loss 23.944434\n",
      "Iteration 466 loss 23.962746\n",
      "Iteration 467 loss 23.948260\n",
      "Iteration 468 loss 23.913341\n",
      "Iteration 469 loss 23.935512\n",
      "Iteration 470 loss 23.870233\n",
      "Iteration 471 loss 23.933190\n",
      "Iteration 472 loss 23.931726\n",
      "Iteration 473 loss 23.915261\n",
      "Iteration 474 loss 23.857553\n",
      "Iteration 475 loss 23.914706\n",
      "Iteration 476 loss 23.886428\n",
      "Iteration 477 loss 23.893432\n",
      "Iteration 478 loss 23.863980\n",
      "Iteration 479 loss 23.856539\n",
      "Iteration 480 loss 23.882489\n",
      "Iteration 481 loss 23.858483\n",
      "Iteration 482 loss 23.865643\n",
      "Iteration 483 loss 23.863037\n",
      "Iteration 484 loss 23.830513\n",
      "Iteration 485 loss 23.827071\n",
      "Iteration 486 loss 23.828058\n",
      "Iteration 487 loss 23.839888\n",
      "Iteration 488 loss 23.782554\n",
      "Iteration 489 loss 23.795786\n",
      "Iteration 490 loss 23.764489\n",
      "Iteration 491 loss 23.797233\n",
      "Iteration 492 loss 23.733266\n",
      "Iteration 493 loss 23.808195\n",
      "Iteration 494 loss 23.802712\n",
      "Iteration 495 loss 23.795310\n",
      "Iteration 496 loss 23.778186\n",
      "Iteration 497 loss 23.768368\n",
      "Iteration 498 loss 23.763918\n",
      "Iteration 499 loss 23.736029\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return XX\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "# Let's start with a 1-layer network with sigmoid activation function.\n",
    "\n",
    "W = [[], np.random.normal(0,0.1,[N,1])]\n",
    "b = [[], np.random.normal(0,0.1,[1,1])]\n",
    "L = len(W)-1\n",
    "\n",
    "def act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return -((1-y) * np.log(1-yhat) + y * np.log(yhat))\n",
    "    \n",
    "# Use mini-batch size 1\n",
    "\n",
    "alpha = 0.01\n",
    "max_iter = 500\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = XX[order[i],:].T\n",
    "        y_this = y[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            a.append(act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss(y_this,a[L][0,0])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "            \n",
    "        # Backprop step... (to fill in!)\n",
    "        delta[L] = a[L] - y_this\n",
    "        db[L] = delta[L].copy()\n",
    "        dW[L] = a[L-1] * delta[L]\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        W[L] = W[L] - alpha * dW[L]\n",
    "        b[L] = b[L] - alpha * db[L]\n",
    "        \n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

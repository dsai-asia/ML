{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 380.203615\n",
      "Iteration 1 loss 376.481795\n",
      "Iteration 2 loss 377.241251\n",
      "Iteration 3 loss 376.696839\n",
      "Iteration 4 loss 376.477832\n",
      "Iteration 5 loss 376.613159\n",
      "Iteration 6 loss 376.346457\n",
      "Iteration 7 loss 375.917764\n",
      "Iteration 8 loss 375.119121\n",
      "Iteration 9 loss 374.910703\n",
      "Iteration 10 loss 373.213126\n",
      "Iteration 11 loss 370.788597\n",
      "Iteration 12 loss 367.132264\n",
      "Iteration 13 loss 360.149841\n",
      "Iteration 14 loss 347.689953\n",
      "Iteration 15 loss 327.932626\n",
      "Iteration 16 loss 295.027145\n",
      "Iteration 17 loss 251.245490\n",
      "Iteration 18 loss 203.710591\n",
      "Iteration 19 loss 161.962016\n",
      "Iteration 20 loss 129.665621\n",
      "Iteration 21 loss 106.088010\n",
      "Iteration 22 loss 89.709142\n",
      "Iteration 23 loss 77.886782\n",
      "Iteration 24 loss 69.321922\n",
      "Iteration 25 loss 62.941945\n",
      "Iteration 26 loss 58.051575\n",
      "Iteration 27 loss 54.154231\n",
      "Iteration 28 loss 51.037998\n",
      "Iteration 29 loss 48.444908\n",
      "Iteration 30 loss 46.341934\n",
      "Iteration 31 loss 44.505411\n",
      "Iteration 32 loss 42.956490\n",
      "Iteration 33 loss 41.544080\n",
      "Iteration 34 loss 40.402639\n",
      "Iteration 35 loss 39.265965\n",
      "Iteration 36 loss 38.455373\n",
      "Iteration 37 loss 37.574202\n",
      "Iteration 38 loss 36.866272\n",
      "Iteration 39 loss 36.114611\n",
      "Iteration 40 loss 35.565585\n",
      "Iteration 41 loss 35.075236\n",
      "Iteration 42 loss 34.574606\n",
      "Iteration 43 loss 34.083127\n",
      "Iteration 44 loss 33.647317\n",
      "Iteration 45 loss 33.263111\n",
      "Iteration 46 loss 32.834278\n",
      "Iteration 47 loss 32.500286\n",
      "Iteration 48 loss 32.156196\n",
      "Iteration 49 loss 31.822692\n",
      "Iteration 50 loss 31.584506\n",
      "Iteration 51 loss 31.273231\n",
      "Iteration 52 loss 30.752184\n",
      "Iteration 53 loss 30.865846\n",
      "Iteration 54 loss 30.509712\n",
      "Iteration 55 loss 30.312723\n",
      "Iteration 56 loss 30.010718\n",
      "Iteration 57 loss 29.855384\n",
      "Iteration 58 loss 29.844487\n",
      "Iteration 59 loss 29.516199\n",
      "Iteration 60 loss 29.335575\n",
      "Iteration 61 loss 29.247154\n",
      "Iteration 62 loss 29.063784\n",
      "Iteration 63 loss 28.871411\n",
      "Iteration 64 loss 28.645076\n",
      "Iteration 65 loss 28.731579\n",
      "Iteration 66 loss 28.486649\n",
      "Iteration 67 loss 28.167564\n",
      "Iteration 68 loss 28.309333\n",
      "Iteration 69 loss 28.097959\n",
      "Iteration 70 loss 28.020673\n",
      "Iteration 71 loss 27.831315\n",
      "Iteration 72 loss 27.772179\n",
      "Iteration 73 loss 27.623484\n",
      "Iteration 74 loss 27.546761\n",
      "Iteration 75 loss 27.525907\n",
      "Iteration 76 loss 27.008620\n",
      "Iteration 77 loss 26.857147\n",
      "Iteration 78 loss 27.033405\n",
      "Iteration 79 loss 27.232197\n",
      "Iteration 80 loss 26.953818\n",
      "Iteration 81 loss 26.754309\n",
      "Iteration 82 loss 26.654725\n",
      "Iteration 83 loss 26.578402\n",
      "Iteration 84 loss 26.695426\n",
      "Iteration 85 loss 26.382922\n",
      "Iteration 86 loss 26.248593\n",
      "Iteration 87 loss 26.151140\n",
      "Iteration 88 loss 26.131878\n",
      "Iteration 89 loss 26.008852\n",
      "Iteration 90 loss 25.834289\n",
      "Iteration 91 loss 25.804730\n",
      "Iteration 92 loss 25.573702\n",
      "Iteration 93 loss 25.509873\n",
      "Iteration 94 loss 25.495084\n",
      "Iteration 95 loss 25.297430\n",
      "Iteration 96 loss 25.176546\n",
      "Iteration 97 loss 25.309748\n",
      "Iteration 98 loss 25.142401\n",
      "Iteration 99 loss 24.901247\n",
      "Iteration 100 loss 24.855690\n",
      "Iteration 101 loss 24.608886\n",
      "Iteration 102 loss 24.618776\n",
      "Iteration 103 loss 24.513553\n",
      "Iteration 104 loss 24.360470\n",
      "Iteration 105 loss 24.181205\n",
      "Iteration 106 loss 24.139770\n",
      "Iteration 107 loss 24.193378\n",
      "Iteration 108 loss 23.946202\n",
      "Iteration 109 loss 23.886618\n",
      "Iteration 110 loss 23.544326\n",
      "Iteration 111 loss 23.895744\n",
      "Iteration 112 loss 23.650563\n",
      "Iteration 113 loss 23.400609\n",
      "Iteration 114 loss 23.281811\n",
      "Iteration 115 loss 23.262934\n",
      "Iteration 116 loss 23.182954\n",
      "Iteration 117 loss 23.017407\n",
      "Iteration 118 loss 22.891819\n",
      "Iteration 119 loss 22.904300\n",
      "Iteration 120 loss 22.741994\n",
      "Iteration 121 loss 22.850377\n",
      "Iteration 122 loss 22.463080\n",
      "Iteration 123 loss 22.670867\n",
      "Iteration 124 loss 22.326830\n",
      "Iteration 125 loss 22.286350\n",
      "Iteration 126 loss 22.254227\n",
      "Iteration 127 loss 22.081831\n",
      "Iteration 128 loss 21.789086\n",
      "Iteration 129 loss 21.985449\n",
      "Iteration 130 loss 21.914455\n",
      "Iteration 131 loss 21.756869\n",
      "Iteration 132 loss 21.659985\n",
      "Iteration 133 loss 21.468062\n",
      "Iteration 134 loss 21.512734\n",
      "Iteration 135 loss 21.292021\n",
      "Iteration 136 loss 21.320888\n",
      "Iteration 137 loss 21.118946\n",
      "Iteration 138 loss 20.943178\n",
      "Iteration 139 loss 21.033070\n",
      "Iteration 140 loss 20.851199\n",
      "Iteration 141 loss 20.791864\n",
      "Iteration 142 loss 20.463271\n",
      "Iteration 143 loss 20.602052\n",
      "Iteration 144 loss 20.534455\n",
      "Iteration 145 loss 20.361078\n",
      "Iteration 146 loss 20.259635\n",
      "Iteration 147 loss 20.211824\n",
      "Iteration 148 loss 19.981553\n",
      "Iteration 149 loss 20.109370\n",
      "Iteration 150 loss 19.847657\n",
      "Iteration 151 loss 19.816303\n",
      "Iteration 152 loss 19.625814\n",
      "Iteration 153 loss 19.662588\n",
      "Iteration 154 loss 19.475395\n",
      "Iteration 155 loss 19.447777\n",
      "Iteration 156 loss 19.312884\n",
      "Iteration 157 loss 19.234511\n",
      "Iteration 158 loss 19.231737\n",
      "Iteration 159 loss 19.081711\n",
      "Iteration 160 loss 18.891671\n",
      "Iteration 161 loss 18.860800\n",
      "Iteration 162 loss 18.689004\n",
      "Iteration 163 loss 18.708402\n",
      "Iteration 164 loss 18.527491\n",
      "Iteration 165 loss 18.398207\n",
      "Iteration 166 loss 18.310220\n",
      "Iteration 167 loss 18.228558\n",
      "Iteration 168 loss 18.183064\n",
      "Iteration 169 loss 18.000287\n",
      "Iteration 170 loss 17.994579\n",
      "Iteration 171 loss 17.800893\n",
      "Iteration 172 loss 17.780990\n",
      "Iteration 173 loss 17.467890\n",
      "Iteration 174 loss 17.291173\n",
      "Iteration 175 loss 17.429590\n",
      "Iteration 176 loss 17.363031\n",
      "Iteration 177 loss 17.205115\n",
      "Iteration 178 loss 17.132701\n",
      "Iteration 179 loss 16.962206\n",
      "Iteration 180 loss 16.954983\n",
      "Iteration 181 loss 16.853322\n",
      "Iteration 182 loss 16.684461\n",
      "Iteration 183 loss 16.477933\n",
      "Iteration 184 loss 16.484831\n",
      "Iteration 185 loss 16.409781\n",
      "Iteration 186 loss 16.201191\n",
      "Iteration 187 loss 16.215227\n",
      "Iteration 188 loss 16.010180\n",
      "Iteration 189 loss 15.980734\n",
      "Iteration 190 loss 15.909852\n",
      "Iteration 191 loss 15.708569\n",
      "Iteration 192 loss 15.732011\n",
      "Iteration 193 loss 15.558500\n",
      "Iteration 194 loss 15.309267\n",
      "Iteration 195 loss 15.391354\n",
      "Iteration 196 loss 15.236405\n",
      "Iteration 197 loss 15.069763\n",
      "Iteration 198 loss 15.016837\n",
      "Iteration 199 loss 15.027914\n",
      "Iteration 200 loss 14.842506\n",
      "Iteration 201 loss 14.804322\n",
      "Iteration 202 loss 14.449346\n",
      "Iteration 203 loss 14.614263\n",
      "Iteration 204 loss 14.478914\n",
      "Iteration 205 loss 14.380885\n",
      "Iteration 206 loss 14.328727\n",
      "Iteration 207 loss 14.112937\n",
      "Iteration 208 loss 14.162737\n",
      "Iteration 209 loss 14.041588\n",
      "Iteration 210 loss 13.767183\n",
      "Iteration 211 loss 13.860943\n",
      "Iteration 212 loss 13.764486\n",
      "Iteration 213 loss 13.518067\n",
      "Iteration 214 loss 13.543999\n",
      "Iteration 215 loss 13.484716\n",
      "Iteration 216 loss 13.204871\n",
      "Iteration 217 loss 13.296384\n",
      "Iteration 218 loss 13.128700\n",
      "Iteration 219 loss 13.070902\n",
      "Iteration 220 loss 12.927983\n",
      "Iteration 221 loss 12.883378\n",
      "Iteration 222 loss 12.764619\n",
      "Iteration 223 loss 12.703942\n",
      "Iteration 224 loss 12.662662\n",
      "Iteration 225 loss 12.574899\n",
      "Iteration 226 loss 12.483566\n",
      "Iteration 227 loss 12.411102\n",
      "Iteration 228 loss 12.251993\n",
      "Iteration 229 loss 12.193486\n",
      "Iteration 230 loss 12.063976\n",
      "Iteration 231 loss 12.018475\n",
      "Iteration 232 loss 11.916475\n",
      "Iteration 233 loss 11.872446\n",
      "Iteration 234 loss 11.662759\n",
      "Iteration 235 loss 11.764422\n",
      "Iteration 236 loss 11.628069\n",
      "Iteration 237 loss 11.537783\n",
      "Iteration 238 loss 11.320977\n",
      "Iteration 239 loss 11.289701\n",
      "Iteration 240 loss 11.258149\n",
      "Iteration 241 loss 11.172758\n",
      "Iteration 242 loss 10.995593\n",
      "Iteration 243 loss 11.043999\n",
      "Iteration 244 loss 10.930028\n",
      "Iteration 245 loss 10.927866\n",
      "Iteration 246 loss 10.823661\n",
      "Iteration 247 loss 10.661133\n",
      "Iteration 248 loss 10.647431\n",
      "Iteration 249 loss 10.510103\n",
      "Iteration 250 loss 10.527474\n",
      "Iteration 251 loss 10.401131\n",
      "Iteration 252 loss 10.269843\n",
      "Iteration 253 loss 10.157635\n",
      "Iteration 254 loss 10.171074\n",
      "Iteration 255 loss 10.117049\n",
      "Iteration 256 loss 9.960907\n",
      "Iteration 257 loss 10.005846\n",
      "Iteration 258 loss 9.803766\n",
      "Iteration 259 loss 9.849120\n",
      "Iteration 260 loss 9.723216\n",
      "Iteration 261 loss 9.487501\n",
      "Iteration 262 loss 9.416001\n",
      "Iteration 263 loss 9.442808\n",
      "Iteration 264 loss 9.551805\n",
      "Iteration 265 loss 9.435976\n",
      "Iteration 266 loss 9.272573\n",
      "Iteration 267 loss 9.240673\n",
      "Iteration 268 loss 9.166213\n",
      "Iteration 269 loss 9.157189\n",
      "Iteration 270 loss 8.997481\n",
      "Iteration 271 loss 9.023962\n",
      "Iteration 272 loss 8.826785\n",
      "Iteration 273 loss 8.721755\n",
      "Iteration 274 loss 8.871790\n",
      "Iteration 275 loss 8.773283\n",
      "Iteration 276 loss 8.479643\n",
      "Iteration 277 loss 8.690451\n",
      "Iteration 278 loss 8.584822\n",
      "Iteration 279 loss 8.500832\n",
      "Iteration 280 loss 8.402931\n",
      "Iteration 281 loss 8.420214\n",
      "Iteration 282 loss 8.285746\n",
      "Iteration 283 loss 8.267911\n",
      "Iteration 284 loss 8.094441\n",
      "Iteration 285 loss 8.089273\n",
      "Iteration 286 loss 7.829064\n",
      "Iteration 287 loss 8.060462\n",
      "Iteration 288 loss 7.936122\n",
      "Iteration 289 loss 7.857855\n",
      "Iteration 290 loss 7.744437\n",
      "Iteration 291 loss 7.755386\n",
      "Iteration 292 loss 7.698301\n",
      "Iteration 293 loss 7.686627\n",
      "Iteration 294 loss 7.562529\n",
      "Iteration 295 loss 7.582211\n",
      "Iteration 296 loss 7.454147\n",
      "Iteration 297 loss 7.319107\n",
      "Iteration 298 loss 7.403177\n",
      "Iteration 299 loss 7.293059\n",
      "Iteration 300 loss 7.160552\n",
      "Iteration 301 loss 7.238017\n",
      "Iteration 302 loss 7.081827\n",
      "Iteration 303 loss 6.970049\n",
      "Iteration 304 loss 7.055928\n",
      "Iteration 305 loss 6.959867\n",
      "Iteration 306 loss 6.947563\n",
      "Iteration 307 loss 6.879891\n",
      "Iteration 308 loss 6.834282\n",
      "Iteration 309 loss 6.681292\n",
      "Iteration 310 loss 6.437897\n",
      "Iteration 311 loss 6.648776\n",
      "Iteration 312 loss 6.606983\n",
      "Iteration 313 loss 6.587146\n",
      "Iteration 314 loss 6.531086\n",
      "Iteration 315 loss 6.477689\n",
      "Iteration 316 loss 6.436386\n",
      "Iteration 317 loss 6.351209\n",
      "Iteration 318 loss 6.238080\n",
      "Iteration 319 loss 6.287570\n",
      "Iteration 320 loss 6.170707\n",
      "Iteration 321 loss 6.120988\n",
      "Iteration 322 loss 6.064256\n",
      "Iteration 323 loss 6.087350\n",
      "Iteration 324 loss 6.013542\n",
      "Iteration 325 loss 5.989510\n",
      "Iteration 326 loss 5.955860\n",
      "Iteration 327 loss 5.934437\n",
      "Iteration 328 loss 5.866529\n",
      "Iteration 329 loss 5.740746\n",
      "Iteration 330 loss 5.849869\n",
      "Iteration 331 loss 5.743574\n",
      "Iteration 332 loss 5.665832\n",
      "Iteration 333 loss 5.549972\n",
      "Iteration 334 loss 5.675803\n",
      "Iteration 335 loss 5.557596\n",
      "Iteration 336 loss 5.557375\n",
      "Iteration 337 loss 5.461425\n",
      "Iteration 338 loss 5.413605\n",
      "Iteration 339 loss 5.377050\n",
      "Iteration 340 loss 5.406369\n",
      "Iteration 341 loss 5.342007\n",
      "Iteration 342 loss 5.297523\n",
      "Iteration 343 loss 5.230734\n",
      "Iteration 344 loss 5.196554\n",
      "Iteration 345 loss 5.161473\n",
      "Iteration 346 loss 5.079690\n",
      "Iteration 347 loss 5.049274\n",
      "Iteration 348 loss 5.063283\n",
      "Iteration 349 loss 5.006854\n",
      "Iteration 350 loss 4.932040\n",
      "Iteration 351 loss 4.946857\n",
      "Iteration 352 loss 4.918780\n",
      "Iteration 353 loss 4.788255\n",
      "Iteration 354 loss 4.808186\n",
      "Iteration 355 loss 4.750387\n",
      "Iteration 356 loss 4.759196\n",
      "Iteration 357 loss 4.619393\n",
      "Iteration 358 loss 4.725223\n",
      "Iteration 359 loss 4.662269\n",
      "Iteration 360 loss 4.615369\n",
      "Iteration 361 loss 4.554065\n",
      "Iteration 362 loss 4.588678\n",
      "Iteration 363 loss 4.507809\n",
      "Iteration 364 loss 4.496391\n",
      "Iteration 365 loss 4.475063\n",
      "Iteration 366 loss 4.405232\n",
      "Iteration 367 loss 4.392351\n",
      "Iteration 368 loss 4.223244\n",
      "Iteration 369 loss 4.338428\n",
      "Iteration 370 loss 4.302415\n",
      "Iteration 371 loss 4.145488\n",
      "Iteration 372 loss 4.270382\n",
      "Iteration 373 loss 4.204497\n",
      "Iteration 374 loss 4.172442\n",
      "Iteration 375 loss 4.131260\n",
      "Iteration 376 loss 4.052143\n",
      "Iteration 377 loss 4.105194\n",
      "Iteration 378 loss 4.027398\n",
      "Iteration 379 loss 4.031455\n",
      "Iteration 380 loss 3.988786\n",
      "Iteration 381 loss 3.959707\n",
      "Iteration 382 loss 3.965500\n",
      "Iteration 383 loss 3.935877\n",
      "Iteration 384 loss 3.901518\n",
      "Iteration 385 loss 3.868867\n",
      "Iteration 386 loss 3.833769\n",
      "Iteration 387 loss 3.797510\n",
      "Iteration 388 loss 3.795812\n",
      "Iteration 389 loss 3.766577\n",
      "Iteration 390 loss 3.736508\n",
      "Iteration 391 loss 3.700572\n",
      "Iteration 392 loss 3.671749\n",
      "Iteration 393 loss 3.624535\n",
      "Iteration 394 loss 3.560255\n",
      "Iteration 395 loss 3.597457\n",
      "Iteration 396 loss 3.620097\n",
      "Iteration 397 loss 3.550528\n",
      "Iteration 398 loss 3.537186\n",
      "Iteration 399 loss 3.487556\n",
      "Iteration 400 loss 3.469129\n",
      "Iteration 401 loss 3.477839\n",
      "Iteration 402 loss 3.420971\n",
      "Iteration 403 loss 3.421273\n",
      "Iteration 404 loss 3.286997\n",
      "Iteration 405 loss 3.421298\n",
      "Iteration 406 loss 3.344986\n",
      "Iteration 407 loss 3.338311\n",
      "Iteration 408 loss 3.273427\n",
      "Iteration 409 loss 3.222927\n",
      "Iteration 410 loss 3.307836\n",
      "Iteration 411 loss 3.264237\n",
      "Iteration 412 loss 3.209206\n",
      "Iteration 413 loss 3.150285\n",
      "Iteration 414 loss 3.196142\n",
      "Iteration 415 loss 3.155166\n",
      "Iteration 416 loss 3.133243\n",
      "Iteration 417 loss 3.102911\n",
      "Iteration 418 loss 3.117038\n",
      "Iteration 419 loss 3.084756\n",
      "Iteration 420 loss 2.990087\n",
      "Iteration 421 loss 3.070611\n",
      "Iteration 422 loss 3.035537\n",
      "Iteration 423 loss 2.969909\n",
      "Iteration 424 loss 2.983898\n",
      "Iteration 425 loss 2.955968\n",
      "Iteration 426 loss 2.957025\n",
      "Iteration 427 loss 2.805739\n",
      "Iteration 428 loss 2.956012\n",
      "Iteration 429 loss 2.900966\n",
      "Iteration 430 loss 2.836493\n",
      "Iteration 431 loss 2.888320\n",
      "Iteration 432 loss 2.844508\n",
      "Iteration 433 loss 2.805502\n",
      "Iteration 434 loss 2.815960\n",
      "Iteration 435 loss 2.780631\n",
      "Iteration 436 loss 2.746690\n",
      "Iteration 437 loss 2.737011\n",
      "Iteration 438 loss 2.705022\n",
      "Iteration 439 loss 2.724852\n",
      "Iteration 440 loss 2.718789\n",
      "Iteration 441 loss 2.688840\n",
      "Iteration 442 loss 2.683525\n",
      "Iteration 443 loss 2.642387\n",
      "Iteration 444 loss 2.643012\n",
      "Iteration 445 loss 2.627992\n",
      "Iteration 446 loss 2.589839\n",
      "Iteration 447 loss 2.615777\n",
      "Iteration 448 loss 2.585270\n",
      "Iteration 449 loss 2.511753\n",
      "Iteration 450 loss 2.558722\n",
      "Iteration 451 loss 2.528136\n",
      "Iteration 452 loss 2.528834\n",
      "Iteration 453 loss 2.512286\n",
      "Iteration 454 loss 2.494442\n",
      "Iteration 455 loss 2.482887\n",
      "Iteration 456 loss 2.445807\n",
      "Iteration 457 loss 2.458009\n",
      "Iteration 458 loss 2.445480\n",
      "Iteration 459 loss 2.366253\n",
      "Iteration 460 loss 2.417221\n",
      "Iteration 461 loss 2.395045\n",
      "Iteration 462 loss 2.358889\n",
      "Iteration 463 loss 2.368301\n",
      "Iteration 464 loss 2.364445\n",
      "Iteration 465 loss 2.349882\n",
      "Iteration 466 loss 2.322405\n",
      "Iteration 467 loss 2.306262\n",
      "Iteration 468 loss 2.299731\n",
      "Iteration 469 loss 2.300528\n",
      "Iteration 470 loss 2.276788\n",
      "Iteration 471 loss 2.213811\n",
      "Iteration 472 loss 2.239039\n",
      "Iteration 473 loss 2.282348\n",
      "Iteration 474 loss 2.211475\n",
      "Iteration 475 loss 2.219816\n",
      "Iteration 476 loss 2.222243\n",
      "Iteration 477 loss 2.177218\n",
      "Iteration 478 loss 2.187543\n",
      "Iteration 479 loss 2.129297\n",
      "Iteration 480 loss 2.178198\n",
      "Iteration 481 loss 2.143225\n",
      "Iteration 482 loss 2.138555\n",
      "Iteration 483 loss 2.134092\n",
      "Iteration 484 loss 2.124436\n",
      "Iteration 485 loss 2.105000\n",
      "Iteration 486 loss 2.095678\n",
      "Iteration 487 loss 2.068733\n",
      "Iteration 488 loss 2.083407\n",
      "Iteration 489 loss 2.046893\n",
      "Iteration 490 loss 2.061021\n",
      "Iteration 491 loss 2.032904\n",
      "Iteration 492 loss 2.031709\n",
      "Iteration 493 loss 2.003195\n",
      "Iteration 494 loss 2.010943\n",
      "Iteration 495 loss 2.000476\n",
      "Iteration 496 loss 1.996478\n",
      "Iteration 497 loss 1.988294\n",
      "Iteration 498 loss 1.972994\n",
      "Iteration 499 loss 1.953284\n",
      "Iteration 500 loss 1.946010\n",
      "Iteration 501 loss 1.945646\n",
      "Iteration 502 loss 1.922516\n",
      "Iteration 503 loss 1.926807\n",
      "Iteration 504 loss 1.910117\n",
      "Iteration 505 loss 1.898285\n",
      "Iteration 506 loss 1.883140\n",
      "Iteration 507 loss 1.896358\n",
      "Iteration 508 loss 1.877572\n",
      "Iteration 509 loss 1.866486\n",
      "Iteration 510 loss 1.861424\n",
      "Iteration 511 loss 1.850036\n",
      "Iteration 512 loss 1.830605\n",
      "Iteration 513 loss 1.841874\n",
      "Iteration 514 loss 1.821804\n",
      "Iteration 515 loss 1.813949\n",
      "Iteration 516 loss 1.805936\n",
      "Iteration 517 loss 1.797304\n",
      "Iteration 518 loss 1.779012\n",
      "Iteration 519 loss 1.786433\n",
      "Iteration 520 loss 1.771950\n",
      "Iteration 521 loss 1.767710\n",
      "Iteration 522 loss 1.754805\n",
      "Iteration 523 loss 1.742930\n",
      "Iteration 524 loss 1.729106\n",
      "Iteration 525 loss 1.714486\n",
      "Iteration 526 loss 1.713308\n",
      "Iteration 527 loss 1.712900\n",
      "Iteration 528 loss 1.708317\n",
      "Iteration 529 loss 1.703557\n",
      "Iteration 530 loss 1.687204\n",
      "Iteration 531 loss 1.688119\n",
      "Iteration 532 loss 1.647725\n",
      "Iteration 533 loss 1.679853\n",
      "Iteration 534 loss 1.670031\n",
      "Iteration 535 loss 1.657760\n",
      "Iteration 536 loss 1.647005\n",
      "Iteration 537 loss 1.626267\n",
      "Iteration 538 loss 1.637717\n",
      "Iteration 539 loss 1.624566\n",
      "Iteration 540 loss 1.615861\n",
      "Iteration 541 loss 1.606400\n",
      "Iteration 542 loss 1.599280\n",
      "Iteration 543 loss 1.602947\n",
      "Iteration 544 loss 1.579813\n",
      "Iteration 545 loss 1.581925\n",
      "Iteration 546 loss 1.576502\n",
      "Iteration 547 loss 1.566766\n",
      "Iteration 548 loss 1.560582\n",
      "Iteration 549 loss 1.559271\n",
      "Iteration 550 loss 1.546560\n",
      "Iteration 551 loss 1.546572\n",
      "Iteration 552 loss 1.528734\n",
      "Iteration 553 loss 1.534796\n",
      "Iteration 554 loss 1.525276\n",
      "Iteration 555 loss 1.519113\n",
      "Iteration 556 loss 1.513288\n",
      "Iteration 557 loss 1.504832\n",
      "Iteration 558 loss 1.495703\n",
      "Iteration 559 loss 1.488030\n",
      "Iteration 560 loss 1.485502\n",
      "Iteration 561 loss 1.479468\n",
      "Iteration 562 loss 1.474725\n",
      "Iteration 563 loss 1.468923\n",
      "Iteration 564 loss 1.457979\n",
      "Iteration 565 loss 1.455928\n",
      "Iteration 566 loss 1.448305\n",
      "Iteration 567 loss 1.444850\n",
      "Iteration 568 loss 1.431860\n",
      "Iteration 569 loss 1.429865\n",
      "Iteration 570 loss 1.426829\n",
      "Iteration 571 loss 1.405923\n",
      "Iteration 572 loss 1.419601\n",
      "Iteration 573 loss 1.397496\n",
      "Iteration 574 loss 1.409322\n",
      "Iteration 575 loss 1.386424\n",
      "Iteration 576 loss 1.386064\n",
      "Iteration 577 loss 1.386457\n",
      "Iteration 578 loss 1.373470\n",
      "Iteration 579 loss 1.369225\n",
      "Iteration 580 loss 1.367983\n",
      "Iteration 581 loss 1.354763\n",
      "Iteration 582 loss 1.358656\n",
      "Iteration 583 loss 1.351108\n",
      "Iteration 584 loss 1.339532\n",
      "Iteration 585 loss 1.341249\n",
      "Iteration 586 loss 1.342804\n",
      "Iteration 587 loss 1.331260\n",
      "Iteration 588 loss 1.327143\n",
      "Iteration 589 loss 1.318602\n",
      "Iteration 590 loss 1.315763\n",
      "Iteration 591 loss 1.310205\n",
      "Iteration 592 loss 1.302978\n",
      "Iteration 593 loss 1.299048\n",
      "Iteration 594 loss 1.296443\n",
      "Iteration 595 loss 1.284542\n",
      "Iteration 596 loss 1.289552\n",
      "Iteration 597 loss 1.280846\n",
      "Iteration 598 loss 1.262223\n",
      "Iteration 599 loss 1.279199\n",
      "Iteration 600 loss 1.264912\n",
      "Iteration 601 loss 1.262761\n",
      "Iteration 602 loss 1.258920\n",
      "Iteration 603 loss 1.242663\n",
      "Iteration 604 loss 1.253011\n",
      "Iteration 605 loss 1.247289\n",
      "Iteration 606 loss 1.235998\n",
      "Iteration 607 loss 1.234141\n",
      "Iteration 608 loss 1.226189\n",
      "Iteration 609 loss 1.228018\n",
      "Iteration 610 loss 1.220668\n",
      "Iteration 611 loss 1.218337\n",
      "Iteration 612 loss 1.213852\n",
      "Iteration 613 loss 1.210840\n",
      "Iteration 614 loss 1.202869\n",
      "Iteration 615 loss 1.199394\n",
      "Iteration 616 loss 1.181379\n",
      "Iteration 617 loss 1.192967\n",
      "Iteration 618 loss 1.187854\n",
      "Iteration 619 loss 1.182949\n",
      "Iteration 620 loss 1.180397\n",
      "Iteration 621 loss 1.172943\n",
      "Iteration 622 loss 1.168221\n",
      "Iteration 623 loss 1.166923\n",
      "Iteration 624 loss 1.159601\n",
      "Iteration 625 loss 1.155349\n",
      "Iteration 626 loss 1.153829\n",
      "Iteration 627 loss 1.150738\n",
      "Iteration 628 loss 1.145945\n",
      "Iteration 629 loss 1.141035\n",
      "Iteration 630 loss 1.132662\n",
      "Iteration 631 loss 1.136102\n",
      "Iteration 632 loss 1.131582\n",
      "Iteration 633 loss 1.115942\n",
      "Iteration 634 loss 1.124769\n",
      "Iteration 635 loss 1.119288\n",
      "Iteration 636 loss 1.114180\n",
      "Iteration 637 loss 1.112631\n",
      "Iteration 638 loss 1.107832\n",
      "Iteration 639 loss 1.101005\n",
      "Iteration 640 loss 1.091757\n",
      "Iteration 641 loss 1.095549\n",
      "Iteration 642 loss 1.094933\n",
      "Iteration 643 loss 1.088813\n",
      "Iteration 644 loss 1.083971\n",
      "Iteration 645 loss 1.083244\n",
      "Iteration 646 loss 1.075178\n",
      "Iteration 647 loss 1.075704\n",
      "Iteration 648 loss 1.070199\n",
      "Iteration 649 loss 1.063131\n",
      "Iteration 650 loss 1.058943\n",
      "Iteration 651 loss 1.048646\n",
      "Iteration 652 loss 1.058695\n",
      "Iteration 653 loss 1.052590\n",
      "Iteration 654 loss 1.045927\n",
      "Iteration 655 loss 1.044044\n",
      "Iteration 656 loss 1.046179\n",
      "Iteration 657 loss 1.034766\n",
      "Iteration 658 loss 1.037102\n",
      "Iteration 659 loss 1.031107\n",
      "Iteration 660 loss 1.028658\n",
      "Iteration 661 loss 1.016275\n",
      "Iteration 662 loss 1.024300\n",
      "Iteration 663 loss 1.018164\n",
      "Iteration 664 loss 1.015935\n",
      "Iteration 665 loss 1.009613\n",
      "Iteration 666 loss 1.005940\n",
      "Iteration 667 loss 1.005383\n",
      "Iteration 668 loss 1.005015\n",
      "Iteration 669 loss 0.996181\n",
      "Iteration 670 loss 0.997543\n",
      "Iteration 671 loss 0.994663\n",
      "Iteration 672 loss 0.989546\n",
      "Iteration 673 loss 0.989204\n",
      "Iteration 674 loss 0.984353\n",
      "Iteration 675 loss 0.980586\n",
      "Iteration 676 loss 0.978664\n",
      "Iteration 677 loss 0.970526\n",
      "Iteration 678 loss 0.973644\n",
      "Iteration 679 loss 0.970109\n",
      "Iteration 680 loss 0.961313\n",
      "Iteration 681 loss 0.962380\n",
      "Iteration 682 loss 0.960616\n",
      "Iteration 683 loss 0.957972\n",
      "Iteration 684 loss 0.954350\n",
      "Iteration 685 loss 0.950329\n",
      "Iteration 686 loss 0.947472\n",
      "Iteration 687 loss 0.946279\n",
      "Iteration 688 loss 0.941698\n",
      "Iteration 689 loss 0.939323\n",
      "Iteration 690 loss 0.939187\n",
      "Iteration 691 loss 0.933878\n",
      "Iteration 692 loss 0.931882\n",
      "Iteration 693 loss 0.928214\n",
      "Iteration 694 loss 0.926164\n",
      "Iteration 695 loss 0.921206\n",
      "Iteration 696 loss 0.921033\n",
      "Iteration 697 loss 0.918166\n",
      "Iteration 698 loss 0.914108\n",
      "Iteration 699 loss 0.913222\n",
      "Iteration 700 loss 0.906818\n",
      "Iteration 701 loss 0.908294\n",
      "Iteration 702 loss 0.903807\n",
      "Iteration 703 loss 0.902337\n",
      "Iteration 704 loss 0.898964\n",
      "Iteration 705 loss 0.892631\n",
      "Iteration 706 loss 0.895736\n",
      "Iteration 707 loss 0.891687\n",
      "Iteration 708 loss 0.887573\n",
      "Iteration 709 loss 0.883854\n",
      "Iteration 710 loss 0.885296\n",
      "Iteration 711 loss 0.880073\n",
      "Iteration 712 loss 0.875588\n",
      "Iteration 713 loss 0.874014\n",
      "Iteration 714 loss 0.875058\n",
      "Iteration 715 loss 0.872158\n",
      "Iteration 716 loss 0.868323\n",
      "Iteration 717 loss 0.859368\n",
      "Iteration 718 loss 0.863332\n",
      "Iteration 719 loss 0.860692\n",
      "Iteration 720 loss 0.859547\n",
      "Iteration 721 loss 0.857060\n",
      "Iteration 722 loss 0.854284\n",
      "Iteration 723 loss 0.846155\n",
      "Iteration 724 loss 0.847579\n",
      "Iteration 725 loss 0.845139\n",
      "Iteration 726 loss 0.844766\n",
      "Iteration 727 loss 0.841567\n",
      "Iteration 728 loss 0.838171\n",
      "Iteration 729 loss 0.833708\n",
      "Iteration 730 loss 0.834074\n",
      "Iteration 731 loss 0.829831\n",
      "Iteration 732 loss 0.829154\n",
      "Iteration 733 loss 0.828634\n",
      "Iteration 734 loss 0.825308\n",
      "Iteration 735 loss 0.822546\n",
      "Iteration 736 loss 0.821193\n",
      "Iteration 737 loss 0.818298\n",
      "Iteration 738 loss 0.815950\n",
      "Iteration 739 loss 0.814447\n",
      "Iteration 740 loss 0.812979\n",
      "Iteration 741 loss 0.810805\n",
      "Iteration 742 loss 0.806959\n",
      "Iteration 743 loss 0.804879\n",
      "Iteration 744 loss 0.801818\n",
      "Iteration 745 loss 0.801991\n",
      "Iteration 746 loss 0.798605\n",
      "Iteration 747 loss 0.793513\n",
      "Iteration 748 loss 0.794548\n",
      "Iteration 749 loss 0.792387\n",
      "Iteration 750 loss 0.790222\n",
      "Iteration 751 loss 0.788713\n",
      "Iteration 752 loss 0.785920\n",
      "Iteration 753 loss 0.785456\n",
      "Iteration 754 loss 0.779694\n",
      "Iteration 755 loss 0.781467\n",
      "Iteration 756 loss 0.777767\n",
      "Iteration 757 loss 0.776210\n",
      "Iteration 758 loss 0.772598\n",
      "Iteration 759 loss 0.773326\n",
      "Iteration 760 loss 0.769883\n",
      "Iteration 761 loss 0.768507\n",
      "Iteration 762 loss 0.764904\n",
      "Iteration 763 loss 0.763592\n",
      "Iteration 764 loss 0.762584\n",
      "Iteration 765 loss 0.760805\n",
      "Iteration 766 loss 0.757741\n",
      "Iteration 767 loss 0.756595\n",
      "Iteration 768 loss 0.753160\n",
      "Iteration 769 loss 0.752799\n",
      "Iteration 770 loss 0.750647\n",
      "Iteration 771 loss 0.748691\n",
      "Iteration 772 loss 0.746966\n",
      "Iteration 773 loss 0.744653\n",
      "Iteration 774 loss 0.742544\n",
      "Iteration 775 loss 0.740996\n",
      "Iteration 776 loss 0.740012\n",
      "Iteration 777 loss 0.737143\n",
      "Iteration 778 loss 0.734157\n",
      "Iteration 779 loss 0.733386\n",
      "Iteration 780 loss 0.731703\n",
      "Iteration 781 loss 0.729737\n",
      "Iteration 782 loss 0.727828\n",
      "Iteration 783 loss 0.727147\n",
      "Iteration 784 loss 0.724902\n",
      "Iteration 785 loss 0.722656\n",
      "Iteration 786 loss 0.720893\n",
      "Iteration 787 loss 0.719628\n",
      "Iteration 788 loss 0.717662\n",
      "Iteration 789 loss 0.715906\n",
      "Iteration 790 loss 0.714684\n",
      "Iteration 791 loss 0.708912\n",
      "Iteration 792 loss 0.710550\n",
      "Iteration 793 loss 0.708717\n",
      "Iteration 794 loss 0.707037\n",
      "Iteration 795 loss 0.705760\n",
      "Iteration 796 loss 0.703932\n",
      "Iteration 797 loss 0.698777\n",
      "Iteration 798 loss 0.700704\n",
      "Iteration 799 loss 0.699459\n",
      "Iteration 800 loss 0.697189\n",
      "Iteration 801 loss 0.695920\n",
      "Iteration 802 loss 0.692820\n",
      "Iteration 803 loss 0.690514\n",
      "Iteration 804 loss 0.689381\n",
      "Iteration 805 loss 0.688719\n",
      "Iteration 806 loss 0.686151\n",
      "Iteration 807 loss 0.685428\n",
      "Iteration 808 loss 0.683289\n",
      "Iteration 809 loss 0.682380\n",
      "Iteration 810 loss 0.680260\n",
      "Iteration 811 loss 0.679047\n",
      "Iteration 812 loss 0.674483\n",
      "Iteration 813 loss 0.676977\n",
      "Iteration 814 loss 0.671479\n",
      "Iteration 815 loss 0.673472\n",
      "Iteration 816 loss 0.671545\n",
      "Iteration 817 loss 0.668710\n",
      "Iteration 818 loss 0.667618\n",
      "Iteration 819 loss 0.666696\n",
      "Iteration 820 loss 0.663948\n",
      "Iteration 821 loss 0.663592\n",
      "Iteration 822 loss 0.661201\n",
      "Iteration 823 loss 0.660221\n",
      "Iteration 824 loss 0.655852\n",
      "Iteration 825 loss 0.657245\n",
      "Iteration 826 loss 0.655459\n",
      "Iteration 827 loss 0.654483\n",
      "Iteration 828 loss 0.650883\n",
      "Iteration 829 loss 0.651489\n",
      "Iteration 830 loss 0.648666\n",
      "Iteration 831 loss 0.648569\n",
      "Iteration 832 loss 0.646191\n",
      "Iteration 833 loss 0.643870\n",
      "Iteration 834 loss 0.644300\n",
      "Iteration 835 loss 0.642012\n",
      "Iteration 836 loss 0.641017\n",
      "Iteration 837 loss 0.638706\n",
      "Iteration 838 loss 0.636973\n",
      "Iteration 839 loss 0.637091\n",
      "Iteration 840 loss 0.635230\n",
      "Iteration 841 loss 0.634012\n",
      "Iteration 842 loss 0.632200\n",
      "Iteration 843 loss 0.629858\n",
      "Iteration 844 loss 0.628592\n",
      "Iteration 845 loss 0.628345\n",
      "Iteration 846 loss 0.626699\n",
      "Iteration 847 loss 0.624053\n",
      "Iteration 848 loss 0.623460\n",
      "Iteration 849 loss 0.621849\n",
      "Iteration 850 loss 0.620320\n",
      "Iteration 851 loss 0.619996\n",
      "Iteration 852 loss 0.617734\n",
      "Iteration 853 loss 0.614969\n",
      "Iteration 854 loss 0.615823\n",
      "Iteration 855 loss 0.614749\n",
      "Iteration 856 loss 0.611830\n",
      "Iteration 857 loss 0.612308\n",
      "Iteration 858 loss 0.610151\n",
      "Iteration 859 loss 0.609026\n",
      "Iteration 860 loss 0.607845\n",
      "Iteration 861 loss 0.606119\n",
      "Iteration 862 loss 0.605170\n",
      "Iteration 863 loss 0.603870\n",
      "Iteration 864 loss 0.601939\n",
      "Iteration 865 loss 0.600685\n",
      "Iteration 866 loss 0.599560\n",
      "Iteration 867 loss 0.597030\n",
      "Iteration 868 loss 0.594851\n",
      "Iteration 869 loss 0.596529\n",
      "Iteration 870 loss 0.594785\n",
      "Iteration 871 loss 0.594669\n",
      "Iteration 872 loss 0.590897\n",
      "Iteration 873 loss 0.590380\n",
      "Iteration 874 loss 0.590889\n",
      "Iteration 875 loss 0.586499\n",
      "Iteration 876 loss 0.587732\n",
      "Iteration 877 loss 0.586002\n",
      "Iteration 878 loss 0.584704\n",
      "Iteration 879 loss 0.584198\n",
      "Iteration 880 loss 0.582787\n",
      "Iteration 881 loss 0.579526\n",
      "Iteration 882 loss 0.578806\n",
      "Iteration 883 loss 0.578940\n",
      "Iteration 884 loss 0.577003\n",
      "Iteration 885 loss 0.577185\n",
      "Iteration 886 loss 0.574877\n",
      "Iteration 887 loss 0.573969\n",
      "Iteration 888 loss 0.571993\n",
      "Iteration 889 loss 0.572353\n",
      "Iteration 890 loss 0.570840\n",
      "Iteration 891 loss 0.569636\n",
      "Iteration 892 loss 0.568067\n",
      "Iteration 893 loss 0.566649\n",
      "Iteration 894 loss 0.566001\n",
      "Iteration 895 loss 0.564638\n",
      "Iteration 896 loss 0.563496\n",
      "Iteration 897 loss 0.561714\n",
      "Iteration 898 loss 0.561172\n",
      "Iteration 899 loss 0.560140\n",
      "Iteration 900 loss 0.558900\n",
      "Iteration 901 loss 0.557569\n",
      "Iteration 902 loss 0.554600\n",
      "Iteration 903 loss 0.556032\n",
      "Iteration 904 loss 0.554697\n",
      "Iteration 905 loss 0.553582\n",
      "Iteration 906 loss 0.550627\n",
      "Iteration 907 loss 0.551399\n",
      "Iteration 908 loss 0.550243\n",
      "Iteration 909 loss 0.547978\n",
      "Iteration 910 loss 0.546491\n",
      "Iteration 911 loss 0.546952\n",
      "Iteration 912 loss 0.545284\n",
      "Iteration 913 loss 0.543383\n",
      "Iteration 914 loss 0.543404\n",
      "Iteration 915 loss 0.542406\n",
      "Iteration 916 loss 0.541297\n",
      "Iteration 917 loss 0.538506\n",
      "Iteration 918 loss 0.539546\n",
      "Iteration 919 loss 0.538134\n",
      "Iteration 920 loss 0.536595\n",
      "Iteration 921 loss 0.535600\n",
      "Iteration 922 loss 0.534649\n",
      "Iteration 923 loss 0.533647\n",
      "Iteration 924 loss 0.531529\n",
      "Iteration 925 loss 0.532324\n",
      "Iteration 926 loss 0.529468\n",
      "Iteration 927 loss 0.529429\n",
      "Iteration 928 loss 0.528558\n",
      "Iteration 929 loss 0.528360\n",
      "Iteration 930 loss 0.527032\n",
      "Iteration 931 loss 0.525201\n",
      "Iteration 932 loss 0.524549\n",
      "Iteration 933 loss 0.523849\n",
      "Iteration 934 loss 0.521924\n",
      "Iteration 935 loss 0.521964\n",
      "Iteration 936 loss 0.519862\n",
      "Iteration 937 loss 0.519576\n",
      "Iteration 938 loss 0.518469\n",
      "Iteration 939 loss 0.517742\n",
      "Iteration 940 loss 0.516463\n",
      "Iteration 941 loss 0.515449\n",
      "Iteration 942 loss 0.514034\n",
      "Iteration 943 loss 0.513626\n",
      "Iteration 944 loss 0.512747\n",
      "Iteration 945 loss 0.511827\n",
      "Iteration 946 loss 0.510821\n",
      "Iteration 947 loss 0.509189\n",
      "Iteration 948 loss 0.507566\n",
      "Iteration 949 loss 0.507918\n",
      "Iteration 950 loss 0.506517\n",
      "Iteration 951 loss 0.505662\n",
      "Iteration 952 loss 0.504928\n",
      "Iteration 953 loss 0.504070\n",
      "Iteration 954 loss 0.503210\n",
      "Iteration 955 loss 0.501672\n",
      "Iteration 956 loss 0.501724\n",
      "Iteration 957 loss 0.500372\n",
      "Iteration 958 loss 0.499362\n",
      "Iteration 959 loss 0.498451\n",
      "Iteration 960 loss 0.497749\n",
      "Iteration 961 loss 0.496687\n",
      "Iteration 962 loss 0.495311\n",
      "Iteration 963 loss 0.494974\n",
      "Iteration 964 loss 0.494034\n",
      "Iteration 965 loss 0.492499\n",
      "Iteration 966 loss 0.491872\n",
      "Iteration 967 loss 0.491163\n",
      "Iteration 968 loss 0.490143\n",
      "Iteration 969 loss 0.489536\n",
      "Iteration 970 loss 0.488089\n",
      "Iteration 971 loss 0.487386\n",
      "Iteration 972 loss 0.486588\n",
      "Iteration 973 loss 0.486010\n",
      "Iteration 974 loss 0.484302\n",
      "Iteration 975 loss 0.483837\n",
      "Iteration 976 loss 0.483012\n",
      "Iteration 977 loss 0.481608\n",
      "Iteration 978 loss 0.481706\n",
      "Iteration 979 loss 0.480582\n",
      "Iteration 980 loss 0.479799\n",
      "Iteration 981 loss 0.478259\n",
      "Iteration 982 loss 0.476892\n",
      "Iteration 983 loss 0.477455\n",
      "Iteration 984 loss 0.475521\n",
      "Iteration 985 loss 0.475649\n",
      "Iteration 986 loss 0.473479\n",
      "Iteration 987 loss 0.472957\n",
      "Iteration 988 loss 0.473047\n",
      "Iteration 989 loss 0.472004\n",
      "Iteration 990 loss 0.471501\n",
      "Iteration 991 loss 0.470491\n",
      "Iteration 992 loss 0.468509\n",
      "Iteration 993 loss 0.468809\n",
      "Iteration 994 loss 0.467804\n",
      "Iteration 995 loss 0.467195\n",
      "Iteration 996 loss 0.466413\n",
      "Iteration 997 loss 0.465314\n",
      "Iteration 998 loss 0.464794\n",
      "Iteration 999 loss 0.463749\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return XX\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h2 = 5\n",
    "h1 = 6\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,1])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[1,1])]\n",
    "L = len(W)-1\n",
    "\n",
    "def act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return -((1-y) * np.log(1-yhat) + y * np.log(yhat))\n",
    "    \n",
    "# Use mini-batch size 1\n",
    "\n",
    "alpha = 0.01\n",
    "max_iter = 1000\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = XX[order[i],:].T\n",
    "        y_this = y[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            a.append(act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss(y_this, a[L][0,0])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "            \n",
    "        # Backprop step\n",
    "\n",
    "        delta[L] = a[L] - y_this\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                delta[l-1] = np.multiply(actder(z[l-1]), W[l] *\n",
    "                             delta[l])\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

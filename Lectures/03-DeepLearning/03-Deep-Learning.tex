\documentclass{beamer}

\mode<presentation>
{
  \setbeamertemplate{background canvas}[square]
  \pgfdeclareimage[width=6em,interpolate=true]{dsailogo}{../dsai-logo}
  \pgfdeclareimage[width=6em,interpolate=true]{erasmuslogo}{../erasmus-logo}
  \titlegraphic{\pgfuseimage{dsailogo} \hspace{0.2in} \pgfuseimage{erasmuslogo}}
  %\usetheme{default}
  \usetheme{Madrid}
  \usecolortheme{rose}
  \usefonttheme[onlysmall]{structurebold}
}

\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{amsmath,amssymb}
\usepackage{graphics}
\usepackage{ragged2e}
\usepackage{array}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{30mm}
\setlength{\TPVertModule}{\TPHorizModule}
\textblockorigin{10mm}{10mm}
\usepackage[english]{babel}
\usepackage{listings}
\setbeamercovered{dynamic}

\AtBeginSection[]{
  \begin{frame}<beamer>
  \frametitle{Outline}
  \tableofcontents[currentsection]
  \end{frame}
}

\title[Machine Learning]{Machine Learning\\Deep Learning}
\author{dsai.asia}
\institute[]{Asian Data Science and Artificial Intelligence Master's Program}
\date{}

% My math definitions

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\mathtt{#1}}
\newcommand{\ten}[1]{\mathcal{#1}}
\newcommand{\crossmat}[1]{\begin{bmatrix} #1 \end{bmatrix}_{\times}}
\newcommand{\nullsp}[1]{{\cal N}(#1)}
\newcommand{\class}[1]{{\cal C}_{#1}}
\def\Rset{\mathbb{R}}
\def\Pset{\mathbb{P}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\cov}{Cov}
\DeclareMathOperator*{\diag}{diag}
\def\norm{\mbox{$\cal{N}$}}

\newcommand{\stereotype}[1]{\guillemotleft{{#1}}\guillemotright}

\newcommand{\myfig}[3]{\centerline{\includegraphics[width={#1}]{{#2}}}
    \centerline{\scriptsize #3}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%             CONTENTS START HERE

%\setbeamertemplate{navigation symbols}{}

\frame{\titlepage}

%--------------------------------------------------------------------
%\part<presentation>{Part name}
%
%\frame{\partpage}

\begin{frame}
\frametitle{Readings}

Readings for these lecture notes:
\begin{itemize}
\item[-] Goodfellow, I., Bengio, Y., and Courville, A. (2016),
  \textit{Deep Learning}, MIT Press, Chapter 6.
\item[-] Bishop, C. (2006), \textit{Pattern Recognition and Machine Learning},
  Springer, Chapters 3, 4, 6, 7.
\item[-] Hastie, T., Tibshirani, R., and Friedman, J. (2016),
  \textit{Elements of Statistical Learning: Data Mining, Inference, and
    Prediction}, Springer, Chapters 2, 3, 4, 12.
\item[-] Ng, A. (2017), \textit{Deep Learning},
  Lecture note set for CS229, Stanford University.
\end{itemize}

These notes contain material $\copyright$ Bishop (2006), Hastie et
al.\ (2016), Goodfellow et al.\ (2016), and Ng (2017).

\end{frame}

%======================================================================
\section{Introduction}
%======================================================================

\begin{frame}[fragile]
\frametitle{Introduction}

  Deep learning involves the training of so-called \alert{neural
    networks}.

  \medskip

  In this unit, we'll study how neural networks work and understand
  basic neural network learning algorithms.

  \medskip

  Then we'll briefly cover some of the modern neural network
  architectures that are generating so much layperson interest in AI
  today.

\end{frame}

%======================================================================
\section{Neural network intuition}
%======================================================================

\begin{frame}{Neural network intuition}{Basic units}

  To begin with, we'll consider the univariate regression setting
  where we want to learn a function $f : x \mapsto y$.

  \medskip

  A simple neural network can represent $f(x)$ by a single
  \alert{neuron} or \alert{unit} that computes
  \[ f(x) = \max(ax+b,0) \]
  for some fixed coefficients $a$ and $b$.

  \medskip

  This particular unit
  is called a \alert{ReLU} (rectified linear unit).

\end{frame}


\begin{frame}{Neural network intuition}{Composing units into networks}

  By stacking such units so that one passes its output to another, we can
  model increasingly complex functions.

  \medskip

  From Ng's course notes, consider the example in which we want to
  predict \alert{house price} given size, number of bedrooms, postal
  code, and wealth of the neighborhood the house is in.

  \medskip

  We could build the model up incrementally, having it compute
  a ``family size'' variable based on the house size and number of bedrooms.

  \medskip

  The postal code could be used to compute how ``walkable'' the neighborhood
  is.

  \medskip

  Combining the zip code with neighborhood wealth might predict
  ``school quality.''

\end{frame}


\begin{frame}{Neural network intuition}{Composing units into networks}

  We might finally decide that the price depends on these three derived
  features: family size, walkable, and school quality:

  \medskip

  \myfig{2.5in}{ng-nn}{Ng (2017), CS229 deep learning lecture notes.}

\end{frame}


\begin{frame}{Neural network intuition}{Composing units into networks}

  Another example: \alert{loan application underwriting}.

  \medskip

  One important attribute of a loan applicant is his/her \alert{income}.

  \medskip

  But high income is meaningless if the applicant's \alert{debt} is very
  high.

  \medskip

  Loan underwriters combine these two features into a higher level feature,
  \alert{debt-to-income ratio}.

  \medskip

  43\% is considered a ``magic'' tolerable debt-to-income ratio for a family.

\end{frame}


\begin{frame}{Neural network intuition}{End-to-end learning}

  See any problems with this architecture as we described it so far?

  \medskip

  Luckily, we don't need to solve those problems, as neural network
  learning is \alert{end-to-end learning}.

  \medskip

  That means \alert{the network figures out for itself what
    intermediate features are best for the task at hand}.

\end{frame}


\begin{frame}{Neural network intuition}{Hidden units}

  Intermediate units between the raw inputs and the output
  are called \alert{hidden units}.

  \medskip

  Example suppose we have:
  \begin{itemize}
  \item Four inputs $x_1$, $x_2$, $x_3$, and $x_4$
  \item Three hidden units
  \item A single output $y$
  \end{itemize}

  \medskip
  
  The goal of the network will be to \alert{find} intermediate
  features that will \alert{best predict} each $y^{(i)}$ from the
  corresponding $\vec{x}^{(i)}$.

  \medskip

  It may be difficult to understand the ``meaning'' of the intermediate
  features thus induced.

  \medskip

  Neural networks are therefore some times called \alert{black boxes}.

\end{frame}


\begin{frame}{Neural network intuition}{Some terminology}

  Here are the terms we've seen so far, and some new ones:
  \begin{itemize}
  \item A \alert{neuron} or \alert{unit} applies some function to its
    inputs to generate an output.
  \item Units may be composed into \alert{neural networks}.
  \item Input features are sometimes represented by units called
    \alert{input units} organized into a \alert{input layer}.
  \item One or more outputs comprise the \alert{output layer}.
  \item Intermediate units are called \alert{hidden units} and may be
    organized into zero or more \alert{hidden layers}.
  \end{itemize}

\end{frame}


\begin{frame}{Neural network intuition}{Notation}

  Suppose we have an input layer composed of features $x_1, x_2, \ldots$

  \medskip

  Ng uses the notation $a_i^{[j]}$ to indicate the \alert{activation}
  of the $i$th unit in the $j$th layer.

  \medskip

  $a_1^{[1]}$ is the output of the first hidden unit in the first hidden layer.

  \medskip

  $a_1^{[2]}$ is the output of the first unit in the second layer
  (the output layer in a network with only one hidden layer).

  \medskip

  To unify the notation, we let $a_i^{[0]} = x_i$, i.e., we treat
  the input as layer 0.

\end{frame}


\begin{frame}{Neural network intuition}{Activation functions}

  We saw a simple ReLU network already.

  \medskip

  \alert{Logistic regression} can also be treated as a simple neural network
  with one output unit and no hidden units:
  \[ g(\vec{x}) = \frac{1}{1+e^{-\vec{w}^\top\vec{x}}} \]
  is written in standard neural network notation as two steps:
  \begin{enumerate}
  \item Calculate the \alert{linear response} $z = \vec{w}^\top\vec{x}+b$.
  \item Calculate the \alert{activation function} $a = \sigma(z)$ where
    $\sigma(z) = 1 / (1+e^{-z})$.
  \end{enumerate}

\end{frame}


\begin{frame}{Neural network intuition}{Activation functions}

  Usually, $g(z)$ is nonlinear. The most common activation functions:
  \begin{itemize}
  \item The sigmoid $g(z) = \frac{1}{1+e^{-z}}$
  \item ReLU (the default activation function in modern neural
    networks) $g(z) = \max(z,0)$
  \item Hyperbolic tangent $g(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$
  \end{itemize}

\end{frame}


\begin{frame}{Neural network intuition}{Full calculation}

  The full calculation proceeds as follows.

  \medskip

  For the first hidden unit in the
  first hidden layer, we calculate
  \[ z_1^{[1]} = \mat{W}_1^{[1]\top}\vec{x} + b_1^{[1]} \]
  and
  \[ a_1^{[1]} = g(z_1^{[1]}), \]
  where $\mat{W}$ is a matrix or parameters or weights.

  \medskip

  We repeat for each unit in each layer to get the final output
  layer.

\end{frame}


\begin{frame}{Neural network intuition}{What remains?}

  Now that we understand the feed-forward computation of a neural network,
  we'll talk about
  \begin{itemize}
  \item \alert{Efficient execution} of the feed-forward computation
  \item The \alert{gradient descent process} used to learn the weights
    $\mat{W}$.
  \end{itemize}
  
\end{frame}

%======================================================================
\section{Efficient computation}
%======================================================================

\begin{frame}{Efficient computation}{Calculating activations}

  Consider calculating hidden unit activations. We have
  $$\begin{matrix}
    z_1^{[1]} = \mat{W}_1^{[1]\top}\vec{x} + b_1^{[1]} & \text{and} & a_1^{[1]} = g(z_1^{[1]}) & \hfill \\
    \vdots & \vdots & \vdots & \hfill \\
    z_4^{[1]} = \mat{W}_4^{[1]\top}\vec{x} + b_4^{[1]} & \text{and} & a_4^{[1]} = g(z_4^{[1]}) & \hfill
  \end{matrix}$$

  Depending on the ``deepness'' of our model, for a single input, we
  may be doing an operation like this for hundreds or thousands of
  units.

  \medskip

  Code to implement this procedure using \texttt{for} loops and the
  like will run \alert{very slowly}, especially if implemented in a
  bytecode based language like Python or Java.

\end{frame}

  
\begin{frame}[fragile]{Efficient computation}{BLAS library}

  What is needed is the ability to perform matrix algebra with a
  single library call or instruction that is highly optimized, using
  CPU instructions provided for \alert{vector operations}.

  \medskip
  
  \alert{BLAS} is a well-known library that does this and is used by
  numpy:

\begin{tiny}
\begin{verbatim}
ldd /usr/lib/python3/dist-packages/numpy/core/multiarray.cpython-35m-x86_64-linux-gnu.so
	linux-vdso.so.1 =>  (0x00007fff40fde000)
	libblas.so.3 => /usr/lib/libblas.so.3 (0x00007f9a14579000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f9a14270000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f9a14053000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f9a13c89000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f9a14b69000)
	libopenblas.so.0 => /usr/lib/libopenblas.so.0 (0x00007f9a11bf5000)
	libgfortran.so.3 => /usr/lib/x86_64-linux-gnu/libgfortran.so.3 (0x00007f9a118ca000)
	libquadmath.so.0 => /usr/lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f9a1168b000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f9a11475000)
\end{verbatim}
\end{tiny}

  \medskip

  Alternatively, we may parallelize computation by recruiting GPU resources.

\end{frame}

  
\begin{frame}[fragile]{Efficient computation}{Vectorized activation calculation}

  For an entire layer, to use vectorized computation, we need to perform
  the operation in one operation:

  \[ \vec{z}^{[1]} = \mat{W}^{[1]\top} \vec{x} + \vec{b}^{[1]} \]

  This can be implemented in a single Python statement:

\begin{verbatim}
W1 = np.matrix(np.random.normal(0,1,(3,4)))
b1 = np.matrix(np.random.normal(0,1,(4,1)))
x = np.matrix(np.random.normal(0,1,(3,1)))
z1 = W1.T * x + b1
\end{verbatim}

  This would run much faster than the equivalent doubly-nested
  \texttt{for} loop.

\end{frame}

  
\begin{frame}[fragile]{Efficient computation}{Vectorized activation calculation}

  Then to calculate $\vec{a}^{[1]}$ as a vector operation, we can
  hopefully use vectorized functions in our implementation language.

  \medskip

  If we have the sigmoid function, for example, we implement
  \[ g(z_i^{[1]}) = \frac{1}{1+e^{-z_i^{[1]}}} \]
  as
  \begin{verbatim}
def g(z):
    return 1 / (1 + exp(-z))
z = np.matrix([[1,2,3]]).T
a = g(z)
  \end{verbatim}

  This will run much faster than executing the \texttt{exp()} function
  within a Python loop.
  
\end{frame}

  
\begin{frame}[fragile]{Efficient computation}{Vectorizing over training examples}

  Next, consider performing a calculation over \alert{many training examples}.

  \medskip

  Performing the operation
  \[ \vec{z}^{[1]} = \mat{W}^{[1]\top} \vec{x}^{(i)} + \vec{b}^{[1]} \]
  inside a loop for every training example $i$ would be slower than the
  vectorized operation
  \[ \vec{z}^{[1]} = \mat{W}^{[1]\top} \mat{X}^\top + \vec{b}^{[1]}. \]
  Note: despite different dimensions of
  $\mat{W}^{[1]\top} \mat{X}^\top$ and $\vec{b}^{[1]}$,
  some languages like Python allow \alert{broadcasting} of the addition
  operation horizontally:
  \begin{verbatim}
>>> W = np.matrix([[1,2,3],[2,3,4]])
>>> b = np.matrix([[2,3]]).T
>>> W+b
matrix([[3, 4, 5],
        [5, 6, 7]])
\end{verbatim}  

\end{frame}

%======================================================================
\section{Backpropagation}
%======================================================================

\begin{frame}{Backpropagation}{Introduction}

  Now that we understand how the feedforward computation of a neural
  network works and how to use optimized vector operations, let's consider
  how to train a neural network.

  \medskip

  The general procedure is called \alert{backpropagation}.

  \medskip
  
  \begin{columns}

    \column{1.6in}
    
  \framebox{\includegraphics[width=1.5in]{soccer}}

  \column{2.9in}

  We'll use Ng's (2017) example of classifying an image as containing a
  soccer ball or not.

  \end{columns}
  
\end{frame}


\begin{frame}{Backpropagation}{Flattening the input}

  First we'll scale the image to a standard size, for example 64 $\times$ 64.

  \begin{center}    
    \framebox{\includegraphics[width=0.75in]{soccer64}}
  \end{center}

  Then we'll \alert{flatten} the 64 $\times$ 64 $\times$ 3 elements
  of the input to a 12,288-element vector and present to our neural network:

  \medskip
  
  \myfig{3.5in}{soccer-flatten}{Ng (2017), CS 229 Lecture notes on deep learning}
  
\end{frame}


\begin{frame}{Backpropagation}{Model, architecture, parameters}

  Some terminology a neural network \alert{model} consists of
  \begin{itemize}
  \item The network \alert{architecture} (number of layers, units, type of units, connectivity between units),
  \item The \alert{parameters} (the values of the weights $\mat{W}^{[i]}$ and
    $\vec{b}^{[i]}$.
  \end{itemize}

  \medskip

  In the forthcoming analysis, we'll assume a 3-layer network architecture

  \myfig{2.5in}{three-layer}{Ng (2017), Deep learning lecture notes for CS 229.}

  The architecture consists of two \alert{fully connected layers}
  followed by a single \alert{logistic sigmoid output}. Now we
  consider how to learn the $3n+14$ parameters of the model through
  backpropagation.

\end{frame}


\begin{frame}{Backpropagation}{Parameter initialization}

  First step: \alert{set initial values of the parameters}.

  \medskip

  Initializing to 0 would be a bad idea, because the output of each layer
  would be identical for every unit, and the gradients backpropagated
  later would also be identical.

  \medskip

  Solution: randomly initialize parameters to small values close to 0,
  e.g., \[ w^{[i]}_{jk} \sim {\cal N}(0,0.1). \]

  A better method in practice is called Xavier/He initialization:
  \[ w^{[i]}_{jk} \sim {\cal N}\left(0,\sqrt{\frac{2}{n^{[i]}+n^{[i-1]}}} \right), \]
  where $n^{[i]}$ is the number of units in layer $i$. This encourages
  the variance of the outputs of a layer to be similar to the variance
  of the inputs.
  
\end{frame}


\begin{frame}{Backpropagation}{Parameter initialization}

  Note that for \alert{ReLU hidden units}, the recommendation for the
  \alert{bias weights} is to use a small \alert{positive} (even
  constant) initial value.

  \medskip

  This ensures that the unit's output is initially positive for most
  training examples.
  
\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  In the case of a single logistic sigmoid at the output layer, after
  forward propagation, we have a predicted value $\hat{y}$.

  \medskip

  Neural network parameter update rules are usually derived in terms of
  backpropagating an \alert{error} or \alert{loss}.

  \medskip

  If we have an objective function such as
  maximum likelihood, we can convert to a loss by \alert{negating} it.

  \medskip

  We thus get the \alert{log loss} function for a network with a single
  logistic sigmoid output:
  \[ {\cal L}(\hat{y},y) = - \left[(1-y)\log(1-\hat{y})+y\log\hat{y}\right]. \]

  Note that it is easy to do the same for a linear output (Gaussian
  distribution for $y$) or softmax ouptut (multinomial distribution
  for $y$).\footnote{See Goodfellow et al.\ (2016) Section 6.2.2.4 for
    discussion of other output types.}

\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  Now, to update the parameters in layer $l$, we update using
  \alert{gradient descent} on the log loss:
  \begin{eqnarray*}
    \mat{W}^{[l]} & \leftarrow & \mat{W}^{[l]} - \alpha \frac{\partial {\cal L}}{\partial \mat{W}^{[l]}} \\
    \vec{b}^{[l]} & \leftarrow & \vec{b}^{[l]} - \alpha \frac{\partial {\cal L}}{\partial \vec{b}^{[l]}}
  \end{eqnarray*}
  
\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  First we consider the weights at the output layer. We have
  \begin{eqnarray*}
    \frac{\partial {\cal L}}{\partial \mat{W}^{[3]}} & = &
    - \frac{\partial}{\partial \mat{W}^{[3]}} \left( (1-y)\log(1-\hat{y})+y \log \hat{y} \right) \\
    & = & -(1-y)\frac{\partial}{\partial \mat{W}^{[3]}} \log
    \left( 1 - g(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]}) \right) \\
    & & \;\;\;\;\; - y \frac{\partial}{\partial \mat{W}^{[3]}} \log \left(
    g(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]} ) \right) \\
    & = & \frac{(1-y)g'(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]})\vec{a}^{[2]\top}}{1 - g(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]})} \\
    & & \;\;\;\;\; -\frac{y g'(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]})\vec{a}^{[2]\top}}{g(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]} )}
  \end{eqnarray*}

\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  Continuing, we note that for this model, $g(z)$ is the logistic
  sigmoid.

  \medskip

  Let's replace $g(z)$ with $\sigma(z)$ and $g'(z)$ with
  $\sigma'(z)$ to make this clear.

  \medskip

  We also recall that $\sigma'(z) = \sigma(z)(1-\sigma(z))$, allowing
  us to simplify the expression:
  \begin{eqnarray*}
    \frac{\partial {\cal L}}{\partial \mat{W}^{[3]}} & = & ... \\
    & = & (1-y)\sigma(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]})\vec{a}^{[2]\top}    -y (1-\sigma(\mat{W}^{[3]}\vec{a}^{[2]} + \vec{b}^{[3]}))\vec{a}^{[2]\top}\\
    & = & (1-y)a^{[3]}\vec{a}^{[2]\top}-y (1-a^{[3]})\vec{a}^{[2]\top} \\
    & = & (a^{[3]}-y)\vec{a}^{[2]\top}.
  \end{eqnarray*}

\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  What about the weights for layer 2?

  \medskip

  We can use the chain rule from calculus. When we have a function $f(z)$
  where $z = g(x)$, we can write
  \[ \frac{\partial f}{\partial x} =
  \frac{\partial f}{\partial g}\frac{\partial g}{\partial x} \]
  In our case we have
  \[ \frac{\partial {\cal L}}{\partial \mat{W}^{[2]}} =
  \frac{\partial \cal L}{\partial a^{[3]}}
  \frac{\partial a^{[3]}}{\partial \vec{z}^{[3]}}
  \frac{\partial \vec{z}^{[3]}}{\partial \vec{a}^{[2]}}
  \frac{\partial \vec{a}^{[2]}}{\partial \vec{z}^{[2]}}
  \frac{\partial \vec{z}^{[2]}}{\partial \mat{W}^{[2]}} . \]
  
\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  To evaluate this expression, let's try to reuse what we already
  have for $\frac{\partial \cal L}{\partial \mat{W}^{[3]}}$ first:
  \[ \frac{\partial \cal L}{\partial \mat{W}^{[3]}} =
  \frac{\partial \cal L}{\partial a^{[3]}}
  \frac{\partial a^{[3]}}{\partial \vec{z}^{[3]}}
  \frac{\partial \vec{z}^{[3]}}{\partial \mat{W}^{[3]}} =
  (a^{[3]}-y)\vec{a}^{[2]} \]
  
  we can reuse the part
  \[ \frac{\partial \cal L}{\partial \vec{z}^{[3]}} = \frac{\partial \cal L}{\partial a^{[3]}}
  \frac{\partial a^{[3]}}{\partial \vec{z}^{[3]}} = 
  a^{[3]}-y. \]

  For the remaining terms, we have
  \begin{eqnarray*}
    \frac{\partial \vec{z}^{[3]}}{\partial \vec{a}^{[2]}} & = & \mat{W}^{[3]}\\
    \frac{\partial \vec{a}^{[2]}}{\partial \vec{z}^{[2]}} & = & g'(\vec{z}^{[2]})\\
    \frac{\partial \vec{z}^{[2]}}{\partial \mat{W}^{[2]}} & = & \vec{a}^{[1]}.
  \end{eqnarray*}
    
\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  Putting the chain rule terms together in an order appropriate for
  vector calculations, we obtain
  \[ \frac{\partial \cal L}{\partial \mat{W}^{[2]}} =
  \diag(g'(\vec{z}^{[2]}))
  \mat{W}^{[3]}
  (a^{[3]}-y)\vec{a}^{[1]\top}. \]

  \medskip
  
  The calculation is also similar for the bias weights, except that
  in place of $\vec{a}^{[1]}$ we have 1.

\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  What about the weights for \alert{layer 1}?

  \medskip
  
  We want
  \[ \frac{\partial \cal L}{\partial w^{[1]}_{ij}}. \]
  We can readily see that $w^{[1]}_{ij}$ affects \alert{all} of the
  second layer activations $\vec{a}^{[2]}$.

  \medskip

  In this case, the applicable more general chain rule,
  when $y = f(\vec{u})$ and $\vec{u} = g(\vec{x})$ is 
  \[ \frac{\partial y}{\partial x_i} = \sum_j \frac{\partial y}{\partial u_j}
  \frac{\partial u_j}{\partial x_i}. \]

\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  In our case, the generalized chain rule gives  
  \[ \frac{\partial \cal L}{\partial w^{[1]}_{ij}} = \sum_k
  \frac{\partial \cal L}{\partial a^{[2]}_k}
  \frac{\partial a^{[2]}_k}{\partial w^{[1]}_{ij}} . \]
  
  \medskip

  Expanding the two terms within the summation, we obtain
  \[ \frac{\partial \cal L}{\partial w^{[1]}_{ij}} = \sum_k
  \frac{\partial \cal L}{\partial a^{[3]}}
  \frac{\partial a^{[3]}}{\partial z^{[3]}}
  \frac{\partial z^{[3]}}{\partial a^{[2]}_k}
  \frac{\partial a^{[2]}_k}{\partial z^{[2]}_k}
  \frac{\partial z^{[2]}_k}{\partial a^{[1]}_j}
  \frac{\partial a^{[1]}_j}{\partial z^{[1]}_j}
  \frac{\partial z^{[1]}_j}{\partial w^{[1]}_{ij}} . \]
  
\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  Getting complicated, right?

  \medskip

  The key to solving the problem efficiently is realizing that the term
  \[ \frac{\partial \cal L}{\partial a^{[3]}}
  \frac{\partial a^{[3]}}{\partial z^{[3]}}
  \frac{\partial z^{[3]}}{\partial a^{[2]}_k}
  \frac{\partial a^{[2]}_k}{\partial z^{[2]}_k} \]
  has already been calculated, in the process of determining
  \[ \frac{\partial \cal L}{\partial w^{[2]}_{jk}} ! \]
  Really, go back and check.
  Now we see that we generally want to reuse terms that look like
  $\frac{\partial \cal L}{\partial z^{[l]}_i}$.

  \medskip
  
  Let's therefore define
  \[ \delta^{[l]}_i = \frac{\partial \cal L}{\partial z^{[l]}_i} .\]
  We then obtain the backpropagation algorithm for our sample network...


\end{frame}


\begin{frame}{Backpropagation}{Parameter update}

  \begin{tabbing}
    xxx \= xxx \= xxx \= \kill
    \underline{\textbf{Backpropagation (fully-connected network)}} \\
    Given example $(\vec{x},y)$:\\
    \> $\vec{a}^{[0]} \leftarrow \vec{x}.$ \\
    \> for $l = 1..L$ do \\
    \> \> $\vec{z}^{[l]} \leftarrow \mat{W}^{[l]} \vec{a}^{[l-1]} + \vec{b}^{[l]}.$ \\
    \> \> $\vec{a}^{[l]} \leftarrow g^{[l]}(\vec{z}^{[l]})$. \\
    \> $\vec{\delta}^{[L]} \leftarrow \frac{\partial \cal L}{\partial \vec{z}^{[L]}}.$ \\
    \> for $l = L..1$ do\\
    \> \> $\frac{\partial \cal L}{\partial \mat{W}^{[l]}} \leftarrow \vec{\delta}^{[l]} \vec{a}^{[l-1]\top}.$ \\
    \> \> $\frac{\partial \cal L}{\partial \vec{b}^{[l]}} \leftarrow \vec{\delta}^{[l]}$.\\
    \> \> if $l > 1$ then\\
    \> \> \> $\vec{\delta}^{[l-1]} \leftarrow
    \diag(g'(\vec{z}^{[l-1]}))
    \mat{W}^{[l]}
    \vec{\delta}^{[l]}$
  \end{tabbing}

\end{frame}


\begin{frame}{Backpropagation}{Stochastic vs.\ batch gradient descent}

  Thus far, the derviation was for a single $(\vec{x},y)$ pair.

  \medskip

  What about batch gradient descent? We would use the rule
  \[ \mat{W}^{[l]} \leftarrow \mat{W}^{[l]} - \alpha \frac{\partial J}{\partial \mat{W}^{[l]}}, \]
  where $J$ is the cost function
  \[ J = \frac{1}{m}\sum_{i=1}^m {\cal L}^{(i)} \]
  and ${\cal L}^{(i)}$ is the loss for a single example.

  \medskip

  Stochastic gradient descent will be more noisy but will usually
  converge faster than batch gradient descent.

\end{frame}


\begin{frame}{Backpropagation}{Mini-batch gradient descent}

  Since batch gradient descent is more accurate but slower to get
  moving than stochastic gradient descent, a common compromise is
  \alert{mini-batch gradient descent}.

  \medskip

  The mini-batch is a compromise between the accuracy of batch and the
  the speed of stochastic gradient descent. We split the data set into
  partitions $B_1, B_2, \ldots$ (or repeatedly sample uniformly from
  the full training set) and let
  \[ J_{i} = \frac{1}{|B_i|}\sum_{j \in B_i}{\cal L}^{(j)}. \]

\end{frame}


\begin{frame}{Backpropagation}{Momentum}

  Another common optimization is called \alert{momentum}.

  \medskip

  The problem is that sometimes noisy data make us jump back and forth across valleys in the loss function, leading to slow convergence.

  \medskip

  With momentum, we remember the last update to each parameter and use
  it to calculate a moving average of the gradient over time.

  \medskip

  We use the following update rule:
  \begin{eqnarray*}
    \mat{V}_{dW^{[l]}} & \leftarrow & \beta \mat{V}_{dW^{[l]}}+ (1-\beta)
    \frac{\partial J}{\partial \mat{W}^{[l]}} \\
    \mat{W}^{[l]} & \leftarrow & \mat{W}^{[l]} - \alpha \mat{V}_{dW^{[l]}}
  \end{eqnarray*}

  The momentum term $\beta$ will encourage the optimization to
  accelerate gradually toward the minimum.
    
\end{frame}


\begin{frame}{Backpropagation}{Adam optimization}

  For a more sophisticated version of momentum that is adaptive,
  take a look at the famous ICLR 2015 paper on \alert{Adam}:
  
  \medskip

  Kingma, D.P.\ and Ba, J.L.\ (2015), Adam: A Method for
  Stochastic Optimization. In \textit{International Conference on Learning
    Representations}.

  \medskip

  Adam usually outperforms other optimization methods such as SGD with
  momentum or RMSProp when applied to large NN models.

\end{frame}

%======================================================================
\section{Avoiding overfitting}
%======================================================================

\begin{frame}{Avoiding overfitting}{Overfitting}

  Neural networks are \alert{universal approximators}.

  \medskip

  Roughly speaking, given enough training data and sufficient model
  complexity, backpropgation can learn \alert{any function} to an
  arbitrary level of accuracy.

  \medskip

  But when model complexity is too high for the training set size, we
  observe \alert{overfitting}: training accuracy is high, but
  validation set/test set accuracy is substantially lower.

  \medskip

  Solutions:
  \begin{itemize}
  \item Decrease model complexity (remove hidden units, remove layers)
  \item Collect more training data
  \item Regularization
  \end{itemize}
  
\end{frame}


\begin{frame}{Avoiding overfitting}{Regularization}

  Let $\vec{w}$ denote a single vector containing the set of all
  parameters in our model (every $w^{[l]}_{ij}$ and $b^{[l]}_i$).

  \medskip

  Let $J$ be the model cost function.

  \medskip

  L2 regularization adds a new term to the cost function:
  \begin{eqnarray*}
    J_{L2} & = & J + \frac{\lambda}{2}\|\vec{w}\|^2 \\
    & = & J + \frac{\lambda}{2}\vec{w}^\top\vec{w}
  \end{eqnarray*}

  \medskip

  Why do this?
  \begin{itemize}
  \item $\lambda=0$ gives us unregularized parameter learning.
  \item Big $\lambda$ encourages solutions with small $\vec{w}$,
    especially, \alert{as many 0 elements as possible}.
  \end{itemize}

  Forcing less useful parameters to 0 decreases model complexity and
  will reduce overfitting.
  
\end{frame}


\begin{frame}{Avoiding overfitting}{Regularization}

  How does the regularizer affect the learning rule?

  \medskip

  Previously, we had
  \[ \vec{w} \leftarrow \vec{w} - \alpha \frac{\partial J}{\partial \vec{w}}. \]
  Now we have
  \begin{eqnarray*}
    \vec{w} & \leftarrow & \vec{w} - \alpha \frac{\partial J}{\partial \vec{w}} - \alpha \frac{\lambda}{2} \frac{\partial \vec{w}^\top\vec{w}}{\partial \vec{w}} \\
    & = & (1 - \alpha \lambda)\vec{w} - \alpha \frac{\partial J}{\partial \vec{w}}
  \end{eqnarray*}
  Think about what this means ($\alpha$ and $\lambda$ are both small
  positive reals).

  \medskip

  On each update, we make all the weights' magnitudes a little
  smaller. Only the most important weights will survive.
  
\end{frame}


\begin{frame}{Avoiding overfitting}{Parameter sharing}

  Let's return to the soccer ball image recognition problem.

  \medskip

  If we used a single logistic regression model for the
  64$\times$64$\times$3 = 12,288 parameters of the model, we'd have to
  train with soccer balls in \alert{all possible positions in the image}.

  \medskip

  Such a model would fail if it encountered a ball in a position it
  had never seen during training.
  
  \medskip

  One solution is \alert{parameter sharing}: each unit in the hidden
  layer looks at a different overlapping sub-region of the image, but
  these units \alert{share the same weights}.

\end{frame}


\begin{frame}{Avoiding overfitting}{Parameter sharing}

  \myfig{2.5in}{soccer-sharing}{Ng (2017), CS229 lecture notes}

  \medskip

  We might draw $\vec{\theta}$ from $\Rset^{4\times 4\times 3}$,
  meaning we have one weight for each of the R, G, and B pixel
  intensities in a 4$\times$4 region.

\end{frame}


\begin{frame}{Avoiding overfitting}{Parameter sharing}

  To learn $\vec{\theta}$:
  \begin{itemize}
    \item We might \alert{slide} the region of interest over each
      positive training image to create many positive 48-element
      training vectors.
    \item Or we might represent each element in the ``convolution'' as
      a separate unit but \alert{tie} their weights together during
      backpropagation.
  \end{itemize}

  \medskip
  
  The convolution style of weight sharing is the basic idea of the
  \alert{convolutional neural network} (CNN):
  \begin{itemize}
  \item Inspired by Hubel and Weisel's model of the responses of
    neurons in the cat's visual cortex to visual stimuli.
  \item Inspired by Fukushima's Neocognitron (1988).
  \end{itemize}
  
\end{frame}


\begin{frame}{Avoiding overfitting}{Parameter sharing}

  \begin{columns}

    \column{2in}
    
    CNNs:
    \begin{itemize}
    \item Applied successfully to handwritten character recognition by
      LeCun and colleagues (LeNet 5, 1998).
    \item Won the ImageNet Large Scale Visual Recognition Challenge in 2012 and
      every ImageNet competition since then.
    \item Jump started the recent round of hype in machine learning and AI!
    \end{itemize}

    \medskip

    \column{2.5in}
    
    \myfig{2.5in}{gartner-2020}{Gartner hype cycle (2020)}

  \end{columns}
  
\end{frame}

%--------------------------------------------------------------------
\section{CNNs for image classification}
%--------------------------------------------------------------------

\begin{frame}{CNNs for image classification}

Convolutional neural networks (CNNs) are suited for a variety of tasks
that involve forming a \alert{big picture} view of data through
\alert{hierarchical synthesis of local spatial and/or temporal
  relationships}.

\medskip

The most obvious type of data we need to do this with is
\alert{images}.  In computer vision, the three primary tasks are
\begin{itemize}
\item Image classification
\item Object detection
\item Image segmentation
\item Object tracking
\end{itemize}
These problems vexed computer vision researchers for decades, but due
to recent deep learning methods we may consider the first two problems
\alert{solved}!

\end{frame}


\begin{frame}
  \frametitle{CNNs for image classification}
  \framesubtitle{Supervised learning problems in computer vision}

  Among these four applications, those involving supervised machine
  learning are \alert{classification}, \alert{detection}, and
  \alert{segmentation}.

  \medskip

  \alert{Detection is really just classification}:
  \begin{itemize}
  \item We may classify each subwindow of an image sequentially using
    a sweep window.
  \item We may look for ``interesting'' locations first using some
    kind of interest operator.
  \item Or, we may classify all possible locations in parallel.
  \end{itemize}

\end{frame}


\begin{frame}{CNNs for image classification}{Supervised learning problems
  in computer vision}

  Similarly, \alert{segmentation is just classification}:
  \begin{itemize}
  \item We may classify each \alert{pixel} as a member of the same
    segment as its neighbors or a different segment.
  \item In \alert{semantic segmentation}, we attach a specific
    \alert{category label} to each pixel of the image.
  \end{itemize}
  
  \medskip

  The key in all of these cases is to be able to classify an image or
  a patch of an image or a pixel of an image.

  \medskip

  We'll thus first look at the state of the art in image classification
  then see how detection and segmentation can be done.

\end{frame}


\begin{frame}{CNNs for image classification}{Continuous convolution}

  \alert{Convolutional neural networks} (CNNs) are good for data with
  a known grid-like topology:
\begin{itemize}
\item Time series form 1-D grids.
\item Images form 2-D grids.
\end{itemize}

Mathematically: convolution is an operation on \alert{two functions}
with a \alert{real-valued} (possibly vector-valued) argument.

\end{frame}


\begin{frame}{CNNs for image classification}{Continuous convolution}

\begin{block}{Continuous convolution for smoothing (Goodfellow et al., 2016)}
We might use convolution for smoothing a 1-D function such as
continuous noisy measurements $x(t)$ of the linear position of a
spaceship:

$$ s(t) = \int x(a) w(t-a) da $$

$$ s(t) = (x * w)(t) $$

\end{block}

$w(\cdot)$ in the smoothing example:
\begin{itemize}
  \item Should be a
\alert{probability density function} to make it a \alert{weighted average}.
\item Should be \alert{0} for negative arguments (the future)
\end{itemize}

\medskip

There are many other applications of continuous convolution.

\end{frame}


\begin{frame}{CNNs for image classification}{Discrete convolution}

  Now we move to \alert{discrete} convolution.

  $x(t)$ is assumed to be a discrete sample from some underlying
  continuous function taken at regular spatial or temporal intervals.

  \medskip
  
  We'll call $x(t)$ the \alert{input} and $w(t)$ the \alert{kernel}.

  \medskip

  The output is usually called a \alert{feature map}.

  \medskip
  
  The continuous integral becomes a \alert{discrete sum}:

  $$ s(t) = (x * w)(t) = \sum_{a = -\infty}^{\infty} x(a) w(t-a) $$

\end{frame}


\begin{frame}{CNNs for image classification}{Multidimensional convolutions}

In a ML application, the input is an arbitrary \alert{multidimensional
  array of data} and the kernel is a \alert{multidimensional array of
  parameters} adapted by the learning algorithm.

\medskip

Multidimensional arrays are called \alert{tensors}.

\medskip

The theoretically infinite functions are assumed 0 everywhere but in
the finite set of points we have stored.

\medskip

Example: a two-dimensional image $I(\cdot,\cdot)$ would best be processed by a
two-dimensional kernel $K(\cdot,\cdot)$:

$$ S(i,j) = (I * K)(i, j) = \sum_m \sum_n I(m,n)K(i-m,j-n). $$

\end{frame}


\begin{frame}{CNNs for image classification}{Multidimensional convolutions}

Convolution is \alert{commutative}, so we can equivalently write

$$ S(i,j) = (I * K)(i,j) = (K * I)(i,j) =
\sum_m \sum_n I(i-m,j-n)K(m,n).$$

This version is easier to implement, as the loop is over the smaller
number of non-zero values in $K(\cdot,\cdot)$.

\end{frame}


\begin{frame}{CNNs for image classification}{Cross-correlation vs.\ convolution}

Note that in both $I * K$ and $K * I$, as the index into the
input \alert{increases}, the input into the kernel \alert{decreases},
and vice versa.

\medskip

We have \alert{flipped} the kernel.

\medskip

Flipping the kernel is necessary mathematically for commutativity, but
is not necessary for the actual information processing. Therefore, we
may use \alert{cross-correlation}

$$S(i,j) = (K \star I)(i,j) = \sum_m \sum_n I(i+m,j+n)K(m,n),$$

in which we don't flip the kernel. You will often see cross-correlation
called convolution in machine learning libraries.

\medskip

In this class, when we say ``convolution'' we will normally mean cross
correlation!

\end{frame}


\begin{frame}{CNNs for image classification}{Example}

  2D convolution operations give us the linear response of a 2D filter
  applied to the pixels in a local region of an image.

  \medskip

  See any of many examples on YouTube, such as
  \url{https://www.youtube.com/watch?v=_iZ3Q7VXiGI} !

  \medskip

  The simplest example is edge detection using, for example, Sobel
  filters:

  $$\begin{bmatrix} 1 & 0 & -1 \\ 2 & 0 & -2 \\ 1 & 0 & -1 \end{bmatrix}
  \;\;\;\;\;
  \begin{bmatrix} 1 & 2 & 1 \\ 0 & 0 & 0 \\ -1 & -2 & -1 \end{bmatrix}$$

\end{frame}


\begin{frame}{CNNs for image classification}{Hierarchical feature maps}

  A convolution may apply to the \alert{input} or \alert{a feature map
    from a previous layer}.

  \medskip

  Typically, convolutions apply to all of the feature maps in the
  previous layer: this is called \alert{convolution over volume}.
  \begin{itemize}
  \item For the input: three maps (R, G, and B) or one map (grayscale).
  \item For an inner layer: the number of kernels. This would be,
    e.g., 96 for AlexNet's second convolutional layer, except that
    AlexNet splits into two separate hierarchies so it is 48 for each
    stream.
  \end{itemize}

\end{frame}


\begin{frame}{CNNs for image classification}{Valid region of the convolution}

Example below. Note that we only use the \alert{valid} parts of the
convolution where the kernel fully overlaps the valid part of the
input.

\myfig{2.2in}{goodfellow-fig9-1}{Goodfellow et al. (2016), Figure 9.1}

\end{frame}


\begin{frame}{CNNs for image classification}{Important ideas behind CNNs}

There are three main important ideas behind the success of
convolutional neural networks:
\begin{itemize}
\item Sparse interactions
\item Parameter sharing
\item Equivariant representations
\end{itemize}

(Aside: a benefit of convolutions is that they can be applied to an
input of variable size.)

\end{frame}


\begin{frame}{CNNs for image classification}{Sparse interactions}

  \alert{Sparse interactions}:
  \begin{itemize}
    \item A fully connected layer has \alert{dense}
interactions (every unit interacts with every unit in the previous
layer)
\item A convolutional layer has \alert{sparse} interactions (each
unit in the output feature map interacts only with a few elements of
the input, via the small kernel.
  \end{itemize}

  \medskip
  
  Fewer parameters means \alert{lower memory requirements} and
  \alert{higher statistical efficiency}.

  \medskip
  
  The specific organization of a convolution also means it can be
  implemented efficiently.

  \medskip

  Dense connections from $m$ inputs to $n$ outputs requires $O(mn)$
  time.  With a kernel of size $k$, we only need $O(kn)$ time.

\end{frame}


\begin{frame}{CNNs for image classification}{Sparse interactions}

Sparse interaction example. Unit $x_3$ only affects units
$s_2$, $s_3$, and $s_4$ in the output feature map.
Compare to dense interaction below.

\myfig{2in}{goodfellow-fig9-2}{Goodfellow et al. (2016), Figure 9.2}

\end{frame}


\begin{frame}{CNNs for image classification}{Sparse interactions}

Looking from above,
unit $s_3$ in the output feature map
has a \alert{receptive field} including $x_2$, $x_3$, and $x_4$.

\myfig{2in}{goodfellow-fig9-3}{Goodfellow et al. (2016), Figure 9.3}

\end{frame}


\begin{frame}{CNNs for image classification}{Sparse interactions}


When convolutional layers are formed into hierarchies, a unit in a
deep layer ($g_3$) can be \alert{indirectly connected} to all or most
of the input.

\medskip

\myfig{2.5in}{goodfellow-fig9-4}{Goodfellow et al. (2016), Figure 9.4}

\end{frame}


\begin{frame}{CNNs for image classification}{Parameter sharing}

  \alert{Parameter sharing}:
  \begin{itemize}
    \item Sharing means using the same parameter for more than one interaction
      in a model.
    \item Shared parameters are also often called \alert{tied weights}.
      \item In a convolution, each weight in the kernel
        is \alert{reused} at every position in the input.
  \end{itemize}

  Parameter sharing \alert{increases statistical efficiency}.
  
\end{frame}


\begin{frame}{CNNs for image classification}{Parameter sharing}

With parameter sharing (top), one parameter is used many times.
Without parameter sharing, one parameter is used only once.
\myfig{3in}{goodfellow-fig9-5}{Goodfellow et al. (2016), Figure 9.5}

\end{frame}


\begin{frame}{CNNs for image classification}{Parameter sharing}

The representational power of convolution with shared parameters is
clear with the following example of vertical edge detection using a
2-element kernel (-1,1):

\myfig{3in}{goodfellow-fig9-6}{Goodfellow et al. (2016), Figure 9.6}

\end{frame}


\begin{frame}{CNNs for image classification}{Equivariance}

  \alert{Equivariance}:
  \begin{itemize}
  \item A function $f(x)$ is \alert{equivariant} to function $g$ if $f(g(x)) =
    g(f(x))$.
  \item Convolution is equivariant to \alert{translation}.
  \item If $f$ is a convolution operation and $g$ is a translation
    operation, we can see that the convolution of a translated version
    of input $x$ is the same as the translation of the convolution of
    $f$ and the input $x$.
  \end{itemize}

  \medskip
  
  How is equivariance useful?

  \medskip

  For time series: we get the same response to the same event,
  \alert{regardless of when} the event occurs.

  \medskip
  
  For images: we get the same response to a local pattern,
  \alert{regardless of where} in the image it occurs.

  \medskip
  
  Note that convolution is NOT equivariant to scale, rotation, etc.

\end{frame}


\begin{frame}{CNNs for image classification}{Activation / rectification}

  One of the insights of the CNN is to perform convolutions
  \alert{hierarchically}.

  \medskip

  However, a hierarchy of purely \alert{linear} transformations would
  ultimately be equivalent to a single linear transformation, which would
  not give powerful pattern matching capabilities.
  
  \medskip
  
  Generally, then, the result of one convolution in the hierarchy
  should be transformed by a nonlinearity.

  \medskip

  ReLU is the most popular
  nonlinear activation function.

  \medskip

  Hyperbolic tangent and the logistic sigmoid are also possible but
  lead to slower learning, according to Krizhevsky et al.\ (AlexNet).

  \medskip
  
  The resulting feature map may possibly be downscaled by a
  \alert{pooling} operation before it is convolved with higher-level
  filters.
  
\end{frame}


\begin{frame}{CNNs for image classification}{Pooling}

The structure of a CNN usually contains several ``macro'' layers
consisting of a convolution, a nonlinearity especially ReLU, then
pooling. Sometimes a ``layer'' means all three operations, and sometimes
each operation is treated separately.

\myfig{2in}{goodfellow-fig9-7}{Goodfellow et al. (2016), Figure 9.7}

\end{frame}


\begin{frame}{CNNs for image classification}{Pooling}

Pooling makes the output feature map approximately \alert{invariant}
to small translations of the input.

\myfig{2in}{goodfellow-fig9-8}{Goodfellow et al. (2016), Figure 9.8}

This is good when we want to know \alert{whether} a feature is in the
input, without knowing precisely \alert{where} it is.

\end{frame}


\begin{frame}{CNNs for image classification}{Pooling}

Pooling \alert{over features} enables invariance to more complex
transformations of the input, such as rotation:
\myfig{3in}{goodfellow-fig9-9}{Goodfellow et al. (2016), Figure 9.9}

\end{frame}


\begin{frame}{CNNs for image classification}{Pooling}

Usually, pooling is combined with downsampling, which reduces the
computational and statistical burden on the next layer.

\medskip

\myfig{3in}{goodfellow-fig9-10}{Goodfellow et al. (2016), Figure 9.10}

\end{frame}


\begin{frame}{CNNs for image classification}{Putting it together}

  \begin{columns}

    \column{2in}

    Some sample CNN architectures, as examples.  Note that practical
networks are deeper, more variable, and have branches.

\medskip

Generally, the kinds of processing we want to do on spatial and temporal
data fit the convolve + pool processing approach, but it may not always
be so. Convolution and pooling may cause \alert{underfitting}.

\column{2.5in}

\myfig{2in}{goodfellow-fig9-11}{Goodfellow et al. (2016), Figure 9.11}

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: tensors}

In general, the input, kernel, and output are all tensors of arbitrary
number of dimensions.

\medskip

Images: 2D data with 3 channels (red, green, blue), which may be combined
into mini-batches, giving a 4D tensor as the input.

\medskip

The kernel applied to these data might be a 4D tensor $\mat{K}_{i,j,k,l}$ giving
strength of connection between input channel $i$, output channel $j$, and
offset $(k,l)$ between the output and input unit.  Note that every feature
map $j$ connects with \alert{every} channel $i$ in the input.

\medskip

The output would then be a 3D tensor $\mat{Z}_{i,j,k}$, where $i$ represents
the output channel or feature map and $(j,k)$ indicates the spatial location.

\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: downsampling via stride}

We may also \alert{downsample} during the convolution operation, using
a stride not equal to 1. Example stride of 2:

\myfig{2in}{goodfellow-fig9-12}{Goodfellow et al. (2016), Figure 9.12}

\end{frame}


\begin{frame}{CNNs for image classification}{Stride}

  Stride is important:
  \begin{itemize}
  \item Typically, the convolution operation is applied at every pixel
    of the input (stride $=$ 1).
  \item When spatial resolution is less important or neighboring
    receptive fields overlap significantly, we may skip some pixels of
    the input (stride $>$ 1).
  \item Most architectures use a stride of 1 for 3$\times$3
    convolutions and a stride of 1--3 for 5$\times$5 convolution.
  \item The trend: smaller kernels and more layers $\rightarrow$ small
    strides (11$\times$11 convolutions as in AlexNet are not often
    seen).
  \end{itemize}
    
\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: padding}

  \begin{columns}

    \column{1.5in}
    
    We may also \alert{pad} the image so that the valid convolution at
    each layer has the same size as the input to the layer:

    \medskip
    
    Usually, the padding is 0.

    \column{3in}
    
    \myfig{2.5in}{goodfellow-fig9-13}{Goodfellow et al. (2016), Figure 9.13}

  \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{Padding}

  Padding is important:
  \begin{itemize}
  \item Without padding, the border would shrink after each
    convolution, and information at the image border would be lost.
  \item In most cases, we should add padding necessary so that the
    output feature map has the same size as the input feature map when
    the stride is 1.
  \item The most common choice for padding seems to be 0-padding.
  \item Matt likes copy-border padding, but most libraries do not implement it.
  \item Most libraries do implement ``reflect'' padding which seems to
    work well (and is better than zero-padding).
  \end{itemize}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: local connection}

In \alert{unshared convolution}, we perform the same dot product of a
set of weights over a local region in the input, but do not use a shared
kernel.

\medskip

This makes sense when we need local sparse computation but have
no reason to apply the same operation throughout the input.

\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: local connection}

Comparison of local connections, convolution, and full connection:

\myfig{2in}{goodfellow-fig9-14}{Goodfellow et al. (2016), Figure 9.14}

\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: organizing by channel}

We may also want separate kernels for each input channel:

\myfig{1.8in}{goodfellow-fig9-15}{Goodfellow et al. (2016), Figure 9.15}

\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: tiled convolution}

Another variation is \alert{tiled convolution} in which we combine
the idea of having separate weights for neighboring positions in the
feature map but also sharing parameters:

\myfig{1.6in}{goodfellow-fig9-16}{Goodfellow et al. (2016), Figure 9.16}

\end{frame}


\begin{frame}{CNNs for image classification}{Refinements: transpose convolution}

\alert{Transpose convolution} means multiplication by the transpose of the
matrix defined by convolution. It is used for
\begin{itemize}
\item Backpropagating error derivatives through a convolutional layer
\item Reconstructing input units from hidden units in an autoencoder
\end{itemize}

\end{frame}


\begin{frame}{CNNs for image classification}{Backpropagation}

Suppose we have a convolution tensor $\mat{K}$ applied to multichannel
tensor $\mat{V}$ with stride $s$: $c(\mat{K},\mat{V},s)$.

\medskip

Suppose the loss function is $\mat{J}(\mat{V},\mat{K})$.

\medskip

Forward propagation gives us $\mat{Z} = c(\mat{K},\mat{V},s)$.

\medskip

$\mat{Z}$ is propagated forward, then during backpropagation, we
receive tensor $\mat{G}$ where
$$\mat{G}_{i,j,k} = \frac{\partial}{\partial \mat{Z}_{i,j,k}}
\mat{J}(\mat{V},\mat{K}) .$$

The derivative of $\mat{J}$ with respect to weight $i,j,k,l$ in $\mat{K}$
is
$$ g(\mat{G},\mat{V},s)_{i,j,k,l} = \frac{\partial}{\partial \mat{K}_{i,j,k,l}}
J(\mat{V},\mat{K}) =
\sum_{m,n} \mat{G}_{i,m,n} \mat{V}_{j,(m-1)\times s+k,(n-1)\times s + l}.$$

\end{frame}


\begin{frame}{CNNs for image classification}{Backpropagation}

The deltas to be backpropgated to previous layers are
$$h(\mat{K},\mat{G},s)_{i,j,k} = \frac{\partial}{\partial \mat{V}_{i,j,k}}
J(\mat{V},\mat{K})$$
$$= \sum_{\substack{l,m\\ \textrm{s.t.} (l-1)\times s + m = j}}
\sum_{\substack{n,p\\ \;\; \textrm{s.t.} (n-1)\times s + p = k}}
\sum_q \mat{K}_{q,i,m,p} \mat{G}_{q,l,n}.$$

\end{frame}


\begin{frame}{CNNs for image classification}{Backpropagation}

  How the bias term is done varies.

  \medskip

  For locally connected layers, each
  unit would have its own bias.

  \medskip

  For convolutional layers, usually, each channel of the output has a
  separate bias shared across all locations.

  \medskip

  Sometimes, each element in the output will have its own bias. This
  is useful for example when we use 0 padding and border elements
  receive less input than interior elements.

\end{frame}


\begin{frame}{CNNs for image classification}{Types of outputs and inputs}

  Besides classification, convolutional networks can also be used to
  produce a \alert{structured} output, itself a multidimensional
  tensor such as an image.

  \medskip
  
  Besides images, the input data could be a single channel audio
  waveform, or a sequence of feature vectors corresponding to samples
  over time, or a 3D volume, or a color video,

\end{frame}


\begin{frame}{CNNs for image classification}{Computational speed}

  To make convolution efficient, we may use \alert{parallel hardware}
  (GPUs).

  \medskip

  In some cases, kernels may be \alert{separable}, e.g., a 2D
  convolution is the composition of two 1D convolutions in the
  different dimensions.

\end{frame}


\begin{frame}{CNNs for image classification}{Convolutions without supervised learning}

  How to obtain convolution kernels without expensive backpropagation?
  \begin{itemize}
  \item Random kernels
  \item Design by hand (e.g., edge detectors or Gabor filters)
  \item Learn with an unsupervised criterion, e.g., k-means clustering of
   image patches in the training set.
  \end{itemize}

\end{frame}


\begin{frame}{CNNs for image classification}{Connections with brain science}

  There is an interesting correspondence between low-level processing
  in well-trained convolutional neural networks for visual tasks and
  the kinds of visual feature extractors known to be present in the
  mammalian visual system:

  \myfig{3in}{goodfellow-fig9-19}{Goodfellow et al. (2016), Figure 9.19}

\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  The story of the CNN begins in the 1980s with Fukushima's
  \alert{Neocognitron}, a hierarchical nerual network designed in
  primitive mimicry of the hierarchical processing in the primate
  visual cortex.

  \medskip

  \myfig{3.5in}{fukushima-fig2}{Fukushima (1980), Fig.\ 2}

\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  Geoff Hinton was one of the rediscoverers of backpropagation in 1986.

  \medskip

  Hinton's postdoc Yann LeCun went on to create the first practical
  modern convolutional neural networks for OCR at AT\&T
  Research.\footnote{Today, Yann LeCun is chief AI scientist at
    Facebook.}

  \medskip

  LeCun and colleagues' LeNet-5 (1998) had the world's best
  performance at recognizing handwritten digits for several years.

  \medskip

  \myfig{4in}{lecun-fig2}{LeCun et al.\ (1998), Fig.\ 2}

\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  The 2000s were a golden era for feature-based approaches to visual
  object recognition such as HOG (histograms of oriented gradients).

  \medskip

  Some research on neural networks continued, but most vision
  researchers ignored CNNs.

  \medskip

  The main development pushing work forward was the emergence of
  \alert{standardized large-scale datasets}.

  \medskip

  In 2006, the PASCAL Visual Object Classification (VOC) challenge
  started with 20 object categories and ran with more difficult
  datasets each year until 2012.

  \medskip

  ``Simple'' feature based methods like HOG could not cope with VOC.

\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  Sample VOC object detection and classification images:

  \myfig{4.5in}{everingham-fig1}{Everingham et al.\ (2015), Fig.\ 1a}

\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  Sample VOC segmentation and action classification images:

  \myfig{4.5in}{everingham-fig1bc}{Everingham et al.\ (2015), Fig.\ 1b-c}

\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  Sample VOC person layout images:

  \medskip
  
  \myfig{4.5in}{everingham-fig1d}{Everingham et al.\ (2015), Fig.\ 1d}

  \medskip
  
  The yearly VOC challenges helped push progress forward, but in retrospect
  it is clear that the dataset was too small.

\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  Another competition, ImageNet, had even greater influence than VOC.

  \medskip
  
  2007: Fei-Fei Li, then at Princeton and later at Stanford, undertook
  a new effort to create a visual version of George Miller's WordNet,
  to be called ImageNet.

  \medskip

  Li failed to get much funding for the project in the beginning but
  found that Amazon Mechanical Turk could be used to get humans to
  label images relatively cheaply.

  \medskip

  2009: ImageNet was released at CVPR in a poster session then joined
  forces with PASCAL VOC for a 2010 competition: the \alert{ImageNet
  Large-Scale Visual Recognition Challenge (ILSVRC)}.

  \medskip

  Eventually, the dataset had over 15 million images over 22,000 categories.

  \medskip

  The 2010 contest dataset comprised 1.2 million images over 1000 categories.
  
\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  Samples from ImageNet:

  \medskip

  \myfig{2.8in}{image-net}{\url{http://image-net.org/explore}}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Origins}

  \begin{columns}

    \column{2.3in}

    \myfig{2.1in}{quartz-imagenet}{\begin{minipage}{2.3in} \url{https://qz.com/1034972/the-data-that-\\changed-the-direction-of-ai-research-\\and-possibly-the-world/} \end{minipage}}

    \column{2.2in}
    
    2010--2011: ImageNet competitors all had error rates over 25\%.

    \medskip

    2012: Krizhevsky, Sutskever, and Hinton submit
    \alert{AlexNet},
    sparking today's explosion of interest in AI and deep learning.

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  Krizhevsky et al.\ begin with some provacative points:
  \begin{itemize}
    \item Nature is extremely diverse. We need \alert{extremely large
      datasets} if we hope to learn that diversity.
    \item Learning from millions of images requires \alert{models with
      large capacity}.
    \item CNN capacity can be controlled by varying their depth and breadth.
    \item The number of parameters in a CNN is smaller than that of
      similarly-sized fully connected models.
    \item CNNs decrease the number of parameters by making assumptions
      that seem to be correct: relevant features' statistics are
      stationary, and dependencies between pixels are mostly local.
  \end{itemize}

  These factors give us hope that CNNs may have the capacity to learn
  large datasets with relatively few parameters.\footnote{Recall that
    fewer parameters generally means lower VC dimension which in turn
    generally means better generalization.}

\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  The authors trained what as of 2012 was the largest CNN ever, with
  60 million parameters.

  \medskip

  Training required a highly efficient implementation of the learning
  and runtime operations on GPUs with C++ and CUDA.

  \medskip

  Training time was 5--6 days on two GTX 580 3GB GPUs.
  
  \medskip

  Current version of the toolkit the authors built is available as
  open source: \url{https://code.google.com/archive/p/cuda-convnet2/}
  
\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  \begin{columns}

    \column{2.2in}
    
    \myfig{2in}{krizhevsky-fig1}{Krizhevsky et al.\ (2012), Fig.\ 1}

    \column{2.3in}
    
    The authors were among the first to exploit the benefits of ReLU
    over other nonlinear activation functions.

    \medskip

    ReLU models learn much faster than $\tanh$ models. $\tanh$
    ``saturates'' at large absolute values (learning depends on the
    slope of the activation function).

    \medskip
  
    Training a 4-layer network on CIFAR 10: solid line shows error
    rate with ReLU, dashed line shows error rates with tanh.

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  AlexNet begins with a preprocessing step in which the input image is
  scaled to 256 pixels in the shortest dimension then is cropped to
  256 $\times$ 256.

  \medskip

  \myfig{4.5in}{alexnet-fig2}{Krizhevsky, Sutskever, and Hinton,
    (2012), Fig.\ 2}

  \medskip

  Five convolutional layers with ReLU activations (Nair and Hinton,
  2010) and max pooling layers are followed by three fully-connected
  layers.

\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  AlexNet layers:
  \begin{itemize}
  \item $224 \times 224 \times 3$ input
  \item Convolution with 96 kernels of size $11 \times 11 \times 3$ with a stride of 4 pixels
  \item ReLU + local response normalization + overlapping max pooling
  \item Convolution with 256 kernels of size $5\times 5\times 48$ with a stride of 1.
  \item ReLU + local response normalization + overlapping max pooling
  \item 384 kernels of size $3\times 3\times 256$ with a stride of 1
  \item ReLU only (no normalization or pooling)
  \item 384 kernels of size $3\times 3\times 192$
  \item ReLU only
  \item 256 kernels of size $3\times 3\times 192$.
  \item ReLU only
  \item 4096 fully connected units
  \item 4096 fully connected units
  \item 1000 fully connected softmax units
  \end{itemize}
  
\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  After ReLU, the next most important technique used is
  \alert{local response normalization}, which reduces errors by more than 1\%.

  \medskip

  Non-saturating activation functions means we can have very large
  activations for some inputs.

  \medskip
  
  Response normalization \alert{reduces large activations} while
  \alert{preserving relative relationships} between different feature
  maps.

  \medskip
  
  Another term for this is \alert{local inhibition}, a property seen
  in real neural circuits.

  \medskip
  
  Letting $a^{i}_{x,y}$ represent the ReLU activation feature map $i$
  at location $(x,y)$, the normalized response is
  $$ b^{i}_{x,y} = a^{i}_{x,y} \left( k + \alpha \sum_{j = \max(0,
    i-n/2)}^{\min( N-1,i+n/2)}\left(a^{j}_{x,y}\right)^2
  \right)^\beta $$

  \medskip

  We are normalizing each unit
  relative to its $n$ neighboring features.

\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  Another technique is \alert{overlapping pooling}.

  \medskip

  Before AlexNet, pooling usually used a stride equal to the width of
  the pooling region, so that neighboring pooled units did not have
  overlapping receptive fields.

  \medskip
  
  The authors find, however, that overlapping pooling is effective. They
  use a stride of 2 and a pooling region of size 3x3.

\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  How to avoid overfitting when we have 60 million parameters?

  \medskip

  For images, \alert{data augmentation} using various image
  transformations makes sense.

  \medskip

  AlexNet authors begin with a 256$\times$256 image then sample
  224$\times$224 patches from the original.
  \begin{itemize}
  \item Training time: 2048 random patches including translation,
    horizontal reflections, and global random intensity
    transformations per image.
  \item Test time: four corner patches plus the center patch are
    processed, and the output layer is averaged over the 5 samples.
  \end{itemize}

  \medskip

  The second trick is \alert{dropout} at the (first two) fully
  connected layers:
  \begin{itemize}
  \item Training time: each output in the feature map is set to 0 with
    probability 0.5. Zeroed outputs do not get any backpropagated
    error.
  \item Test time: all units are used but output is multipled by 0.5.
  \end{itemize}
  
\end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  Training parameters:
  \begin{itemize}
  \item Stochastic gradient descent
  \item Batch size 128 examples
  \item Momentum 0.9
  \item Weight decay 0.0005
  \item Weights initialized with samples from ${\cal N}(0,0.01)$
  \item Biases initialized to 1 for most layers (to place ReLU in the
    positive region) and 0 for other layers (first and third
    convolutional layer).
  \end{itemize}

  \end{frame}


\begin{frame}{CNNs for image classification}{AlexNet (2012)}

  The first convolutional layer learns representations reminiscent of
  neurons in visual cortex:

  \myfig{2in}{krizhevsky-fig3}{Krizhevsky et al.\ (2012), Fig.\ 3}

  This is after 90 epochs through the 1.2 million images in ImageNet.
  
  \medskip

  The result: top-5 error rate dropped from 26\% (2011) to 15.3\%. 
  
\end{frame}


\begin{frame}{CNNs for image classification}{ZFNet (2013)}

  In the 2013 ILSVRC, a ``tweaked'' version of AlexNet reduced the
  top-5 error rate to 12\%.
  
\end{frame}

\begin{frame}{CNNs for image classification}{GoogLeNet (2014)}

  \begin{columns}

    \column{3in}

    In the 2014 ILSVRC, Google's entry achieved a further huge
    improvement to a 6.7\% top-5 error rate.

    \medskip

    Principles: smaller convolutions, more layers, \alert{inception}
    modules.

    \medskip

    AlexNet's 60 million parameters reduced to 4 million.

    \medskip

    Szegedy, C., Liu, W., Jia, Y., Sermanet, P., 
    Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and 
    Rabinovich, A. (2015), Going deeper with convolutions.
    \textit{IEEE Computer Society Conference on Computer Vision and Pattern
    Recognition (CVPR)}, pages 1--9.

    \column{1.5in}

    \myfig{0.6in}{szegedy-fig3}{Szegedy et al.\ (2014), Fig.\ 3}

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{GoogLeNet (2014)}

  Before Inception, accuracy was achieved by larger networks with more
  feature maps and more parameters.

  \medskip

  Overfitting is avoided by aggressive data augmentation and dropout.

  \medskip

  Inception idea: can we have deeper, wider networks with a fixed computational
  budget?

  \medskip

  Goal: 1.5 billion multiply-adds at inference time.

  \medskip

  This was achieved with a 22-layer (27 including pooling layers)
  model with a modular structure.

  \medskip

  The module idea was inspired by ``Network in Network.''

\end{frame}


\begin{frame}{CNNs for image classification}{GoogLeNet (2014)}

  In 2013, Lin, Chen, and Yan at NUS had introduced the concept of
  Network-In-Network:
  \begin{itemize}
  \item In place of simple convolutions, local operations are
    performed by small multilayer perceptrons.
  \item The entire module is then scanned over the input like a
    convolution to produce a new feature map.
  \end{itemize}
    
  \medskip
  
  \myfig{3in}{lin-fig1}{Lin, Chen, and Yan (2013), Fig.\ 1}
  
\end{frame}


\begin{frame}{CNNs for image classification}{GoogLeNet (2014)}

  Inception modules simplify the NIN concept, replacing the MLP with a
  single 1$\times$1$\times D$ convolution followed by ReLU, which can
  be implemented with standard CNN tools.
  
  \medskip

  The 1$\times$1 convolutions also aim to capture some of the
  theoretical work suggesting that extracting and combining sparse
  clusters of features over the image is optimal.
  
  \medskip

  An inception module thus combines 1$\times$1, 3$\times$3, and
  5$\times$5 convolutions all feeding a single aggregating feature
  map.

  \medskip

  \myfig{3in}{szegedy-fig2}{Szegedy et al.\ (2014), Fig.\ 2}
  
\end{frame}


\begin{frame}{CNNs for image classification}{GoogLeNet (2014)}

  1x1 convolutions occur both \alert{before} larger 3x3 and 5x5
  convolutions, and \alert{after} them.

  \medskip

  Before: a \alert{reduction} step that reduces the number of
  feature maps the 3x3 or 5x5 operates over, making them more
  efficient.

  \medskip
	
  After: a \alert{project} step where redundancy is removed
  and output dimensionality is reduced.

  \medskip

  Besides an approximation to network-in-network, the architecture
  is intended to approximate theoretically optimal construction of
  \alert{large, sparse, locally connected layers} that progressively
  cluster correlated features at previous layers.

  \medskip

  Proportion of 3x3 and 5x5 convolutions increases at later layers.

  \medskip

  Pooling layers are used, but unpooled features are also propagated,
  to get both spatial accuracy and translation invariance.

\end{frame}


\begin{frame}{CNNs for image classification}{GoogLeNet (2014)}

  Auxiliary classifiers are used at intermediate layers to increase
  their discrimination ability.

  \medskip

  Training was CPU-only, with SGD, Polyak averaging, multiple models
  at prediction time, and aggressive data augmentation.

  \medskip

  The model was also used for detection in ILSVRC 2014, applying selective
  search for region proposals and GoogLeNet to classify
  those regions, without bounding box regression.

\end{frame}


\begin{frame}{CNNs for image classification}{GoogLeNet (2014)}

  GoogLeNet Image classification setup:
  \begin{itemize}
  \item 7 different models with different training pattern sampling. 
  \item Test images are scaled to four sizes (256, 288, 320, and 352).
  \item For each scaled image, the left, center, and right or top,
    center, and bottom squares are taken.
  \item For each such image, 5 224$\times$224 crops (4 corners plus center)
    and the entire region scaled to 224$\times$224 are taken.
  \item For each crop, we take the original and horizontally flipped image.
  \item Total test images per input: $4\times 3\times 6\times 2 = 144$.
  \end{itemize}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  After 2014, research on image classification network continued
  at a fast pace.

  \medskip

  Several labs continued to improve their models to perform better
  at ILSVRC.

  \medskip

  Szegedy and colleagues increased GoogLeNet's size to 5 billion
  multiply-adds, trying to use that budget as efficiently as possible.

  \medskip

  Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016),
  Rethinking the Inception architecture for computer vision, \textit{CVPR}.

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  Some principles:
  \begin{itemize}
    \item Avoid information bottlenecks early in processing. Decrease
      dimensionality gradually as we move deeper.
    \item Keep dimensionality high for local processing.
    \item Reduce dimensionality before performing spatial aggregation.
    \item Balance the width and the depth of the network. Increased
      capacity should be achieved by increasing both depth and width.
  \end{itemize}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  The authors discuss a variety of ways of reducing computation
  without reducing representational capacity.

  \medskip

  Example: replacing a 5x5 convolution with a hierarchy of two
  3x3s:

  \medskip

  \myfig{2in}{szegedy-15-fig1}{Szegedy et al. (2016), Figure 1}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  When we factorize convolutions in this way, should we apply
  ReLU to every layer or only at the end?

  \medskip

  Experiments show that ReLU at every step is better than linear
  activations in the factored layers:

  \medskip

  \myfig{3in}{szegedy-15-fig2}{Szegedy et al. (2016), Figure 2}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  The 5x5 factorization gives a new structure for the basic
  Inception module:

  \begin{columns}

  \column{2in}

  \myfig{1.5in}{szegedy-15-fig4}{Szegedy et al. (2016), Figure 4}

  \column{2in}

  \myfig{1.5in}{szegedy-15-fig5}{Szegedy et al. (2016), Figure 5}

  \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  \begin{columns}

  \column{3in}
  
  Another way of factorizing:
  replacing a 3x3 with a 3x1 followed by a 1x3.

  \medskip

  This process can go on; any $n$x$n$ convolution can be
  factored into a $n$x1 followed by a 1x$n$.

  \medskip

  The authors find this is not very useful in early 
  layers, but is effective for medium-sized grids of 12-20
  units wide.

  \column{1.5in}

  \myfig{1.4in}{szegedy-15-fig3}{Szegedy et al. (2016), Figure 3}

  \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  \begin{columns}

  \column{2in}

  Inception module for medium-sized grids:

  \medskip

  \myfig{1.8in}{szegedy-15-fig6}{Szegedy et al. (2016), Figure 6}

  \column{2in}

  Inception module for coarsest grids:
	  
  \medskip

  \myfig{1.8in}{szegedy-15-fig7}{Szegedy et al. (2016), Figure 7}

  \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  The side classifiers from GoogLeNet turn out not to be as effective
  as first thought.

  \medskip

  Removing one of the two side classifiers does not adversely affect
  performance.

  \medskip

  The side classifier seems to work as a regularizer rather than
  helping to create more discriminative features in early layers.

  \medskip

  Auxiliary classifier on top of the last 17x17 feature map:

  \medskip

  \myfig{2in}{szegedy-15-fig8}{Szegedy et al. (2016), Figure 8}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  \begin{columns}

  \column{2in}

  There are multiple ways of reducing grid size. The left structure
  introduces a bottleneck. The right structure avoids the bottleneck
  but introduces more computation.

  \medskip

  \myfig{1.8in}{szegedy-15-fig9}{Szegedy et al. (2016), Figure 9}

  \column{2in}

  Alternative: perform pooling with stride 2 and convolution with stride
  2 in parallel:

  \medskip

  \myfig{1.8in}{szegedy-15-fig10}{Szegedy et al. (2016), Figure 10}

  \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception v3 (2016)}

  Putting all these ideas together resulted in Inception-v2, a 42-layer
  extension of GoogLeNet.

  \medskip

  Additional features leading to Inception-v3:
  \begin{itemize}
    \item RMSprop optimizer
    \item Label smoothing: rather than one-hot ground truth, we
      mix the one-hot distribution with the prior distribution
      over the classes.
    \item Batch normalization of FC layers and conv layers in the
      side network.
  \end{itemize}

  \medskip

  Result was the highest single-crop top-1 accuracy on ILSVRC to
  date.

\end{frame}


\begin{frame}{CNNs for image classification}{VGG (2014)}

  The 2nd place entry in 2014 was VGG (Simonyan and Zisserman, 2014).

  \medskip

  Important features:
  \begin{itemize}
  \item 3$\times$3 filters only
  \item 16--19 layers
  \item Otherwise similar to AlexNet
  \item 138 million parameters
  \item Tested on multiple crops through additional convolutional steps
    rather than averaging multiple crops
  \end{itemize}

  \medskip

  We learn that a deeper network with smaller convolutions is better
  than a shallower network with larger convolutions.

\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  The 2015 ILSRVC winner was \alert{ResNet} from Microsoft
  Research.\footnote{Note, though, that Baidu had an entry that beat ResNet,
  but the entry was disqualified for cheating.}

  \medskip

  He, K., Zhang, X., Ren, S., and Sun, J. (2016), Deep Residual
  Learning for Image Recognition, In \textit{IEEE Computer Society
    Conference on Computer Vision and Pattern Recognition}, pages
  770--778.

  \medskip

  ResNet pushes the notion that ``more depth is better'' to the extreme.

  \medskip

  The problem faced by very deep networks is \alert{degredation}:
  though adding more layers improves training error to a point,
  eventually, training error starts to \alert{increase}.

\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  \myfig{3.8in}{he-fig1}{He et al.\ (2016), Fig.\ 1}

  \medskip

  As we are talking about training error, the degredation is
  \alert{not overfitting}.

  \medskip

  A deeper model should be at least as good as its shallow cousin
  (think about constructing a deep network from a shallow one by
  adding identity mappings).

  \medskip
  
  Degredation is due to the vanishing integrity of the training signal
  as the network gets deeper.

\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  To overcome degredataion in very deep networks, ResNet uses the
  concept of \alert{residual learning}:

  \medskip

  \myfig{2in}{he-fig2}{He et al.\ (2016), Fig.\ 2}

  \medskip

  To learn a mapping ${\cal H}(\vec{x})$, we let intermediate layers
  learn another mapping ${\cal F}(\vec{x}) = {\cal H}(\vec{x})-\vec{x}$
  then compute ${\cal H}(\vec{x})$ at the output.

  \medskip

  Residual learning can be implemented with \alert{shortcut
    connections} that add the input to the output of the subnetwork.
  
\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  When a subnetwork changes the dimensionality of input $\vec{x}$,
  then a dimensionality changing mapping is used instead of the
  identity mapping.

  \medskip
  
  He et al.\ demonstrate that
  \begin{itemize}
    \item Very deep networks without shortcut connections fail to
      learn the training set as well as similar networks with shortcut
      connections.
    \item A \alert{152-layer network} with shortcut connections can
      learn on ImageNet and CIFAR and won ILSVRC 2015 (3.56\% top-5
      error).
  \end{itemize}
  
\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  \begin{columns}

    \column{1.4in}
    
    \myfig{1.2in}{he-fig3}{He et al.\ (2016), Fig.\ 3}

    \column{3.1in}

    Baseline model (middle) is a 34-layer network
    \begin{itemize}
    \item Mostly 3$\times$3 convolutions
    \item Equal-size mappings always have the same number of filters
    \item Mappings that downsample do so by half, with double the
      number of filters
    \end{itemize}

    \medskip

    Shortcut connections in residual version (right) are performed
    by 1$\times$1 convolutions with stride of 2.
    
  \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  \myfig{4in}{he-fig4}{He et al.\ (2016), Fig.\ 4}

  \medskip

  Left: plain networks with 18 or 34 layers. Right: residual networks
  with 18 or 34 layers.

  \medskip

  Residual layers give better convergence for the small network and
  better accuracy for the larger network. Only the larger residual
  model pushes training error lower than validation error.
  
\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  Identity or projection? Experiments show not much impact of either
  choice.

  \medskip

  The authors decided to use identity for same-size mappings
  and projection for decreased-size mappings.

\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  \myfig{4in}{he-fig5}{He et al.\ (2016), Fig.\ 5}

  \medskip

  Ordinary residual block (left) and bottleneck residual block (right).

  \medskip

  Bottlenecks do not seem to hurt performance and are more economical
  in terms of calculations and number of parameters, so are used in
  the 152-layer ResNet.

\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  \myfig{4.2in}{he-fig6}{He et al.\ (2016), Fig.\ 6}

  \medskip

  CIFAR results.
  Left: plain networks perform worse with more layers, both training
  and test.

  \medskip

  Middle: residual networks perform better with more layers, both
  training and test.

  \medskip

  Right: extremely large residual networks with more than 1000 layers
  learn well but exhibit overfitting.
  
\end{frame}


\begin{frame}{CNNs for image classification}{ResNet (2015)}

  \begin{columns}

    \column{2.5in}
    
    \myfig{2.4in}{he-fig7}{He et al.\ (2016), Fig.\ 7}

    \column{2in}

    Top: standard deviation of layer output responses after BN but
    before skip connection adding and ReLU.

    \medskip

    Bottom: same data, sorted.

    \medskip

    Residual layers are less active than plain layers.

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  The Inception group, inspired by the success of ResNet in ILSVRC 2015 and
  other competitions, considered whether residual connections can improve
  Inception.

  \medskip
  
  Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. (2017),
  Inception-v4, Inception-ResNet and the Impact of Residual
  Connections on Learning. \textit{Association for the Advancement
    of Artificial Intelligence Conference on AI (AAAI)}.

  \medskip

  A version of Inception-ResNet achieved a 3.08\% top-5 error rate on
  ILSVRC 2016 (but was beaten by Trimps-Soushen with 2.99\%).

\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}
    
    \column{1.5in}

    \myfig{1in}{szegedy-16-fig3}{Szegedy et at.\ (2017), Fig.\ 3}

    \column{3in}

    Stem network used at the input of Inception v4 and
    Inception-ResNet v2.

    \medskip

    No ``V'' means the input is same-padded so that output is same
    size as input.

    \medskip

    ``V'' means valid only.
    
  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{2.2in}
    
    \myfig{2in}{szegedy-16-fig4}{Szegedy et at.\ (2017), Fig.\ 4}

    \medskip

    35$\times$35 Inception module for Inception v4 (Inception A).

    \column{2.2in}
    
    \myfig{2in}{szegedy-16-fig5}{Szegedy et at.\ (2017), Fig.\ 5}

    \medskip

    17$\times$17 Inception module for Inception v4 (Inception B).

    \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{2.2in}
    
    \myfig{2in}{szegedy-16-fig6}{Szegedy et at.\ (2017), Fig.\ 6}

    \medskip

    8$\times$8 Inception module for Inception v4 (Inception C).
    
    \column{2.2in}
    
    \myfig{2in}{szegedy-16-fig7}{Szegedy et at.\ (2017), Fig.\ 7}

    \medskip

    Module for reduction from 35$\times$35 to $17\times 17$ (Reduction-A).

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{2in}
    
    \myfig{2in}{szegedy-16-fig8}{Szegedy et at.\ (2017), Fig.\ 8}

    \medskip

    Module for reduction from $17\times 17$ to $8\times 8$ (Reduction-B).

    \column{1.1in}

    \myfig{1in}{szegedy-16-fig9}{Szegedy et at.\ (2017), Fig.\ 9}

    \column{1.4in}
  
    Complete Inception v4 model.

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{2in}
    
    \myfig{2in}{szegedy-16-fig10}{Szegedy et at.\ (2017), Fig.\ 10}

    \medskip

    35$\times$35 Inception ResNet module (Inception-ResNet-A).

    \column{2in}
    
    \myfig{1.8in}{szegedy-16-fig11}{Szegedy et at.\ (2017), Fig.\ 11}

    \medskip

    17$\times$17 Inception ResNet module (Inception-ResNet-B).

  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{2in}
    
    \myfig{2in}{szegedy-16-fig12}{Szegedy et at.\ (2017), Fig.\ 12}

    \medskip

    17$\times$17 to 8$\times$8 reduction (Reduction-B) for
    Inception-ResNet-v1.

    \column{2in}
    
    \myfig{1.8in}{szegedy-16-fig13}{Szegedy et at.\ (2017), Fig.\ 13}

    \medskip
    
    8$\times$8 Inception-ResNet module (Inception-ResNet-C).
    
  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{1.3in}
    
    \myfig{1in}{szegedy-16-fig14}{Szegedy et at.\ (2017), Fig.\ 14}

    \column{0.9in}
    
    Stem of Inception-ResNet-v1.

    \column{1.2in}
    
    \myfig{1.1in}{szegedy-16-fig15}{Szegedy et at.\ (2017), Fig.\ 15}

    \column{0.9in}
    
    Complete Incpetion-ResNet-v1 and Inception-ResNet-v2 architectures.
    
  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{2in}
    
    \myfig{2in}{szegedy-16-fig16}{Szegedy et at.\ (2017), Fig.\ 16}

    \medskip

    35$\times$35 module for
    Inception-ResNet-v2 (Incpetion-ResNet-A).

    \column{2in}
    
    \myfig{1.6in}{szegedy-16-fig17}{Szegedy et at.\ (2017), Fig.\ 17}

    \medskip
    
    17$\times$17 module for Inception-ResNet-v2 (Inception-ResNet-B).
    
  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  \begin{columns}

    \column{2in}
    
    \myfig{2in}{szegedy-16-fig18}{Szegedy et at.\ (2017), Fig.\ 18}

    \medskip

    17$\times$17 reduction to 8$\times$8 module for
    Inception-ResNet-v2 (Incpetion-ResNet-A).

    \column{2in}
    
    \myfig{1.6in}{szegedy-16-fig19}{Szegedy et at.\ (2017), Fig.\ 19}

    \medskip
    
    8$\times$8 module for Inception-ResNet-v2 (Inception-ResNet-C).
    
  \end{columns}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Inception-ResNet (2016)}

  Conclusion from Inception-ResNet: training of Inception models is
  faster with residual connections.

  \medskip

  Besides techniques for exploiting residual connections in Inception
  modules, the paper introduces the idea of \alert{residual scaling}.

  \medskip

  Residual scaling was prompted by the finding that with large numbers
  of filters, regardless of learning rate, later layers started to
  produce only zeros due to unstable updates.

  \medskip

  Weighting the residuals added to a module's output by 0.1--0.3
  before adding improved stability of training.
  
\end{frame}


\begin{frame}{CNNs for image classification}{ILSVRC 2016}

  The classification challenge continued in 2016 and 2017.

  \medskip

  In 2016, the Trimps-Soushen team (sponsored by the Chinese Ministry
  of Public Security) won with 2.99\% top-5 error rate.

  \medskip

  Trimps-Soushen used a fusion strategy combining the results
  of various Inception and ResNet models, weighted by their accuracy.

\end{frame}


\begin{frame}{CNNs for image classification}{Squeeze and excitation (ILSVRC 2017)}

  The final recognition challenge was won with 2.25\% error rate by a
  team from Momenta (a Chinese self-driving car company) and
  Oxford.\footnote{Trained Caffe models available at
    \url{https://github.com/hujie-frank/SENet}.}

  \medskip

  The model is called a squeeze-and-excitation network (SENet).
  
  \medskip

  Hu, J., Shen, L., and Sun, G.\ (2018), Squeeze-and-excitation
  networks.  In \textit{IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition}.

\end{frame}


\begin{frame}{CNNs for image classification}{Squeeze and excitation (ILSVRC 2017)}

  Basic idea: reweight (scale) channels globally according to
  informativeness. The technique is called \alert{feature
    recalibration}.

  \medskip

  We have a transformation $\mat{F} : {\cal X} \mapsto {\cal U}$ with
  ${\cal X} = \Rset^{H' \times W' \times C'}$ and ${\cal U} =
  \Rset^{H\times W\times C}$.

  \medskip

  A \alert{squeeze} operation aggregates feature map $\mat{U} \in
  {\cal U}$ across dimensions $H \times W$ to produce a \alert{channel
    descriptor} vector $\vec{z}$. It's a simple average of the
  elements in each channel (no parameters).

  \medskip
  
  \myfig{4in}{hu-fig1}{Hu, Shen, and Sun (2018), Fig.\ 1}

  \medskip

  Excitation is more complicated...
  
\end{frame}


\begin{frame}{CNNs for image classification}{Squeeze and excitation (ILSVRC 2017)}

  A squeeze is followed by an \alert{excitation} operation that performs a
  sample-specific reweighting of the channels of $\mat{U}$.
  \[ \vec{s} = \vec{F}_{ex}(\vec{z},\mat{W}) = \sigma(g(\vec{z},\mat{W}))
  = \sigma(\mat{W}_2\delta(\mat{W}_1\vec{z})) \]

  $\delta(\cdot)$ is ReLU, and $\sigma(\cdot)$ is the logistic sigmoid.

  \medskip
  
  $\mat{W}_1$ performs dimensionality reduction, and $\mat{W}_2$
  performs dimensionality expansion.

  \medskip

  The elements of $\vec{s}$ are then used to scale $\mat{U}$.

  \medskip
  
  For SE to be useful, we can see that $\delta(\mat{W}_1\vec{z})$
  should be a low-dimensional coding of the global activity of each
  feature map based on which $\mat{W}_2$ will decide which feature
  maps to weight more or less actively.

\end{frame}


\begin{frame}{CNNs for image classification}{Squeeze and excitation (ILSVRC 2017)}

  SE can be applied to simple CNNs like AlexNet easily. The authors
  also show how to apply SE to Inception modules and ResNet.

  \medskip

  \begin{columns}
    \column{2.2in}
    \myfig{2in}{hu-fig2}{Hu, Shen, and Sun (2018), Fig.\ 2}
    \column{2.2in}
    \myfig{2in}{hu-fig3}{Hu, Shen, and Sun (2018), Fig.\ 3}
  \end{columns}

\end{frame}


\begin{frame}{CNNs for image classification}{Squeeze and excitation (ILSVRC 2017)}

  The approach improves the single-crop ImageNet
  performance of several models:

  \medskip
  
  \myfig{4.5in}{hu-table2}{Hu, Shen, and Sun (2018), Table 2}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Squeeze and excitation (ILSVRC 2017)}

  The performance improvement is immediate and consistent:

  \medskip
  
  \myfig{4.5in}{hu-fig4}{Hu, Shen, and Sun (2018), Fig.\ 4}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Squeeze and excitation (ILSVRC 2017)}

  SE also improves the performance of smaller networks aimed at mobile
  devices:

  \medskip
  
  \myfig{4in}{hu-table3}{Hu, Shen, and Sun (2018), Table 3}
  
\end{frame}


\begin{frame}{CNNs for image classification}{Summary}

  Lessons to be learned from ILSVRC entries:
  \begin{itemize}
  \item Use small convolutions
  \item Go deeper
  \item Reduce dimensionality and number of parameters when possible
  \item Combine multiple models for improved performance
  \item Learn residuals rather than direct mappings
  \item Amplify informative channels
  \end{itemize}
  
\end{frame}

%======================================================================
\section{Conclusion}
%======================================================================

\begin{frame}{Conclusion}

  This has been a very brief introduction to deep learning.

  \medskip

  Hopefully you get the main ideas and can see how to extend them to
  new problems.

  \medskip

  Want to go further?
  \begin{itemize}
  \item Read up on the literature, beginning with AlexNet then moving forward.
  \item Learn one or more frameworks such as Caffe, Tensorflow,
    Darknet, or Torch.
  \item A nice resource: \url{https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html}
  \item Take the DS\&AI Computer Vision course.
  \item Take the DS\&AI Recent Trends in Machine Learning (Deep
    Learning) course.
  \end{itemize}

\end{frame}

\end{document}


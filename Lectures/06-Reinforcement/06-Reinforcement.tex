\documentclass{beamer}

\mode<presentation>
{
  \setbeamertemplate{background canvas}[square]
  \pgfdeclareimage[width=6em,interpolate=true]{dsailogo}{../dsai-logo}
  \pgfdeclareimage[width=6em,interpolate=true]{erasmuslogo}{../erasmus-logo}
  \titlegraphic{\pgfuseimage{dsailogo} \hspace{0.2in} \pgfuseimage{erasmuslogo}}
  %\usetheme{default}
  \usetheme{Madrid}
  \usecolortheme{rose}
  \usefonttheme[onlysmall]{structurebold}
}

\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{amsmath,amssymb}
\usepackage{graphics}
\usepackage{ragged2e}
\usepackage{array}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{30mm}
\setlength{\TPVertModule}{\TPHorizModule}
\textblockorigin{10mm}{10mm}
\usepackage[english]{babel}
\usepackage{listings}
\setbeamercovered{dynamic}

\AtBeginSection[]{
  \begin{frame}<beamer>
  \frametitle{Outline}
  \tableofcontents[currentsection]
  \end{frame}
}

\title[Machine Learning]{Machine Learning\\Reinforcement Learning}
\author{dsai.asia}
\institute[]{
  Asian Data Science and Artificial Intelligence Master's Program}
\date{}

% My math definitions

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\mathtt{#1}}
\newcommand{\ten}[1]{\mathcal{#1}}
\newcommand{\crossmat}[1]{\begin{bmatrix} #1 \end{bmatrix}_{\times}}
\renewcommand{\null}[1]{{\cal N}(#1)}
\newcommand{\class}[1]{{\cal C}_{#1}}
\def\Rset{\mathbb{R}}
\def\Expec{\mathbb{E}}
\def\Pset{\mathbb{P}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\cov}{Cov}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\var}{Var}
\def\norm{\mbox{$\cal{N}$}}

\newcommand{\stereotype}[1]{\guillemotleft{{#1}}\guillemotright}

\newcommand{\myfig}[3]{\centerline{\includegraphics[width={#1}]{{#2}}}
    \centerline{\scriptsize #3}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%             CONTENTS START HERE

%\setbeamertemplate{navigation symbols}{}

\frame{\titlepage}

%--------------------------------------------------------------------
%\part<presentation>{Part name}
%
%\frame{\partpage}

\begin{frame}
\frametitle{Readings}

Readings for these lecture notes:
\begin{itemize}
\item[-] Ng, A. (2017), \textit{Reinforcement Learning and Control}.
  Lecture note set 12 for CS229, Stanford University.
\item[-] Sutton, R.S. and Barto, A.G. (2018), \textit{Reinforcement
  Learning: An Introduction}, 2nd ed., MIT Press.
\end{itemize}

These notes contain material $\copyright$ Ng (2017) and Sutton and
Barto (2018).

\end{frame}

%======================================================================
\section{Introduction}
%======================================================================

\begin{frame}{Introduction}

  Now we consider the problem of \alert{reinforcement learning}.

  \medskip

  In the supervised learning setting, we give the machine examples of
  what it should do in particular circumstances.

  \medskip

  Sometimes it is not easy or even possible to come up with such a
  training set.  Examples include robot motion control, game playing
  systems, stock trading policies, dynamic allocation of resources to
  a Web application, ...

  \medskip

  In some of these domains, rather than providing a definite answer,
  we provide the machine with a \alert{reward function} indicating when
  it is doing well and when it is doing poorly.


\end{frame}


\begin{frame}{Introduction}

  Example reward functions:
  \begin{itemize}
    \item Robot control: give positive rewards for moving forward and
      negative rewards for falling over.
    \item Game playing: give positive rewards for winning, negative
      rewards for losing.
    \item Robotic exploration: give positive rewards for expanding
      the ``frontier,'' negative rewards for going over the same old
      ground.
    \item Robotic vacuum cleaning: give positive rewards for cleaning
      dirty areas, negative rewards for spending time in clean areas.
    \item Web application resource allocation: positive rewards for
      serving requests quickly, negative rewards for using excessive
      resources.
  \end{itemize}

  \medskip

  Usually, reinforcement learning problems are posed as \alert{Markov
    decision processes}.
  
\end{frame}

%======================================================================
\section{Markov decision processes}
%======================================================================

\begin{frame}{Markov decision processes}{Formal definition}

  A Markov decision process is a tuple $(S, A, \{P_{sa}\}, \gamma, R)$,
  where
  \begin{itemize}
  \item $S$ is a set of \alert{states}.
  \item $A$ is a set of \alert{actions}.
  \item $P_{sa}$ are the \alert{state transition probabilities}. For
    each state $s \in S$ and action $a \in A$ $P_{sa}$ is a
    distribution over $S$. $P_{s_ia_j}(s_k)$ gives the probability
    that we transition to state $s_k$ when we perform action $a_j$ in
    state $s_i$.
  \item $\gamma \in [0,1)$ is called the \alert{discount factor}.
  \item $R : S \times A \mapsto \Rset$ is the \alert{reward function}.
    Sometimes we use $R : S \mapsto \Rset$.
  \end{itemize}
  
\end{frame}


\begin{frame}{Markov decision processes}{Payoff}

  A MDP proceeds as follows:
  \begin{itemize}
  \item Start in some state $s_0$.
  \item Choose action $a_0 \in A$.
  \item Sample $s_1 \sim P_{s_0a_0}$.
  \item Choose action $a_1$.
  \item Sample $s_2 \sim P_{s_1a_1}$.
  \item Repeat...
  \end{itemize}

  \medskip

  We can represent the progress of the process as
  \[ s_0 \stackrel{a_0}{\rightarrow} s_1 \stackrel{a_1}{\rightarrow}
     s_2 \stackrel{a_2}{\rightarrow} s_3 \stackrel{a_3}{\rightarrow} \cdots
  \]

  Given such a sequence, we can compute the \alert{total payoff} as
  \[ R(s_0,a_0) + \gamma R(s_1,a_1) + \gamma^2 R(s_2,a_2) + \cdots \]
  or
  \[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots .\]
  
\end{frame}


\begin{frame}{Markov decision processes}{Simple MDP example}

\myfig{3in}{mdp}{\url{https://en.wikipedia.org/wiki/Markov_decision_process}}

\end{frame}


\begin{frame}{Markov decision processes}{Discount}

  The reward at timestep $t$ is \alert{discounted} by a factor $\gamma^t$.

  \medskip

  In some applications, time might not matter and we would use $\gamma=1$.

  \medskip

  In many other applications, we want to accrue positive rewards as quickly
  as possible, so $\gamma < 1$.

  \medskip

  When $R(\cdot)$ is the amount of money we make, $\gamma$ would
  represent the effect of interest (money accrued today is more
  valuable in the long term than money accrued tomorrow).

\end{frame}


\begin{frame}{Markov decision processes}{Policy, value function}

  How should our learning agent behave?

  \medskip

  A \alert{policy} is a function $\pi : S \mapsto A$ mapping from states
  to actions.

  \medskip

  \alert{Executing} policy $\pi$ means when we are in state $s$, we
  take action $a = \pi(s)$.

  \medskip

  The \alert{value function} for policy $\pi$ is
  \[ V^\pi(s)=\Expec[ R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+\cdots \mid s_0 = s ; \pi], \]
  i.e., the expected sum of discounted rewards upon starting in state
  $s$ and taking actions according to $\pi$.
  
\end{frame}


\begin{frame}{Markov decision processes}{Bellman equations}

  Given a fixed policy $\pi$, the value function $V^\pi$ satisfies the
  \alert{Bellman equations}
  \[ V^\pi(s) = R(s) + \gamma\sum_{s'\in S} P_{s\pi(s)}(s')V^\pi(s') . \]
  $R(s)$ is called the \alert{immediate reward} we get for starting in
  state $s$.

  \medskip

  The second term (the summation) can be rewritten $\Expec_{s' \sim
    P_{s\pi(s)}}[V^\pi(s')]$, i.e., the expected sum of discounted
  rewards for starting in state $s'$, where $s'$ is distributed
  according to $P_{s\pi(s)}$, the distribution over states resulting
  from action $\pi(s)$ in state $s$.

\end{frame}


\begin{frame}{Markov decision processes}{Solving the Bellman equations}

  The Bellman equations give us a simple way to determine $V^\pi$ for
  finite-state MDPs ($|S| < \infty$).

  \medskip

  Simply write down the equation $V^\pi(s)$ for each $s\in S$.

  \medskip

  When $R$, $P_{sa}$, and $\pi$ are fixed, we have $|S|$ linear
  equations in $|S|$ unknowns.

  \medskip

  [Exercise: find $V^\pi$ for a small MDP such as a robot maze escape.]
  
\end{frame}


\begin{frame}{Markov decision processes}{Optimal value function and policy}

  We define the \alert{optimal value function} as
  \[ V^*(s) = \max_\pi V^\pi(s) .\]
  We can write the Bellman equations for the optimal value function as
  \[ V^*(s) = R(s) + \max_{a\in A}\gamma\sum_{s'\in S}P_{sa}(s')V^*(s') .\]
  We have the immediate reward plus the maximum over all $a$ of the
  expected future sum of discounted rewards we'll get from $a$.

  \medskip

  We define the \alert{optimal policy} $\pi^* : S \mapsto A$ as
  \begin{equation}
    \label{optpol}
    \pi^*(s) = \argmax_{a \in A} \sum_{s'\in S} P_{sa}(s')V^*(s')
  \end{equation}
  
\end{frame}


\begin{frame}{Markov decision processes}{Optimal policy}

  Some facts:
  \[ \forall s, V^*(s) = V^{\pi^*}(s). \]
  \[ \forall s, \pi, V^{\pi^*}(s) \ge V^\pi(s). \]
  i.e., $\pi^*$ as defined in Equation (1) is the optimal policy.

  \medskip

  Note that $\pi^*$ is optimal for \textit{all} states $s$.
  So there is no need to modify $\pi^*$ according to the start state.

  \medskip

  So the question, then, is \alert{how can we find $\pi^*$ for a given MDP?}
  
\end{frame}

%======================================================================
\section{Value iteration and policy iteration}
%======================================================================

\begin{frame}{Value iteration and policy iteration}{Value iteration}

  For the time being, we assume our MDP has a finite state space and
  action space ($|S| < \infty$, $|A| < \infty$).

  \begin{block}{\textbf{Algorithm} \textsc{Value-Iteration}}
    \begin{tabbing}
      xxx \= xxx \= xxx \= \kill    
      \> For each state $s$ \\
      \> \> $V(s) \leftarrow 0$ \\
      \> Repeat until convergence \\
      \> \> For each state $s$ \\
      \> \> \>
      $V(s) \leftarrow R(s) + \max_{a\in A} \gamma \sum_{s'\in S} P_{sa}(s')V(s')$
    \end{tabbing}
  \end{block}

  \medskip

  The inner loop can be \alert{synchronous} (compute all values
  before overwriting) or \alert{asynchronous} (compute and update values
  one by one in some order).

  \medskip

  Theorem: \alert{under synchronous or asynchronous value iteration, $V$ converges to $V^*$}.
    
\end{frame}


\begin{frame}{Value iteration and policy iteration}{Policy iteration}

  After running value iteration, we use Equation (1) to find $\pi^*$.

  \medskip

  \alert{Policy iteration} computes $\pi^*$ somewhat more directly:
  \begin{block}{\textbf{Algorithm} \textsc{Policy-Iteration}}
    \begin{tabbing}
      xxx \= xxx \= xxx \= \kill    
      \> Initialize $\pi$ randomly \\
      \> Repeat until convergence \\
      \> \> $V \leftarrow V^\pi$ \\
      \> \> For each state $s$ \\
      \> \> \> $\pi(s) \leftarrow \argmax_{a\in A}\sum_{s'\in S}P_{sa}(s')V(s')$
    \end{tabbing}
  \end{block}

  \medskip

  Theorem: after a finite number of iterations, \alert{$V$ will converge to
  $V^*$ and $\pi$ will converge to $\pi^*$}.
  
\end{frame}


\begin{frame}{Value iteration and policy iteration}{Which to use?}

  Policy iteration converges quickly for small MDPs.

  \medskip

  However, computing $V^\pi$ requires solving a prohibitively large
  linear system for large problems.

  \medskip

  Value iteration is therefore more often used in practice.
  
\end{frame}

%======================================================================
\section{Learning a model for a MDP}
%======================================================================

\begin{frame}{Learning a model for a MDP}{Assumptions thus far}

  So we see that reinforcement learning is straightforward when
  \begin{itemize}
  \item $|S| < \infty$
  \item $|A| < \infty$
  \item $P_{sa}$ is given
  \item $R(s)$ is given
  \item The outcome $s'$ obtained from executing action $a$ in state
    $s$ is observable.
  \item The reward $R(s)$ obtained from state $s$ is immediately
    observable.
  \item The \alert{horizon} is infinite (the total payoff calculation
    is an infinite sum).
  \end{itemize}

  \medskip

  We would want to relax each of these assumptions.

  \medskip

  Relaxing assumptions 3 and 4 require \alert{estimating $P_{sa}$
    and/or $R(s)$ from data}.

  \medskip

  [Think about what is known in example real-world problems.]

\end{frame}


\begin{frame}{Learning a model for a MDP}{Estimating $P_{sa}$}

  When $P_{sa}$ is unknown, we run the MDP multiple times in a
  simulator or the real world and observe the behavior of the system:
  \[ s_0^{(1)} \stackrel{a_0^{(1)}}{\rightarrow}
  s_1^{(1)} \stackrel{a_1^{(1)}}{\rightarrow}
  s_2^{(1)} \stackrel{a_2^{(1)}}{\rightarrow}
  s_3^{(1)} \stackrel{a_3^{(1)}}{\rightarrow} \cdots \]
  \[ s_0^{(2)} \stackrel{a_0^{(2)}}{\rightarrow}
  s_1^{(2)} \stackrel{a_1^{(2)}}{\rightarrow}
  s_2^{(2)} \stackrel{a_2^{(2)}}{\rightarrow}
  s_3^{(2)} \stackrel{a_3^{(2)}}{\rightarrow} \cdots \]

  Examples:
  \begin{itemize}
  \item For an inverted pendulum, we run until the pendulum falls over.
  \item For a game, we try a large but finite number of moves.
  \end{itemize}

\end{frame}


\begin{frame}{Learning a model for a MDP}{Estimating $P_{sa}$}

  \alert{With enough experience}, the maximum likelihood estimate of
  the state transition probabilities are
  \[ P_{sa}(s') = \frac{\text{\# of times we took action $a$ in state $s$ and got to $s'$}}{\text{\# of times we took action $a$ in state $s$}} .\]

  (When we obtain $0/0$ we can assume $P_{sa}(s') = 1/|S|$.)

  \medskip

  To do this efficiently, we just need to record the counts for the
  numerator and denominator over time and increment the appropriate
  counts each time we take an action and observe the result.

  \medskip

  Estimating $R$ is similar. Just keep track of the number of times we
  entered state $s$ and what reward we received in order to calculate
  the expected reward.
  
\end{frame}


\begin{frame}{Learning a model for a MDP}{Estimating $P_{sa}$}

  Putting value iteration together with model learning for $P_{sa}$, we obtain
  \begin{block}{\textbf{Algorithm} \textsc{Value-Iteration-With-Transition-Learning}}
    \begin{tabbing}
      xxx \= xxx \= xxx \= \kill
      \> Initialize $\pi$ randomly \\
      \> Repeat \\
      \> \> Execute $\pi$ in the MDP for some number of trials \\
      \> \> Use accumulated experience to update $P_{sa}$ \\
      \> \> Use value iteration to estimate $V$ \\
      \> \> Update $\pi$ to be the greedy policy for $V$ \\
    \end{tabbing}
  \end{block}

  \medskip

  Value iteration will converge more quickly if we begin with the $V$
  obtained in the previous step.

  \medskip

  Note that there is a related, more stochastic algorithm called
  \alert{Q-learning} that learns the expected value of taking action
  $a$ in state $s$.
  
\end{frame}

%======================================================================
\section{Temporal-Difference Methods}
%======================================================================

\begin{frame}{Temporal-Difference Methods}{Introduction}

  \alert{TD learning} combines ideas from Monte Carlo methods with
  ideas from dynamic programming.\footnote{``Dynamic programming''
    refers to any computational algorithm that
    solves a complex problem by breaking it down into smaller problems
    recursively.}

  \medskip

  TD methods update estimates of $V$ or $Q$ iteratively based on
  other learned estimates.

\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD prediction}

  Our first TD method is \alert{TD prediction}, which is the TD
  version of policy evaluation, aiming to estimate $V^\pi$.

  \medskip

  Monte Carlo methods wait until the real return following a visit
  is known:
  $$V(S_t) \leftarrow V(s_t) + \alpha \left[ G_t - V(s_t) \right],$$
  where $G_t$ is the \alert{actual return} obtained in state $s_t$
  following $\pi$ to the end of the episode.

  \medskip

  TD methods don't wait until $G_t$ is known. Instead, the simplest
  TD method, TD(0), makes the update
  $$ V(s_t) \leftarrow V(s_t) + \alpha \left[ R_{t+1} + \gamma V(s_{t+1})
    - V(s_t) \right] $$
  right after transitioning to $s_{t+1}$ and receiving $R_{t+1}$.

\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD prediction}

  Take note of the difference: MC uses a target of $G_t$, whereas
  TD(0) uses a target of $R_{t+1} + \gamma V(s_{t+1})$.

  \medskip
  
  \begin{block}{\textbf{Algorithm} \textsc{Tabular TD(0) for $V^\pi$}}

  \begin{tabbing}
    xxx \= xxx \= xxx \= \kill
    \> Input: policy $\pi$, learning rate $\alpha$ \\
    \> Initialize $V(s)$ for all $s \in S$ arbitrarily except $V(\mathrm{terminal}) = 0$\\
    \> For each episode: \\
    \> \> Initialize $s$ \\
    \> \> For each step in episode: \\
    \> \> \> $a \leftarrow \pi(s)$ \\
    \> \> \> $s' \sim P_{sa}$ \\
    \> \> \> $V(s) \leftarrow V(s) + \alpha \left[ R(s') + \gamma V(s') - V(s)\right]$ \\
  \end{tabbing}

  \end{block}

\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD prediction}

  There's a nice illustration of the difference between MC and TD
  policy evaluation in Sutton and Barto (p.\ 122): driving home from
  work every day and continually updating an estimate of how long it
  will take to get home.

  \medskip

  Reward is the elapsed time on each leg of your journey (when we
  switch to control, we would have to use negative reward).

  \medskip
  
  \myfig{4in}{sutton-fig6-1}{Sutton and Barto (2018), Figure 6.1:
  MC (left) vs.\ TD (right).}
  
\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD prediction}

  Both the MC and TD methods are \alert{guaranteed to converge} to $V^\pi$
  if $\alpha$ is small enough and decreases over time.

  \medskip

  MC is fine for short episodes, but TD is preferred when episodes
  are long or infinite.

  \medskip

  Empirical studies usually find that TD methods converge faster than
  MC.
  
\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD control: SARSA}

  Now we consider the \alert{control} problem, the task of finding $\pi^*$.

  \medskip

  SARSA is an \alert{on-policy} TD policy iteration method.

  \medskip

  We introduce the \alert{action value function} $q_\pi(s,a)$, which
  is the discounted return expected after executing action $a$ in state $s$ then
  following $\pi$.

  \medskip

  The SARSA update is
  $$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha\left[ R(s_{t+1}) + \gamma
    Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) \right]. $$

  The method is called SARSA due to the quintuple $(s_t, a_t, r_{t+1},
  s_{t+1}, a_{t+1})$.
  
\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD control: SARSA}

  \begin{block}{\textbf{Algorithm} \textsc{SARSA on-policy TD control}}
  \begin{tabbing}
    xxx \= xxx \= xxx \= \kill
    \> Input: learning rate $\alpha$, exploration probability $\varepsilon$ \\
    \> $\forall s, a$, initialize $Q(s,a)$ arbitrarily
    except $Q(\mathrm{terminal},\cdot) = 0$ \\
    \> For each episode: \\
    \> \> Initialize $s$, choose $a$ using policy derived from $Q$ (e.g., $\varepsilon$-greedy) \\
    \> \> For each step in episode: \\
    \> \> \> Take action $a$, observe $r$, $s'$ \\
    \> \> \> Choose $a'$ from $s'$ using policy derived from $Q$ (e.g., $\varepsilon$-greedy) \\
    \> \> \> $Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]$ \\
    \> \> \> $s \leftarrow s'; a \leftarrow a'$
  \end{tabbing}
  \end{block}

\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD control: SARSA}

  There's a nice example of SARSA for the ``Windy Gridworld'' example
  in Sutton and Barto on p.\ 130.

  \medskip

  \alert{Exercise:} reproduce the SARSA result shown in the graph on
  p.\ 130, then do Exercise 6.9 on p.\ 131.

\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD control: Q-learning}

  \alert{Q-learning} (Watkins, 1989) was one of the early
  breakthroughs in RL.

  \medskip

  It is also fundamental to the Mnih et al.\ (2013) breakthrough with DQN.

  \medskip

  The Q-learning update is
  $$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ R(s_{t+1}) + \gamma
    \max_a Q(s_{t+1},a) - Q(s_t,a_t) \right]. $$

  \medskip

  Compare with SARSA.

  \medskip
  
  Actions are taken using $\varepsilon$-greedy, but the updates are
  based on the action that would be taken with the optimal policy,
  without $\varepsilon$-greedy.

  \medskip

  Because of this difference, Q-learning is an \alert{off-policy} TD
  method.

\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD control: Q-learning}
  
  \begin{block}{\textbf{Algorithm} \textsc{Q-learning off-policy TD control}}
  \begin{tabbing}
    xxx \= xxx \= xxx \= \kill
    \> Input: learning rate $\alpha$, exploration probability $\varepsilon$ \\
    \> $\forall s, a$, initialize $Q(s,a)$ arbitrarily
    except $Q(\mathrm{terminal},\cdot) = 0$ \\
    \> For each episode: \\
    \> \> Initialize $s$ \\
    \> \> For each step in episode: \\
    \> \> \> Choose $a$ using policy derived from $Q$ (e.g., $\varepsilon$-greedy) \\
    \> \> \> Take action $a$, observe $r$, $s'$ \\
    \> \> \> $Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$ \\
    \> \> \> $s \leftarrow s'$
  \end{tabbing}
  \end{block}

  \medskip
  
  Note that Q-learning is a little simpler than SARSA, and note
  precisely why it is off-policy.

\end{frame}


\begin{frame}{Temporal-Difference Methods}{TD control: Q-learning}

  There is a nice demonstration of the differences between SARSA and
  Q-learning, the ``Cliff Walking'' example in Sutton and Barto on
  p.\ 132.

  \medskip

  \alert{Exercise}: implement SARSA and Q-learning for the cliff walking
  example to demonstrate that Q-learning learns an optimal policy that is
  different from (and better than) the $\varepsilon$-greedy policy learned
  by SARSA.
  
\end{frame}


%======================================================================
\section{Continuous state MDPs}
%======================================================================

\begin{frame}{Continuous state MDPs}{Examples}

  Now that we can estimate models for $P_{sa}$ and $R$, we have
  the following remaining assumptions:
  \begin{itemize}
  \item $|S| < \infty$
  \item $|A| < \infty$
  \item The outcome $s'$ obtained from executing action $a$ in state
    $s$ is observable.
  \item The reward $R(s)$ obtained from state $s$ is immediately
    observable.
  \end{itemize}

  \medskip

  What about \alert{relaxing the finite state assumption}?

  \medskip

  In a car or ground robot, we might have a state
  $(x,y,\theta,\dot{x},\dot{y},\dot{\theta})$. $S = \Rset^6$.

  \medskip

  In the inverted pendulum problem, we have
  $(x,\theta,\dot{x},\dot{\theta})$. $S=\Rset^4$.

  \medskip

  For an aircraft, we have $(x,y,z,\phi,\theta,\psi,\dot{x},\dot{y},\dot{z},\dot{\phi},\dot{\theta},\dot{\psi})$. $S = \Rset^{12}$.

\end{frame}


\begin{frame}{Continuous state MDPs}{Discretization}

  Depending on the situation, we may be able to \alert{discretize} the state space.

  \medskip

  If so, we can simply run value iteration or policy iteration on the
  discrete approximation to the state space.

  \medskip

  For example, with $S=\Rset^2$, we use a \alert{grid}. Rather than $\vec{s} = (s_1,s_2) \in \Rset^2$ we break up the range of $s_1$ into $k$ bins and $s_2$ into $l$ bins and use $\overline{S} = \{1,...,k\}\times \{1,...,l\}$.

  \medskip

  \myfig{1.5in}{ng-discrete}{Ng (2017), CS 229 lecture notes, set 12}
  
\end{frame}


\begin{frame}{Continuous state MDPs}{Discretization}

  Problem 1 with discretization: \alert{inaccuracy}. Suppose the value
  $V(s)$ was really linear in $s$. We might obtain something like this:

  \myfig{2.5in}{ng-approx}{Ng (2017), CS 229 lecture notes, set 12}

\end{frame}


\begin{frame}{Continuous state MDPs}{Discretization}

  Problem 2 with discretization: \alert{the curse of dimensionality}.

  \medskip

  The number of parameters we have to estimate is exponential in the number
  of variables in the state.

  \medskip

  Ng:
  \begin{itemize}
  \item Discretization is good for 1D and 2D problems.
  \item Discretization sometimes works with clever representations for 3D-6D problems.
  \item It never works for higher dimensional problems!
  \end{itemize}
  
\end{frame}


\begin{frame}{Continuous state MDPs}{Value function approximation}

  The next technique in arsenal for dealing with continuous state
  spaces is \alert{value function approximation}.

  \medskip

  Value function approximation works best when we have a
  \alert{simulator} for the MDP at hand or when running trials in the
  real world is fast and cheap.

  \medskip

  \myfig{2in}{ng-sim}{Ng (2017), CS 229 lecture notes, set 12}

  \medskip

  [Simulating an inverted pendulum is straightforward if all
    parameters like the pendulum mass are known. Think about
    simulators for other tasks.]

\end{frame}


\begin{frame}{Continuous state MDPs}{Value function approximation}

  We first need to know $P_{sa}$. One possible form is
  \[ \vec{s}_{t+1} = \mat{A}\vec{s}_t + \mat{B}\vec{a}_t + \vec{\epsilon}_t, \]
  where $\epsilon_t \sim {\cal N}(0,\Sigma)$.

  \medskip

  $\mat{A}$ and $\mat{B}$ might be given, or we could estimate them.

\end{frame}


\begin{frame}{Continuous state MDPs}{Value function approximation}

  To estimate $\mat{A}$ and $\mat{B}$, first we collect data from the
  simulation or real world MDP:
  \begin{eqnarray*}
  s_0^{(1)} \stackrel{a_0^{(1)}}{\rightarrow}
  s_1^{(1)} \stackrel{a_1^{(1)}}{\rightarrow}
  s_2^{(1)} \stackrel{a_2^{(1)}}{\rightarrow} \cdots
   \stackrel{a_{T-1}^{(1)}}{\rightarrow} s_T^{(1)} \\
  s_0^{(2)} \stackrel{a_0^{(2)}}{\rightarrow}
  s_1^{(2)} \stackrel{a_1^{(2)}}{\rightarrow}
  s_2^{(2)} \stackrel{a_2^{(2)}}{\rightarrow} \cdots
  \stackrel{a_{T-1}^{(2)}}{\rightarrow} s_T^{(1)}
  \end{eqnarray*}

  \medskip
  
  The maximum likelihood estimator, if
  $\epsilon_t \sim {\cal N}(0,\sigma^2\mat{I})$, would be

  \[ \mat{A}^*,\mat{B}^* = \argmin_{\mat{A},\mat{B}} \sum_{i=1}^m \sum_{t=0}^{T-1}
  \left\| \vec{s}_{t+1}^{(i)}-\left(\mat{A}\vec{s}_t^{(i)} + \mat{B}\vec{a}_t^{(i)}\right) \right\|^2 .\]
  (If $\Sigma$ is unknown, we could also estimate it.)

\end{frame}


\begin{frame}{Continuous state MDPs}{Value function approximation}

  Once we have $\mat{A}$, $\mat{B}$, and possibly $\Sigma$, we
  can either build a \alert{deterministic} model in which we assume
  \[ \vec{s}_{t+1} = \mat{A}\vec{s}_t + \mat{B}\vec{a}_t \]
  precisely, or a \alert{stochastic} model in which we treat
  $\vec{s}_{t+1}$ as random.

  \medskip

  Other alternatives: add nonlinear mappings $\phi_s$ and $\phi_a$ to obtain
  \[ \vec{s}_{t+1} = \mat{A}\phi_s(\vec{s}_t) + \mat{B}\phi_a(\vec{a}_t), \]
  or use locally weighted linear regression to estimate $\vec{s}_{t+1}$ from
    $\vec{s}_t$ and $\vec{a}_t$.
  
\end{frame}


\begin{frame}{Continuous state MDPs}{Value function approximation}

  Modeling the state transition is just the first step!

  \medskip

  Next, we perform \alert{fitted value iteration} to approximate the
  value function.

  \medskip

  For simplicity, we'll assume $S = \Rset^n$ but $A = \{ 1, \ldots, k \}$ for
  a small number $k$ of possible discrete actions.

  \medskip

  (Even if $A$ is really continuous, discretizing $A$ is often
  feasible due to lower dimensionality than $S$.)

  \medskip

  We will essentially, then, approximate $V(\vec{s})$ as a linear or
  nonlinear function of the state:
  \[ V(\vec{s}) = \vec{\theta}^\top\vec{\phi}(\vec{s}) \]
  where $\vec{\phi}$ is an appropriate feature mapping.

  \medskip

  For each state in each sampled state transition sequence, we will
  compute a training signal $y^{(i)}$ that approximates $R(\vec{s}) +
  \gamma \max_a\Expec_{\vec{s}'\sim P_{\vec{s}a}}[V(\vec{s}')]$, i.e.,
  the value function.
  
\end{frame}


\begin{frame}{Continuous state MDPs}{Value function approximation}

  Here is the complete algorithm:
  \begin{tabbing}
    xxx \= xxx \= xxx \= xxx \= \kill
    \textbf{Algorithm} \textsc{Fitted-Value-Iteration} \\
    \> Randomly sample $m$ states $\vec{s}^{(1)},\vec{s}^{(2)},\ldots,\vec{s}^{(m)} \in S$ \\
    \> $\vec{\theta} \leftarrow \vec{0}$ \\
    \> Repeat \\
    \> \> For $i \in 1..m$ \\
    \> \> \> For each action $a \in A$ \\
    \> \> \> \> Sample $\vec{s}_1', \ldots, \vec{s}_k' \sim
    P_{\vec{s}^{(i)}a}$ (using a model of the MDP) \\
    \> \> \> \> // Set $q(a)$ to an estimate of $R(\vec{s}^{(i)})+\gamma \Expec_{\vec{s}'\sim P_{\vec{s}^{(i)}a}}[V(\vec{s}')]$ \\
    \> \> \> \> $q(a) \leftarrow \frac{1}{k}\sum_{j=1}^k R(\vec{s}^{(i)}) + \gamma V(\vec{s}'_j)$ \\
    \> \> \> // Set $y^{(i)}$ to an estimate of $R(\vec{s}^{(i)})+\gamma \max_a \Expec_{\vec{s}'\sim P_{\vec{s}^{(i)}a}}[V(\vec{s}')]$ \\    \> \> \> $y^{(i)} \leftarrow \max_a q(a)$ \\
    \> \> $\vec{\theta} \leftarrow \argmin_{\vec{\theta}} \frac{1}{2}\sum_{i=1}^m \left( \vec{\theta}^\top \phi(\vec{s}^{(i)}) - y^{(i)}\right)^2$ \\
  \end{tabbing}
  
\end{frame}


\begin{frame}{Continuous state MDPs}{Value function approximation}

  Rather than linear regression, we can use any appropriate regression
  algorithm, such as locally weighted linear regression.

  \medskip
  
  Unfortunately, fitted value iteration is \alert{not guaranteed to converge}
  like discrete-space value iteration.

  \medskip

  In practice, it does converge for many problems.

  \medskip

  If we have a deterministic model of the MDP, then $k=1$ is sufficient.

  \medskip

  The \alert{approximately optimal policy} is determined by
  $V(\vec{s})$, which is an approximation to $V^*$:
  \[ \pi(\vec{s}) = \argmax_a \Expec_{\vec{s}'\sim P_{\vec{s}a}}[V(\vec{s}')] . \]
  
\end{frame}

%======================================================================
\section{Finite-horizon MDPs}
%======================================================================

\begin{frame}{Finite-horizon MDPs}

  Up to now, we always assumed an \alert{infinite horizon}
  (our learning agent's total payoff is a infinite sum).
  
  \medskip

  However, in the real world, we would often want to consider a
  \alert{finite horizon} MDP $({\cal S},{\cal A},P_{sa},T,R)$ with $T>0$
  placing a \alert{limit} on the time the agent has to accrue its
  payoff:
  \[ R(s_0,a_0) + \gamma R(s_1,a_1) + \cdots + R(s_T,a_T) \]

  \medskip

  This leads to some important consequences, especially that the
  optimal policy becomes \alert{non-stationary} (different depending
  on the start state).

  \medskip

  With some other generalizations ($P_{sa}$ and $R$ are dynamic, i.e.,
  changing over time), value iteration turns into a \alert{dynamic
    programming} algorithm for solving the Bellman equations.
  
\end{frame}

%======================================================================
\section{Linear quadratic regulation (LQR)}
%======================================================================

\begin{frame}{Linear quadratic regulation (LQR)}

  Suppose ${\cal S} = \Rset^n$ and ${\cal A} = \Rset^d$.

  \medskip

  Suppose also \alert{linear transitions}
  \[ \vec{s}_{t+1} = \mat{A}\vec{s}_t + \mat{B}\vec{a}_t + \vec{w}_t \]
  with $\vec{w}_t \sim {\cal N}(\vec{0},\mat{\Sigma}_t)$.

  \medskip

  Finally, suppose we have \alert{quadratic rewards}
  \[ R^{(t)}(\vec{s}_t,a_t) = - \vec{s}_t^\top\mat{U}_t\vec{s}_t
                             - \vec{a}^\top \mat{W}\vec{a}_t, \]
  where $\mat{U}_t \in \Rset^{n\times n}$ and $\mat{W}_t \in
  \Rset^{d\times d}$ are positive definite, forcing the reward to
  always be negative.
  
\end{frame}


\begin{frame}{Linear quadratic regulation (LQR)}

  The quadratic rewards mean that we want to take \alert{small, smooth
  actions} that keep the system \alert{as close to $\vec{s}_t = \vec{0}$} as
  possible.

  \medskip

  Think about deviation of the inverted pendulum, deviation of our car
  from the center of the lane, etc.

  \medskip

  The \alert{LQR algorithm} first estimates $\mat{A}_t$, $\mat{B}_t$,
  and $\mat{\Sigma}_t$, then uses dynamic programming to find the
  optimal policy.
  
\end{frame}

%======================================================================
\section{LQR for nonlinear dynamics}
%======================================================================

\begin{frame}{LQR for nonlinear dynamics}

  Now we suppose that the dynamics, rather than being linear
  \[ \vec{s}_{t+1} = \mat{A}\vec{s}_{t} + \mat{B}\vec{a}_t + \vec{w}_t, \]
  we instead have a nonlinear relationship:
  \[ \vec{s}_{t+1} = f(\vec{s}_t,\vec{a}_t) + \vec{w}_t . \]
  An example is the inverted pendulum, in which the elements of the
  next position vector
  depend on the cosine or sine of the current angle and so on.

  \medskip

  In this situation, we can obtain a Taylor expansion of
  $f(\cdot,\cdot)$ around $\vec{s}_t$ and $\vec{a}_t$ to
  \alert{linearize} the function, then perform LQR as before.

  \medskip

  There is a further generalization called \alert{differential dynamic
    programming} for the case in which the reward function is more
  complicated than quadratic rewards (for example, approximating a
  gold standard trajectory for a rocket).

\end{frame}

%======================================================================
\section{Linear quadratic Gaussian (LQG) regulation}
%======================================================================

\begin{frame}{Linear quadratic Gaussian (LQG) regulation}

  Sometimes we are not able to fully observe $\vec{s}_t$.

  \medskip

  Instead, we may only be able to take an \alert{observation} related
  to $\vec{s}_t$, such taking a picture with a camera or a distance
  scan with a laser or a GPS reading from a GPS device.

  \medskip

  This leads to the idea of the \alert{partially observable Markov
    decision process (POMDP)}.

  \medskip

  We assume that at each timestep $t$ we obtain an observation
  $\vec{z}_t \in {\cal O}$ related to the state $\vec{s}_t$ by a distribution
  $P_{zs}(\vec{z} \mid \vec{s})$.

  \medskip

  A POMDP is thus given by a tuple $({\cal S},{\cal O},{\cal
    A},P_{sa},P_{zs},T,R)$.

  \medskip
  
  In the POMDP setting, we will maintain a \alert{belief} over the states
  $p(\vec{s}_t \mid \vec{z}_1, \ldots, \vec{z}_t)$, and the policy will map
  beliefs to actions rather than crisp states to actions.
  
\end{frame}


\begin{frame}{Linear quadratic Gaussian (LQG) regulation}

  In the special case that the dynamics and observations are both linear
  with Gaussian noise, i.e.,
  \begin{eqnarray*}
    \vec{s}_{t+1} & = & \mat{A}\vec{s}_t + \mat{B}\vec{a}_t + \vec{w}_t \\
    \vec{z}_t & = & \mat{C}\vec{s}_t + \vec{v}_t
  \end{eqnarray*}
  with $\vec{w}_t \sim {\cal N}(\vec{0},\mat{\Sigma}_s)$ and
  $\vec{v}_t \sim {\cal N}(\vec{0},\mat{\Sigma}_z)$, we obtain the
  \alert{Kalman filter} for maintaining our belief $p(\vec{s}_t \mid \vec{z}_1,
  \ldots,\vec{z}_t)$.

  \medskip

  The Kalman filter gives us a mean and covariance for the belief at
  each time $t$.

  \medskip

  Based on mean at time $t$, we can use the regular LQR for
  deterimining the optimal action.

\end{frame}

\end{document}


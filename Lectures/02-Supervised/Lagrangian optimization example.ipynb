{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrangian optimization example\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\mathbf{#1}} \\newcommand{\\mat}[1]{\\mathtt{#1}}$\n",
    "When we want to find a minimum or maximum of a function $f(\\vec{w})$ subject to a constraint $g(\\vec{w}) = 0$,\n",
    "we can use the Lagrangian approach to solving it.\n",
    "\n",
    "$\\newcommand{\\lgr}{{\\cal L}}$\n",
    "The Lagrangian is the function $\\lgr(\\vec{w},\\lambda) = f(\\vec{w}) + \\lambda g(\\vec{w})$.\n",
    "\n",
    "If we can find $(\\vec{w}^*,\\lambda^*)$, a stationary point of $\\lgr$, then $\\vec{w}^*$ is a stationary point of $f$.\n",
    "\n",
    "In class we had the simple example $f(\\vec{w}) = \\|w\\|^2$ and $g(\\vec{w}) = w_1 + w_2 - 4$.\n",
    "\n",
    "By inspection, we can easily see that the solution is $w_1 = w_2 = 2$.\n",
    "\n",
    "For the analysis, we write the Lagrangian $\\lgr(\\vec{w},\\lambda) = w_1^2 + w_2^2 + \\lambda(w_1 + w_2 - 4)$.\n",
    "\n",
    "Clearly, $$\\frac{\\partial \\lgr}{\\partial w_1} = 2 w_1 + \\lambda$$\n",
    "$$\\frac{\\partial \\lgr}{\\partial w_2} = 2 w_2 + \\lambda$$\n",
    "$$\\frac{\\partial \\lgr}{\\partial \\lambda} = w_1 + w_2 - 4.$$\n",
    "\n",
    "To obtain the stationariy point(s) of this system, we set the partials to 0 and solve. From\n",
    "$$\\lambda = -2w_1 = -2w_2,$$\n",
    "we obtain $w_1 = w_2$ then the solution $w_1^* = 2, w_2^* = 2, \\lambda^* = -4$.\n",
    "\n",
    "We already knew $(w_1,w_2) = (2,2)$ by inspection, but now we have confirmed it analytically. We also know it's a minimum by inspection, but to confirm this analytically, we need the Hessian\n",
    "$$\\mat{H}_\\lgr = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 \\lgr}{\\partial^2 \\lambda} & \\frac{\\partial^2 \\lgr}{\\partial \\lambda \\partial w_1} & \\frac{\\partial^2 \\lgr}{\\partial \\lambda \\partial w_2} \\\\\n",
    "\\frac{\\partial^2 \\lgr}{\\partial w_1 \\partial \\lambda} & \\frac{\\partial^2 \\lgr}{\\partial^2 w_1} & \\frac{\\partial^2 \\lgr}{\\partial w_1 \\partial w_2} \\\\ \\frac{\\partial^2 \\lgr}{\\partial w_2 \\partial \\lambda} & \\frac{\\partial^2 \\lgr}{\\partial w_2 \\partial w_1} & \\frac{\\partial^2 \\lgr}{\\partial^2 w_2} \\end{bmatrix} = \n",
    "\\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 2 & 0 \\\\ 1 & 0 & 2 \\end{bmatrix} $$\n",
    "Take a look at the eigenvalues of $\\mat{H}_\\lgr$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: -0.732051, 2.732051, 2.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Hl = np.array([[0, 1, 1], [1, 2, 0], [1, 0, 2]])\n",
    "eigvals, eigvecs = np.linalg.eig(Hl)\n",
    "print(\"Eigenvalues: %f, %f, %f\" % (eigvals[0], eigvals[1], eigvals[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a mixture of positive and negative eigenvalues (-0.732051, 2.732051, and 2.0), the critical point we've found is a *saddle point* of the Lagrangian. It is actually always the case that the solution of the original optimization is a saddle point of the Lagrangian function.\n",
    "\n",
    "To determine if the point is a minimum, maximum, or saddle point of the original optimization problem, we consider the determinant of various submatrices of $\\mat{H}_\\lgr$.\n",
    "\n",
    "For $m$ constraints, we consider the $2m+1$-th up to the $n$th principal minors of $\\mat{H}_\\lgr$. These are the upper left square submatrices of $\\mat{H}_\\lgr$. In our case, $m = 1 $ and $n = 3 = 2m+1$, so we have only one principal minor to consider, $\\mat{H}_\\lgr$ itself. We get its determinant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant of Hl: -4.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Determinant of Hl: %f\" % np.linalg.det(Hl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a minimum, all of the minors' determinants must have a sign of $(-1)^m$ which means negative for $m=1$. Since this is true, we can conclude that the critical point is a minimum of $f(\\vec{w})$! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

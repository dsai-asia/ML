{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Logistic Regression\n",
    "\n",
    "Thus far, the problems we've encountered have been *regression* problems, in which the target $y \\in \\mathbb{R}$.\n",
    "\n",
    "Today we'll start experimenting with *classification* problems, beginning with *binary* classification problems, in which the target $y \\in \\{ 0, 1 \\}$.\n",
    "\n",
    "## Background\n",
    "\n",
    "The simplest approach to classification, applicable when the input feature vector $\\mathbf{x} \\in \\mathbb{R}^n$, is a simple generalization of what we\n",
    "do in linear regression. Recall that in linear regression, we assume that the target is drawn from a Gaussian distribution whose mean is a linear function\n",
    "of $\\mathbf{x}$:\n",
    "\n",
    "$$ y \\sim {\\cal N}(\\theta^\\top \\mathbf{x}, \\sigma^2) $$\n",
    "\n",
    "In logistic regression, similarly, we'll assume that the target is drawn from a Bernoulli distribution with parameter $p$ being the probability of\n",
    "class 1:\n",
    "\n",
    "$$ y \\sim \\text{Bernoulli}(p) $$\n",
    "\n",
    "That's fine, but how do we model the parameter $p$? How is it related to $\\mathbf{x}$?\n",
    "\n",
    "In linear regression, we assume that the mean of the Gaussian is $\\theta^\\top \\mathbf{x}$, i.e., a linear function of $\\mathbf{x}$.\n",
    "\n",
    "In logistic regression, we'll assume that $p$ is a \"squashed\" linear function of $\\mathbf{x}$, i.e.,\n",
    "\n",
    "$$ p = \\text{sigmoid}(\\theta^\\top \\mathbf{x}) = g(\\theta^\\top \\mathbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}. $$\n",
    "\n",
    "Later, when we introduce generalized linear models, we'll see why $p$ should take this form. For now, though, we can simply note that the selection makes\n",
    "sense. Since $p$ is a discrete probability, $p$ is bounded by $0 \\le p \\le 1$. The sigmoid function $g(\\cdot)$ conveniently obeys these bounds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp( -z ))\n",
    "\n",
    "z = np.arange(-5, 5, 0.1)\n",
    "plt.plot(z, f(z), 'b-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)')\n",
    "plt.title('Logistic sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sigmoid approaches 0 as its input approaches $-\\infty$ and approaches 1 as its input approaches $+\\infty$. If its input is 0, its value is 0.5.\n",
    "\n",
    "Again, this choice of function may seem strange at this point, but bear with it! We'll derive this function from a more general principle, the generalized\n",
    "linear model, later.\n",
    "\n",
    "OK then, we now understand that for logistic regression, the assumptions are:\n",
    "\n",
    "1. The *data* are pairs $(\\textbf{x}, y) \\in \\mathbb{R}^n \\times \\{ 0, 1 \\}$.\n",
    "1. The *hypothesis function* is $h_\\theta(\\textbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}$.\n",
    "\n",
    "What else do we need... ? A cost function and an algorithm for minimizing that cost function!\n",
    "\n",
    "## Cost function for logistic regression\n",
    "\n",
    "You can refer to the lecture notes to see the derivation, but for this lab, let's just skip to the chase.\n",
    "With the hypothesis $h_\\theta(\\mathbf{x})$ chosen as above, the log likelihood function $\\ell(\\theta)$ can be derived as\n",
    "$$ \\ell(\\theta) = \\log {\\cal L}(\\theta) =  \\sum_{i=1}^{m}y^{(i)}\\log(h_{\\theta}(\\mathbf{x}^{(i)})) + (1 - y^{(i)})\\log(1 - (h_{\\theta}(\\mathbf{x}^{(i)})) .$$\n",
    "\n",
    "Negating the log likelihood function to obtain a loss function, we have\n",
    "\n",
    "$$ J(\\theta) = - \\sum_{i=1}^m y^{(i)}\\log h_\\theta(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(\\textbf{x}^{(i)})) .$$\n",
    "\n",
    "There is no closed-form solution to this problem like there is in linear regression, so we have to use gradient descent to find $\\theta$ minimizing $J(\\theta)$.\n",
    "Luckily, the function *is* convex in $\\theta$ so there is just a single global minimum, and gradient descent is guaranteed to get us there eventually if we take\n",
    "the right step size.\n",
    "\n",
    "The *stochastic* gradient of $J$, for a single observed pair $(\\mathbf{x}, y)$, turns out to be (see lecture notes)\n",
    "\n",
    "$$\\nabla_J(\\theta) = (h_\\theta(\\mathbf{x}) - y)\\mathbf{x} .$$\n",
    "\n",
    "Give some thought as to whether following this gradient to increase the loss $J$ would make a worse classifier, and vice versa!\n",
    "\n",
    "Finally, we obtain the update rule for the $j$th iteration selecting training pattern $i$:\n",
    "\n",
    "$$ \\theta^{(j+1)} \\leftarrow \\theta^{(j)} + \\alpha(y^{(i)} - h_\\theta(\\textbf{x}^{(i)}))\\textbf{x}^{(i)} .$$ \n",
    "\n",
    "Note that we can perform *batch gradient descent* simply by summing the single-pair gradient over the entire training set before taking a step,\n",
    "or *mini-batch gradient descent* by summing over a small subset of the data.\n",
    "\n",
    "## Example dataset 1: student admissions data\n",
    "\n",
    "This example is from Andrew Ng's machine learning course on Coursera.\n",
    "\n",
    "The data contain students' scores for two standardized tests and an admission decision (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load student admissions data. The data file does not contain headers,\n",
    "# so we use hard coded indices for exam 1, exam2, and the admission decision.\n",
    "\n",
    "data = np.loadtxt('ex2data1.txt',delimiter = ',')\n",
    "exam1_data = data[:,0]\n",
    "exam2_data = data[:,1]\n",
    "X = np.array([exam1_data, exam2_data]).T\n",
    "y = data[:,2]\n",
    "\n",
    "# Output some sample data\n",
    "\n",
    "print('Exam scores', X[0:5,:])\n",
    "print('-----------------------------')\n",
    "print('Admission decision', y[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "idx_0 = np.where(y == 0)\n",
    "idx_1 = np.where(y == 1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(5, 5)) \n",
    "ax = plt.axes()\n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(exam1_data[idx_0], exam2_data[idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(exam1_data[idx_1], exam2_data[idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find good values for $\\theta$ without normalizing the data.\n",
    "We will definitely want to split the data into train and test, however..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# As usual, we fix the seed to eliminate random differences between different runs\n",
    "\n",
    "random.seed(12)\n",
    "\n",
    "# Partion data into training and test datasets\n",
    "\n",
    "m, n = X.shape\n",
    "XX = np.insert(X, 0, 1, axis=1)\n",
    "y = y.reshape(m, 1)\n",
    "idx = np.arange(0, m)\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(m * percent_train)\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important functions needed later\n",
    "\n",
    "Let's put all of our important functions here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):   \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def h(X, theta):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def grad_j(X, y, y_pred):\n",
    "    return X.T @ (y - y_pred) / X.shape[0]\n",
    "    \n",
    "def j(theta, X, y):    \n",
    "    y_pred = h(X, theta)\n",
    "    error = (-y * np.log(y_pred)) - ((1 - y) * np.log(1 - y_pred))\n",
    "    cost = sum(error) / X.shape[0]\n",
    "    grad = grad_j(X, y, y_pred)\n",
    "    return cost[0], grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize theta\n",
    "\n",
    "In any iterative algorithm, we need an initial guess. Here we'll just use zeros for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our parameters, and use them to make some predictions\n",
    "\n",
    "theta_initial = np.zeros((n+1, 1))\n",
    "\n",
    "print('Initial theta:', theta_initial)\n",
    "print('Initial predictions:', h(XX, theta_initial)[0:5,:])\n",
    "print('Targets:', y[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "Here's a function to do batch training for `num_iters` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, theta_initial, alpha, num_iters):\n",
    "    theta = theta_initial\n",
    "    j_history = []\n",
    "    for i in range(num_iters):\n",
    "        cost, grad = j(theta, X, y)\n",
    "        theta = theta + alpha * grad\n",
    "        j_history.append(cost)\n",
    "    return theta, j_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training\n",
    "\n",
    "Here we run the training function for a million batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 1000000 iterations on full training set\n",
    "\n",
    "alpha = .0005\n",
    "num_iters = 1000000\n",
    "theta, j_history = train(X_train, y_train, theta_initial, alpha, num_iters)\n",
    "\n",
    "print(\"Theta optimized:\", theta)\n",
    "print(\"Cost with optimized theta:\", j_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss curve\n",
    "\n",
    "Next let's plot the loss curve (loss as a function of iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(j_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (no normalization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-lab exercise from Example 1 (Total 35 points)\n",
    "\n",
    "That took a long time, right?\n",
    "\n",
    "We'll see if we can do better. We will try the following:\n",
    "\n",
    "1. Try increasing the learning rate $\\alpha$ and starting with a better initial $\\theta$. How much does it help?\n",
    "   - Try at least 2 learning rate $\\alpha$ with 2 difference $\\theta$ (4 experiments)\n",
    "   - Do not forget to plot the loss curve to compare your results\n",
    "\n",
    "2. Better yet, try *normalizing the data* and see if the training converges better. How did it go? \n",
    "   - Be sure to plot loss curves to compare the results with unnormalized and normalized data.\n",
    "\n",
    "3. Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report.\n",
    "\n",
    "Do this work in the following steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 (5 points)\n",
    "\n",
    "Fill in two different values for $\\alpha$ and $\\theta$.\n",
    "\n",
    "Use variable names `alpha1`, `alpha2`, `theta_initial1`, and `theta_initial2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "469661aa6773cd5305bcbca1054c82d7",
     "grade": false,
     "grade_id": "cell-98f6eb46f41c6686",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grade task: change 'None' value to number(s) or function\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "# declare your alphas\n",
    "# alpha1 = None\n",
    "# alpha2 = None\n",
    "\n",
    "# initialize thetas as you want\n",
    "# theta_initial1 = None\n",
    "# theta_initial2 = None\n",
    "\n",
    "# define your num iterations\n",
    "# num_iters = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "053e98ba0c27eb56eadb89857573b428",
     "grade": true,
     "grade_id": "cell-f9ecb4ab4402c8b4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_list = [alpha1, alpha2]\n",
    "print('alpha 1:', alpha1)\n",
    "print('alpha 2:', alpha2)\n",
    "\n",
    "theta_initial_list = [theta_initial1, theta_initial2]\n",
    "print('theta 1:', theta_initial_list[0])\n",
    "print('theta 2:', theta_initial_list[1])\n",
    "\n",
    "print('Use num iterations:', num_iters)\n",
    "\n",
    "# Test function: Do not remove\n",
    "assert alpha_list[0] is not None and alpha_list[1] is not None, \"Alpha has not been filled\"\n",
    "chk1 = isinstance(alpha_list[0], (int, float))\n",
    "chk2 = isinstance(alpha_list[1], (int, float))\n",
    "assert chk1 and chk2, \"Alpha must be number\"\n",
    "assert theta_initial_list[0] is not None and theta_initial_list[1] is not None, \"initialized theta has not been filled\"\n",
    "chk1 = isinstance(theta_initial_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(theta_initial_list[1], (list,np.ndarray))\n",
    "assert chk1 and chk2, \"Theta must be list\"\n",
    "chk1 = ((n+1, 1) == theta_initial_list[0].shape)\n",
    "chk2 = ((n+1, 1) == theta_initial_list[1].shape)\n",
    "assert chk1 and chk2, \"Theta size are incorrect\"\n",
    "assert num_iters is not None and isinstance(num_iters, int), \"num_iters must be integer\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 (5 points)\n",
    "\n",
    "Fill in the code required to train your model on a particular $\\alpha$ and $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea4f55cd93441770beebd895608e4e7",
     "grade": false,
     "grade_id": "cell-77a540a2a0cc2031",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grade task: change 'None, None' value to number(s) or function\n",
    "j_history_list = []\n",
    "theta_list = []\n",
    "for alpha in alpha_list:\n",
    "    for theta_initial in theta_initial_list:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # theta_i, j_history_i = None, None\n",
    "        j_history_list.append(j_history_i)\n",
    "        theta_list.append(theta_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d8b32e551ac03e7e946f2c677e92d0",
     "grade": true,
     "grade_id": "cell-57627ff7e32cd714",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test function: Do not remove\n",
    "assert theta_list[0] is not None and j_history_list[0] is not None, \"No values in theta_list or j_history_list\"\n",
    "chk1 = isinstance(theta_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(j_history_list[0][0], (int, float))\n",
    "assert chk1 and chk2, \"Wrong type in theta_list or j_history_list\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3 (10 points)\n",
    "\n",
    "Write code to plot loss curves for each of the sequences in `j_history_list` from the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f63a65931d5f3da4e50c7cd36990e0f",
     "grade": true,
     "grade_id": "cell-33ed3657769adb04",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f75ff7e6c6cd980abee60df22fe5c731",
     "grade": false,
     "grade_id": "cell-59c862747f0fb871",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.4 (10 points)\n",
    "\n",
    "- Repeat your training, but **normalize** your data before training\n",
    "- Compare the results between normalized data and unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b1cd115f866825c9970f827ee43d67f",
     "grade": false,
     "grade_id": "cell-43c3a360c938e4be",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.5 (5 points)\n",
    "\n",
    "Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your discussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic regression decision boundary\n",
    "\n",
    "Note that when $\\theta^\\top \\textbf{x} = 0$, we have $h_\\theta(\\textbf{x}) = 0.5$. That is, we are\n",
    "equally unsure as to whether $\\textbf{x}$ belongs to class 0 or class 1. The contour at which\n",
    "$h_\\theta(\\textbf{x}) = 0.5$ is called the classifier's *decision boundary*.\n",
    "\n",
    "We know that in the plane, the equation $$ax+by+c=0$$ is the general form of a 2D line. In our case, we have\n",
    "$$\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0$$ as our decision boundary, but clearly, this is just a 2D line\n",
    "in the plane. So when we plot $x_1$ against $x_2$, it is easy to plot the boundary line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_points(X, theta):\n",
    "    v_orthogonal = np.array([[theta[1,0]],[theta[2,0]]])\n",
    "    v_ortho_length = np.sqrt(v_orthogonal.T @ v_orthogonal)\n",
    "    dist_ortho = theta[0,0] / v_ortho_length\n",
    "    v_orthogonal = v_orthogonal / v_ortho_length\n",
    "    v_parallel = np.array([[-v_orthogonal[1,0]],[v_orthogonal[0,0]]])\n",
    "    projections = X @ v_parallel\n",
    "    proj_1 = min(projections)\n",
    "    proj_2 = max(projections)\n",
    "    point_1 = proj_1 * v_parallel - dist_ortho * v_orthogonal\n",
    "    point_2 = proj_2 * v_parallel - dist_ortho * v_orthogonal\n",
    "    return point_1, point_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(5,5)) \n",
    "ax = plt.axes() \n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Logistic regression boundary')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(X[:,0][idx_0], X[:,1][idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(X[:,0][idx_1], X[:,1][idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "point_1, point_2 = boundary_points(X, theta)\n",
    "plt.plot([point_1[0,0], point_2[0,0]],[point_1[1,0], point_2[1,0]], 'g-')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You may have to adjust the above code to make it work with normalized data.\n",
    "\n",
    "### Test set performance\n",
    "\n",
    "Now let's apply the learned classifier to the test data we reserved in the beginning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, y_pred):\n",
    "    return 1 - np.square(y - y_pred).sum() / np.square(y - y.mean()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_soft = h(X_test, theta)\n",
    "y_test_pred_hard = (y_test_pred_soft > 0.5).astype(int)\n",
    "\n",
    "test_rsq_soft = r_squared(y_test, y_test_pred_soft)\n",
    "test_rsq_hard = r_squared(y_test, y_test_pred_hard)\n",
    "test_acc = (y_test_pred_hard == y_test).astype(int).sum() / y_test.shape[0]\n",
    "\n",
    "print('Got test set soft R^2 %0.4f, hard R^2 %0.4f, accuracy %0.2f' % (test_rsq_soft, test_rsq_hard, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, accuracy is probably the more useful measure of goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Loan prediction dataset\n",
    "\n",
    "Let's take another example dataset and see what we can do with it.\n",
    "\n",
    "This dataset is from [Kaggle](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset).\n",
    "\n",
    "The data concern loan applications. It has 12 independent variables, including 5 categorical variables. The dependent variable is the decision \"Yes\" or \"No\" for extending a loan to an individual who applied.\n",
    "\n",
    "One thing we will have to do is to clean the data, by filling in missing values and converting categorical data to reals.\n",
    "We will use the Python libraries pandas and sklearn to help with the data cleaning and preparation.\n",
    "\n",
    "### Read the data and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas. You may need to run \"pip3 install pandas\" at the console if it's not already installed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import the data\n",
    "\n",
    "data_train = pd.read_csv('train_LoanPrediction.csv')\n",
    "data_test = pd.read_csv('test_LoanPrediction.csv')\n",
    "\n",
    "# Start to explore the data\n",
    "\n",
    "print('Training data shape', data_train.shape)\n",
    "print('Test data shape', data_test.shape)\n",
    "\n",
    "print('Training data:\\n', data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training and test data\n",
    "\n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())\n",
    "print('Missing values for test data \\n ------------------------\\n', data_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values\n",
    "\n",
    "We can see from the above table that the `Married` column has 3 missing values in the training dataset and 0 missing values in the test dataset.\n",
    "Let's take a look at the distribution over the datasets then fill in the missing values in approximately the same ratio.\n",
    "\n",
    "You may be interested to look at the [documentation of the Pandas `fillna()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). It's great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ratio of each category value\n",
    "# Divide the missing values based on ratio\n",
    "# Fillin the missing values\n",
    "# Print the values before and after filling the missing values for confirmation\n",
    "\n",
    "print(data_train['Married'].value_counts())\n",
    "\n",
    "married = data_train['Married'].value_counts()\n",
    "print('Elements in Married variable', married.shape)\n",
    "print('Married ratio ', married[0]/sum(married.values))\n",
    "\n",
    "def fill_martial_status(data, yes_num_train, no_num_train):        \n",
    "    data['Married'].fillna('Yes', inplace = True, limit = yes_num_train)\n",
    "    data['Married'].fillna('No', inplace = True, limit = no_num_train)  \n",
    "\n",
    "fill_martial_status(data_train, 2, 1)\n",
    "print(data_train['Married'].value_counts()) \n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the number of examples missing the `Married` attribute is 0.\n",
    "\n",
    "Let's complete the data processing based on examples given and logistic regression model on training dataset. Then we'll get the model's accuracy (goodness of fit) on the test dataset.\n",
    "\n",
    "Here is another example of filling in missing values for the `Dependents` (number of children and other dependents)\n",
    "attribute. We see that categorical values are all numeric except one value \"3+\"\n",
    "Let's create a new category value \"4\" for \"3+\" and ensure that all the data is numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "dependent = data_train['Dependents'].value_counts()\n",
    "\n",
    "print('Dependent ratio 1 ', dependent['0'] / sum(dependent.values))\n",
    "print('Dependent ratio 2 ', dependent['1'] / sum(dependent.values))\n",
    "print('Dependent ratio 3 ', dependent['2'] / sum(dependent.values))\n",
    "print('Dependent ratio 3+ ', dependent['3+'] / sum(dependent.values))\n",
    "\n",
    "def fill_dependent_status(num_0_train, num_1_train, num_2_train, num_3_train, num_0_test, num_1_test, num_2_test, num_3_test):        \n",
    "    data_train['Dependents'].fillna('0', inplace=True, limit = num_0_train)\n",
    "    data_train['Dependents'].fillna('1', inplace=True, limit = num_1_train)\n",
    "    data_train['Dependents'].fillna('2', inplace=True, limit = num_2_train)\n",
    "    data_train['Dependents'].fillna('3+', inplace=True, limit = num_3_train)\n",
    "    data_test['Dependents'].fillna('0', inplace=True, limit = num_0_test)\n",
    "    data_test['Dependents'].fillna('1', inplace=True, limit = num_1_test)\n",
    "    data_test['Dependents'].fillna('2', inplace=True, limit = num_2_test)\n",
    "    data_test['Dependents'].fillna('3+', inplace=True, limit = num_3_test)\n",
    "\n",
    "fill_dependent_status(9, 2, 2, 2, 5, 2, 2, 1)\n",
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "\n",
    "# Convert category value \"3+\" to \"4\"\n",
    "\n",
    "data_train['Dependents'].replace('3+', 4, inplace = True)\n",
    "data_test['Dependents'].replace('3+', 4, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once missing values are filled in, you'll want to convert strings to numbers.\n",
    "\n",
    "Finally, here's an example of replacing missing values for a numeric attribute. Typically, we would use the mean of the attribute over the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['LoanAmount'].value_counts())\n",
    "\n",
    "LoanAmt = data_train['LoanAmount'].value_counts()\n",
    "\n",
    "print('mean loan amount ', np.mean(data_train[\"LoanAmount\"]))\n",
    "\n",
    "loan_amount_mean = np.mean(data_train[\"LoanAmount\"])\n",
    "\n",
    "data_train['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 22)\n",
    "data_test['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "338b0327bbf4b37bb74ca23c46b81e52",
     "grade": false,
     "grade_id": "cell-2e5822ee2e432273",
     "locked": true,
     "points": 65,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Take-home exercise (65 points)\n",
    "\n",
    "Using the data from Example 2 above, finish the data cleaning and\n",
    "preparation. Build a logistic regression model based on the\n",
    "cleaned dataset and report the accuracy on the test and training sets.\n",
    "\n",
    "- Set up $\\mathbf{x}$ and $y$ data (10 points)\n",
    "- Train a logistic regression model and return the values of $\\theta$ and $J$ you obtained. Find the best $\\alpha$ you can; you may find it best to normalize before training. (30 points)\n",
    "- Using the best model parameters $\\theta$ you can find, run on the test set and get the model's accuracy. (10 points)\n",
    "- Summarize what you did to find the best results in this take home exercise. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To turn in\n",
    "\n",
    "Turn in this Jupyter notebook with your solutions to he exercises and your experiment reports,\n",
    "both for the in-lab exercise and the take-home exercise. Be sure you've discussed what\n",
    "you learned in terms of normalization and data cleaning and the results\n",
    "you obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

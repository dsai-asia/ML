{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Midterm, Aug Semester, 2020\n",
    "\n",
    "In this exam, you will demonstrate your understanding of the material from the lectures, tutorials, and problem sets.\n",
    "\n",
    "Exam submission is on Gradescope. Before you begin the exam, make sure you have created an account at [http://gradescope.com](http://gradescope.com). Then you have to sign up for AT82.03 in Aug 2020. The course signup code is MY3YPW .\n",
    "\n",
    "For each question, insert your answer directly in this sheet. When complete, export the sheet as a PDF and upload to Gradescope. If you are using puffer.cs.ait.ac.th, in JupyterLab, click on \"File\" then \"Export Notebook As\" then select PDF. If you are running Jupyter on your own machine, you may need to install the `nbconvert` package with pip or conda.\n",
    "\n",
    "Note that you have **2.5 hours** to do the exam. Also note that there are short answer questions that you may be able to answer faster than the coding questions. You might consider answering those questions first to get as much credit as possible!\n",
    "\n",
    "## Question 1 (10 points)\n",
    "\n",
    "We have seen that regression problems and binary classification problems have seemingly different cost functions. For regression, we normally use the cost function\n",
    "$$J(\\theta) = \\sum_{i=1}^m \\left( h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)} \\right)^2,$$\n",
    "whereas for binary classifiation, we normally use the cost function\n",
    "$$J(\\theta) = \\sum_{i=1}^m \\left( y^{(i)} \\log h_\\theta(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\log(1 - h_\\theta(\\mathbf{x}^{(i)})) \\right). $$\n",
    "\n",
    "Briefly explain (in English) how these two different cost functions are derived from the same idea or principle.\n",
    "\n",
    "*Write your answer here.*\n",
    "\n",
    "## Question 2 (20 points)\n",
    "\n",
    "Generate a sample of 200 points from a 2D Gaussian with mean $\\mu_1 = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$ and covariance $\\Sigma_1 = \\begin{bmatrix} 4 & 0 \\\\ 0 & 4 \\end{bmatrix}$\n",
    "\n",
    "Generate a second sample of 200 points from a 2D Gaussian with mean $\\mu_2 = \\begin{bmatrix} 5 \\\\ 5 \\end{bmatrix}$ and covariance $\\Sigma_2 = \\begin{bmatrix} 4 & 0 \\\\ 0 & 4 \\end{bmatrix}$.\n",
    "\n",
    "Assuming the first set of points is class 0 and the second set of points is class 1, split the dataset into 80% training and 20% testing, and plot the\n",
    "training set and test set in separate plots with different colors for each class below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code to generate the data and plot them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (20 points)\n",
    "\n",
    "Build a logistic regression model for the training set in Question 2. Plot the test set with correctly and incorrectly classified\n",
    "points using different symbols, and show the classification boundary in a plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code to train the logistic regression model and plot the results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 points)\n",
    "\n",
    "Explain why it is not possible to train a SVM using the techniques we've studied so far in class to classify the data from Question 2. Give a value for $\\mu_2$ in Question 2 keeping the other values constant that would make it very likely that the training could be classified using the SVM techniques we studied so far in class.\n",
    "\n",
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 points)\n",
    "\n",
    "Suppose you are building a linear SVM in $\\mathbb{R}^2$. The two classes are represented as 'X's and 'O's in the diagram below.\n",
    "\n",
    "<img src=\"http://www.cs.ait.ac.th/~mdailey/class/ml/q2.jpg\" width=\"400\"></img>\n",
    "\n",
    "If the training data consisted only of the four examples A, B, C, and D in the diagram above, which would be the support\n",
    "vectors?\n",
    "\n",
    "*Place your answer here*\n",
    "\n",
    "## Question 6 (10 points)\n",
    "\n",
    "In Question 3 (the linear SVM),\n",
    "if you were told that Lagrange multipliers $\\alpha_A = 1.0$ and $\\alpha_D = 2.0$, what would $\\alpha_B$ and $\\alpha_C$ be?\n",
    "\n",
    "*Place your answer here*\n",
    "\n",
    "## Question 7 (10 points)\n",
    "\n",
    "Suppose the optimal linear SVM for Question 3 had $\\mathbf{w} = \\begin{bmatrix} 1 & 1 \\end{bmatrix}^\\top$ and $b = -2$.\n",
    "Find the geometric margin $\\gamma^{(i)}$ for $\\mathbf{x}^{(i)} = \\begin{bmatrix} 2 & 5 \\end{bmatrix}^\\top$ and $y^{(i)} = 1$. \n",
    "\n",
    "*Place your answer here*\n",
    "\n",
    "## Question 8 (10 points)\n",
    "\n",
    "Consider the SVM kernel $K(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z})^2$. Explain the similarities and differences between a SVM with this kernel and a logistic regression using a quadratic polynomial transformation (an ordinary logistic regression preceeded by a transformation of the input vector. If they are exactly the same, prove it. If not, clearly explain how they are different.\n",
    "\n",
    "*Place your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
